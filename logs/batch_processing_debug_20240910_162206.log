2024-09-10 16:22:11,723 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 16:22:11,723 [33mPress CTRL+C to quit[0m
2024-09-10 16:22:16,384 Request with ID 34d6b690 for model gpt2-124m received
2024-09-10 16:22:16,384 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 16:22:16,384 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 16:22:16,385 127.0.0.1 - - [10/Sep/2024 16:22:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:16,414 Remaining requests condition met for model gpt2-124m
2024-09-10 16:22:16,414 Updated batch size:1
2024-09-10 16:22:16,414 Loading model gpt2-124m
2024-09-10 16:22:17,210 Request with ID 0da82b1b for model gpt2-124m received
2024-09-10 16:22:17,210 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 16:22:17,210 127.0.0.1 - - [10/Sep/2024 16:22:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:17,493 Processed batch: ['34d6b690'] with model gpt2-124m in 0.9961 seconds
2024-09-10 16:22:17,494 Latency for request 34d6b690 with model gpt2-124m: 1.1091 seconds
2024-09-10 16:22:17,541 Request with ID da9fdbe1 for model gpt2medium-355m received
2024-09-10 16:22:17,541 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 16:22:17,541 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 16:22:17,541 127.0.0.1 - - [10/Sep/2024 16:22:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:17,604 Request with ID 8c7d72ac for model gpt2-124m received
2024-09-10 16:22:17,604 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 16:22:17,604 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 16:22:17,604 127.0.0.1 - - [10/Sep/2024 16:22:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:17,977 Request with ID d1b7c52e for model gpt2-124m received
2024-09-10 16:22:17,978 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 16:22:17,978 127.0.0.1 - - [10/Sep/2024 16:22:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:18,640 Request with ID 96c314db for model distilgpt2-124m received
2024-09-10 16:22:18,640 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 16:22:18,640 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 16:22:18,640 127.0.0.1 - - [10/Sep/2024 16:22:18] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:19,251 Request with ID a53fb77b for model distilgpt2-124m received
2024-09-10 16:22:19,252 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 16:22:19,252 127.0.0.1 - - [10/Sep/2024 16:22:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:19,818 Request with ID f63525a9 for model gpt2medium-355m received
2024-09-10 16:22:19,819 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 16:22:19,819 127.0.0.1 - - [10/Sep/2024 16:22:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:20,212 Request with ID ee701d58 for model distilgpt2-124m received
2024-09-10 16:22:20,212 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 16:22:20,212 127.0.0.1 - - [10/Sep/2024 16:22:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:20,667 Request with ID acdc1409 for model gpt2medium-355m received
2024-09-10 16:22:20,667 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-10 16:22:20,667 127.0.0.1 - - [10/Sep/2024 16:22:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:21,156 Request with ID 4339bdf3 for model gpt2-124m received
2024-09-10 16:22:21,156 Adjusted time limit based on total queue size 10: 7.5000 seconds
2024-09-10 16:22:21,157 127.0.0.1 - - [10/Sep/2024 16:22:21] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:21,769 Request with ID 908436f2 for model gpt2medium-355m received
2024-09-10 16:22:21,769 Adjusted time limit based on total queue size 11: 7.5000 seconds
2024-09-10 16:22:21,770 127.0.0.1 - - [10/Sep/2024 16:22:21] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:22,132 Request with ID f7f75620 for model gpt2-124m received
2024-09-10 16:22:22,133 Adjusted time limit based on total queue size 12: 7.5000 seconds
2024-09-10 16:22:22,133 127.0.0.1 - - [10/Sep/2024 16:22:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:22,468 Request with ID c67a40c5 for model distilgpt2-124m received
2024-09-10 16:22:22,468 Adjusted time limit based on total queue size 13: 7.5000 seconds
2024-09-10 16:22:22,469 127.0.0.1 - - [10/Sep/2024 16:22:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:23,543 Request with ID 890461e0 for model gpt2-124m received
2024-09-10 16:22:23,543 Adjusted time limit based on total queue size 14: 7.5000 seconds
2024-09-10 16:22:23,544 127.0.0.1 - - [10/Sep/2024 16:22:23] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:24,744 Request with ID 7fb6b432 for model gpt2-124m received
2024-09-10 16:22:24,744 Adjusted time limit based on total queue size 15: 7.5000 seconds
2024-09-10 16:22:24,745 127.0.0.1 - - [10/Sep/2024 16:22:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:25,477 Request with ID 14a5a2f4 for model distilgpt2-124m received
2024-09-10 16:22:25,478 Adjusted time limit based on total queue size 16: 3.7500 seconds
2024-09-10 16:22:25,478 127.0.0.1 - - [10/Sep/2024 16:22:25] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:28,295 Request with ID 7dd6cb14 for model gpt2-124m received
2024-09-10 16:22:28,295 Adjusted time limit based on total queue size 17: 3.7500 seconds
2024-09-10 16:22:28,295 Batch size condition met for model gpt2-124m
2024-09-10 16:22:28,296 Updated batch size:8
2024-09-10 16:22:28,296 Loading model gpt2-124m
2024-09-10 16:22:29,374 Processed batch: ['0da82b1b', '8c7d72ac', 'd1b7c52e', '4339bdf3', 'f7f75620', '890461e0', '7fb6b432', '7dd6cb14'] with model gpt2-124m in 1.0778 seconds
2024-09-10 16:22:29,374 Latency for request 0da82b1b with model gpt2-124m: 12.1642 seconds
2024-09-10 16:22:29,375 Latency for request 8c7d72ac with model gpt2-124m: 11.7702 seconds
2024-09-10 16:22:29,376 Latency for request d1b7c52e with model gpt2-124m: 11.3965 seconds
2024-09-10 16:22:29,376 Latency for request 4339bdf3 with model gpt2-124m: 8.2178 seconds
2024-09-10 16:22:29,376 Latency for request f7f75620 with model gpt2-124m: 7.2414 seconds
2024-09-10 16:22:29,376 Latency for request 890461e0 with model gpt2-124m: 5.8308 seconds
2024-09-10 16:22:29,376 Latency for request 7fb6b432 with model gpt2-124m: 4.6301 seconds
2024-09-10 16:22:29,377 Latency for request 7dd6cb14 with model gpt2-124m: 1.0794 seconds
2024-09-10 16:22:29,377 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 443, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 276, in process_batch
    logging.debug()
TypeError: debug() missing 1 required positional argument: 'msg'
2024-09-10 16:22:29,378 127.0.0.1 - - [10/Sep/2024 16:22:29] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-10 16:22:29,537 Request with ID 5bde7ae4 for model gpt2medium-355m received
2024-09-10 16:22:29,537 Adjusted time limit based on total queue size 10: 7.5000 seconds
2024-09-10 16:22:29,537 127.0.0.1 - - [10/Sep/2024 16:22:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:30,480 Request with ID 52e03106 for model gpt2medium-355m received
2024-09-10 16:22:30,480 Adjusted time limit based on total queue size 11: 7.5000 seconds
2024-09-10 16:22:30,482 127.0.0.1 - - [10/Sep/2024 16:22:30] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:31,112 Request with ID e4a138ab for model gpt2medium-355m received
2024-09-10 16:22:31,113 Adjusted time limit based on total queue size 12: 7.5000 seconds
2024-09-10 16:22:31,113 127.0.0.1 - - [10/Sep/2024 16:22:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:32,270 Request with ID cd5c6ddc for model gpt2medium-355m received
2024-09-10 16:22:32,271 Adjusted time limit based on total queue size 13: 7.5000 seconds
2024-09-10 16:22:32,271 Batch size condition met for model gpt2medium-355m
2024-09-10 16:22:32,271 Updated batch size:8
2024-09-10 16:22:32,271 Loading model gpt2medium-355m
2024-09-10 16:22:32,976 Request with ID b17340a0 for model gpt2-124m received
2024-09-10 16:22:32,976 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 16:22:32,976 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 16:22:32,977 127.0.0.1 - - [10/Sep/2024 16:22:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:33,384 Request with ID 7dca8b10 for model distilgpt2-124m received
2024-09-10 16:22:33,384 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 16:22:33,384 127.0.0.1 - - [10/Sep/2024 16:22:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:34,445 Request with ID d6297607 for model distilgpt2-124m received
2024-09-10 16:22:34,445 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 16:22:34,445 127.0.0.1 - - [10/Sep/2024 16:22:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:35,347 Request with ID 3eb74ffd for model distilgpt2-124m received
2024-09-10 16:22:35,347 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-10 16:22:35,347 Batch size condition met for model distilgpt2-124m
2024-09-10 16:22:35,475 Request with ID 03928acd for model gpt2-124m received
2024-09-10 16:22:35,476 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 16:22:35,476 127.0.0.1 - - [10/Sep/2024 16:22:35] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:22:37,419 Processed batch: ['da9fdbe1', 'f63525a9', 'acdc1409', '908436f2', '5bde7ae4', '52e03106', 'e4a138ab', 'cd5c6ddc'] with model gpt2medium-355m in 5.0354 seconds
2024-09-10 16:22:37,419 Latency for request da9fdbe1 with model gpt2medium-355m: 19.8784 seconds
2024-09-10 16:22:37,421 Latency for request f63525a9 with model gpt2medium-355m: 17.6007 seconds
2024-09-10 16:22:37,421 Latency for request acdc1409 with model gpt2medium-355m: 16.7526 seconds
2024-09-10 16:22:37,421 Latency for request 908436f2 with model gpt2medium-355m: 15.6504 seconds
2024-09-10 16:22:37,421 Latency for request 5bde7ae4 with model gpt2medium-355m: 7.8819 seconds
2024-09-10 16:22:37,422 Latency for request 52e03106 with model gpt2medium-355m: 6.9393 seconds
2024-09-10 16:22:37,422 Latency for request e4a138ab with model gpt2medium-355m: 6.3068 seconds
2024-09-10 16:22:37,422 Latency for request cd5c6ddc with model gpt2medium-355m: 5.1487 seconds
2024-09-10 16:22:37,422 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 443, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 276, in process_batch
    logging.debug()
TypeError: debug() missing 1 required positional argument: 'msg'
2024-09-10 16:22:37,422 Updated batch size:8
2024-09-10 16:22:37,423 127.0.0.1 - - [10/Sep/2024 16:22:37] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-10 16:22:37,423 Loading model distilgpt2-124m
2024-09-10 16:22:38,433 Processed batch: ['96c314db', 'a53fb77b', 'ee701d58', 'c67a40c5', '14a5a2f4', '7dca8b10', 'd6297607', '3eb74ffd'] with model distilgpt2-124m in 0.9256 seconds
2024-09-10 16:22:38,434 Latency for request 96c314db with model distilgpt2-124m: 19.7937 seconds
2024-09-10 16:22:38,435 Latency for request a53fb77b with model distilgpt2-124m: 19.1823 seconds
2024-09-10 16:22:38,436 Latency for request ee701d58 with model distilgpt2-124m: 18.2218 seconds
2024-09-10 16:22:38,436 Latency for request c67a40c5 with model distilgpt2-124m: 15.9651 seconds
2024-09-10 16:22:38,436 Latency for request 14a5a2f4 with model distilgpt2-124m: 12.9563 seconds
2024-09-10 16:22:38,436 Latency for request 7dca8b10 with model distilgpt2-124m: 5.0491 seconds
2024-09-10 16:22:38,436 Latency for request d6297607 with model distilgpt2-124m: 3.9883 seconds
2024-09-10 16:22:38,437 Latency for request 3eb74ffd with model distilgpt2-124m: 3.0862 seconds
2024-09-10 16:22:38,437 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 443, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler.py", line 276, in process_batch
    logging.debug()
TypeError: debug() missing 1 required positional argument: 'msg'
2024-09-10 16:22:38,437 127.0.0.1 - - [10/Sep/2024 16:22:38] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
