2024-09-17 11:43:44,348 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-17 11:43:44,348 [33mPress CTRL+C to quit[0m
2024-09-17 11:43:48,201 Request with ID 5c7a72f9 for model gpt2medium-355m received
2024-09-17 11:43:48,201 Batch size condition met for model gpt2medium-355m
2024-09-17 11:43:48,201 Next: call load_model for gpt2medium-355m
2024-09-17 11:43:48,330 Loaded model gpt2medium-355m
2024-09-17 11:43:48,330 Batch processing started for model gpt2medium-355m
2024-09-17 11:43:49,528 Request with ID b283d05f for model gpt2medium-355m received
2024-09-17 11:43:49,528 Batch size condition met for model gpt2medium-355m
2024-09-17 11:43:50,582 Processed batch: ['5c7a72f9'] with model gpt2medium-355m in 2.2517 seconds
2024-09-17 11:43:50,582 Latency for request 5c7a72f9 with model gpt2medium-355m: 2.3811 seconds
2024-09-17 11:43:50,584 127.0.0.1 - - [17/Sep/2024 11:43:50] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:50,584 Next: call load_model for gpt2medium-355m
2024-09-17 11:43:50,585 Model gpt2medium-355m already loaded
2024-09-17 11:43:50,585 Batch processing started for model gpt2medium-355m
2024-09-17 11:43:51,782 Request with ID 0eb3ecd2 for model gpt2-124m received
2024-09-17 11:43:51,782 Batch size condition met for model gpt2-124m
2024-09-17 11:43:52,753 Processed batch: ['b283d05f'] with model gpt2medium-355m in 2.1688 seconds
2024-09-17 11:43:52,754 Latency for request b283d05f with model gpt2medium-355m: 3.2259 seconds
2024-09-17 11:43:52,754 127.0.0.1 - - [17/Sep/2024 11:43:52] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:52,754 Next: call load_model for gpt2-124m
2024-09-17 11:43:52,762 Unloaded previous model
2024-09-17 11:43:52,823 Loaded model gpt2-124m
2024-09-17 11:43:52,823 Batch processing started for model gpt2-124m
2024-09-17 11:43:54,109 Request with ID 0ffa0244 for model distilgpt2-124m received
2024-09-17 11:43:54,109 Batch size condition met for model distilgpt2-124m
2024-09-17 11:43:54,426 Processed batch: ['0eb3ecd2'] with model gpt2-124m in 1.6023 seconds
2024-09-17 11:43:54,426 Latency for request 0eb3ecd2 with model gpt2-124m: 2.6439 seconds
2024-09-17 11:43:54,426 127.0.0.1 - - [17/Sep/2024 11:43:54] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:54,426 Next: call load_model for distilgpt2-124m
2024-09-17 11:43:54,433 Unloaded previous model
2024-09-17 11:43:54,477 Loaded model distilgpt2-124m
2024-09-17 11:43:54,477 Batch processing started for model distilgpt2-124m
2024-09-17 11:43:55,543 Request with ID da49c790 for model gpt2-124m received
2024-09-17 11:43:55,544 Batch size condition met for model gpt2-124m
2024-09-17 11:43:55,597 Processed batch: ['0ffa0244'] with model distilgpt2-124m in 1.1198 seconds
2024-09-17 11:43:55,597 Latency for request 0ffa0244 with model distilgpt2-124m: 1.4881 seconds
2024-09-17 11:43:55,598 127.0.0.1 - - [17/Sep/2024 11:43:55] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:55,598 Next: call load_model for gpt2-124m
2024-09-17 11:43:55,603 Unloaded previous model
2024-09-17 11:43:55,658 Loaded model gpt2-124m
2024-09-17 11:43:55,659 Batch processing started for model gpt2-124m
2024-09-17 11:43:56,483 Processed batch: ['da49c790'] with model gpt2-124m in 0.8241 seconds
2024-09-17 11:43:56,483 Latency for request da49c790 with model gpt2-124m: 0.9392 seconds
2024-09-17 11:43:56,484 127.0.0.1 - - [17/Sep/2024 11:43:56] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:57,724 Request with ID 7a4e9107 for model gpt2medium-355m received
2024-09-17 11:43:57,724 Batch size condition met for model gpt2medium-355m
2024-09-17 11:43:57,724 Next: call load_model for gpt2medium-355m
2024-09-17 11:43:57,735 Unloaded previous model
2024-09-17 11:43:57,852 Loaded model gpt2medium-355m
2024-09-17 11:43:57,852 Batch processing started for model gpt2medium-355m
2024-09-17 11:43:58,175 Processed batch: ['7a4e9107'] with model gpt2medium-355m in 0.3229 seconds
2024-09-17 11:43:58,175 Latency for request 7a4e9107 with model gpt2medium-355m: 0.4509 seconds
2024-09-17 11:43:58,176 127.0.0.1 - - [17/Sep/2024 11:43:58] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:43:58,202 Total time: 9.9747 seconds
2024-09-17 11:43:58,202 Total inference time: 8.2896 seconds
2024-09-17 11:43:58,202 Inference time as percentage of total time: 83.11%
2024-09-17 11:43:58,202 END
2024-09-17 11:43:58,202 127.0.0.1 - - [17/Sep/2024 11:43:58] "POST /inference HTTP/1.1" 200 -
