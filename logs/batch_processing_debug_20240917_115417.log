2024-09-17 11:54:17,806 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-17 11:54:17,806 [33mPress CTRL+C to quit[0m
2024-09-17 11:54:25,249 Request with ID 1d9d776a for model gpt2medium-355m received
2024-09-17 11:54:25,249 Batch size condition met for model gpt2medium-355m
2024-09-17 11:54:25,249 Next: call load_model for gpt2medium-355m
2024-09-17 11:54:25,375 Loaded model gpt2medium-355m
2024-09-17 11:54:25,375 Batch processing started for model gpt2medium-355m
2024-09-17 11:54:26,575 Request with ID 30f11d43 for model gpt2medium-355m received
2024-09-17 11:54:26,576 Batch size condition met for model gpt2medium-355m
2024-09-17 11:54:27,579 Processed batch: ['1d9d776a'] with model gpt2medium-355m in 2.2047 seconds
2024-09-17 11:54:27,580 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 463, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 233, in process_batch
    sys_info = monitor.get_sys_info()
               ^^^^^^^
NameError: name 'monitor' is not defined. Did you mean: 'Monitor'?
2024-09-17 11:54:27,580 Next: call load_model for gpt2medium-355m
2024-09-17 11:54:27,582 127.0.0.1 - - [17/Sep/2024 11:54:27] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 11:54:27,582 Model gpt2medium-355m already loaded
2024-09-17 11:54:27,582 Batch processing started for model gpt2medium-355m
2024-09-17 11:54:28,830 Request with ID 528ecb67 for model gpt2-124m received
2024-09-17 11:54:28,831 Batch size condition met for model gpt2-124m
2024-09-17 11:54:29,664 Processed batch: ['30f11d43'] with model gpt2medium-355m in 2.0819 seconds
2024-09-17 11:54:29,664 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 463, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 233, in process_batch
    sys_info = monitor.get_sys_info()
               ^^^^^^^
NameError: name 'monitor' is not defined. Did you mean: 'Monitor'?
2024-09-17 11:54:29,665 Next: call load_model for gpt2-124m
2024-09-17 11:54:29,665 127.0.0.1 - - [17/Sep/2024 11:54:29] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 11:54:29,665 Unloaded previous model
2024-09-17 11:54:29,732 Loaded model gpt2-124m
2024-09-17 11:54:29,732 Batch processing started for model gpt2-124m
2024-09-17 11:54:31,158 Request with ID f0ac8203 for model distilgpt2-124m received
2024-09-17 11:54:31,158 Batch size condition met for model distilgpt2-124m
2024-09-17 11:54:31,354 Processed batch: ['528ecb67'] with model gpt2-124m in 1.6222 seconds
2024-09-17 11:54:31,354 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 463, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 233, in process_batch
    sys_info = monitor.get_sys_info()
               ^^^^^^^
NameError: name 'monitor' is not defined. Did you mean: 'Monitor'?
2024-09-17 11:54:31,354 Next: call load_model for distilgpt2-124m
2024-09-17 11:54:31,355 127.0.0.1 - - [17/Sep/2024 11:54:31] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 11:54:31,355 Unloaded previous model
2024-09-17 11:54:31,408 Loaded model distilgpt2-124m
2024-09-17 11:54:31,408 Batch processing started for model distilgpt2-124m
2024-09-17 11:54:32,539 Processed batch: ['f0ac8203'] with model distilgpt2-124m in 1.1306 seconds
2024-09-17 11:54:32,539 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 463, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 233, in process_batch
    sys_info = monitor.get_sys_info()
               ^^^^^^^
NameError: name 'monitor' is not defined. Did you mean: 'Monitor'?
2024-09-17 11:54:32,540 127.0.0.1 - - [17/Sep/2024 11:54:32] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 11:54:32,591 Request with ID 25a40eac for model gpt2-124m received
2024-09-17 11:54:32,592 Batch size condition met for model gpt2-124m
2024-09-17 11:54:32,592 Next: call load_model for gpt2-124m
2024-09-17 11:54:32,592 Unloaded previous model
2024-09-17 11:54:32,653 Loaded model gpt2-124m
2024-09-17 11:54:32,653 Batch processing started for model gpt2-124m
2024-09-17 11:54:33,462 Processed batch: ['25a40eac'] with model gpt2-124m in 0.8084 seconds
2024-09-17 11:54:33,462 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 463, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 233, in process_batch
    sys_info = monitor.get_sys_info()
               ^^^^^^^
NameError: name 'monitor' is not defined. Did you mean: 'Monitor'?
2024-09-17 11:54:33,462 127.0.0.1 - - [17/Sep/2024 11:54:33] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
