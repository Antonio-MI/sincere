2024-09-18 13:06:31,375 Using device: cpu
2024-09-18 13:06:31,375 Scheduling mode set as batchedFCFS
2024-09-18 13:06:31,392 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 13:06:31,392 [33mPress CTRL+C to quit[0m
2024-09-18 13:06:40,678 Request with ID 3a08b55f for model gpt2medium-355m received
2024-09-18 13:06:40,678 127.0.0.1 - - [18/Sep/2024 13:06:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:40,811 Request with ID a1685567 for model distilgpt2-124m received
2024-09-18 13:06:40,811 127.0.0.1 - - [18/Sep/2024 13:06:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:40,829 Request with ID f90c8c79 for model gpt2medium-355m received
2024-09-18 13:06:40,829 127.0.0.1 - - [18/Sep/2024 13:06:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,035 Request with ID dcbb920d for model distilgpt2-124m received
2024-09-18 13:06:41,036 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,043 Request with ID cd20a094 for model distilgpt2-124m received
2024-09-18 13:06:41,044 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,068 Request with ID f481f2be for model gpt2-124m received
2024-09-18 13:06:41,069 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,167 Request with ID f43baca6 for model gpt2-124m received
2024-09-18 13:06:41,167 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,226 Request with ID 8653e176 for model gpt2medium-355m received
2024-09-18 13:06:41,227 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,279 Request with ID ebf98742 for model gpt2medium-355m received
2024-09-18 13:06:41,279 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:41,279 Next: call load_model for gpt2medium-355m
2024-09-18 13:06:41,342 Request with ID 594677df for model gpt2medium-355m received
2024-09-18 13:06:41,342 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,458 Loaded model gpt2medium-355m
2024-09-18 13:06:41,459 Batch processing started for model gpt2medium-355m
2024-09-18 13:06:41,748 Request with ID 7671fdeb for model gpt2medium-355m received
2024-09-18 13:06:41,748 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:41,946 Request with ID 3461a66e for model gpt2-124m received
2024-09-18 13:06:41,946 127.0.0.1 - - [18/Sep/2024 13:06:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,007 Request with ID e13bf30a for model gpt2-124m received
2024-09-18 13:06:42,007 Batch size condition met for model gpt2-124m
2024-09-18 13:06:42,346 Request with ID 51acfac9 for model distilgpt2-124m received
2024-09-18 13:06:42,346 Batch size condition met for model distilgpt2-124m
2024-09-18 13:06:42,383 Request with ID 71d8924e for model gpt2medium-355m received
2024-09-18 13:06:42,383 127.0.0.1 - - [18/Sep/2024 13:06:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,500 Request with ID 34451d58 for model gpt2medium-355m received
2024-09-18 13:06:42,500 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:42,520 Request with ID 714a98c7 for model distilgpt2-124m received
2024-09-18 13:06:42,520 127.0.0.1 - - [18/Sep/2024 13:06:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,630 Request with ID 95db83ea for model distilgpt2-124m received
2024-09-18 13:06:42,630 127.0.0.1 - - [18/Sep/2024 13:06:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,655 Request with ID 9433bd6e for model gpt2medium-355m received
2024-09-18 13:06:42,655 127.0.0.1 - - [18/Sep/2024 13:06:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,841 Request with ID f004a853 for model distilgpt2-124m received
2024-09-18 13:06:42,841 127.0.0.1 - - [18/Sep/2024 13:06:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:42,892 Request with ID cbcdde86 for model distilgpt2-124m received
2024-09-18 13:06:42,892 Batch size condition met for model distilgpt2-124m
2024-09-18 13:06:43,024 Request with ID 441f5d20 for model distilgpt2-124m received
2024-09-18 13:06:43,024 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,047 Request with ID 08e8a87d for model distilgpt2-124m received
2024-09-18 13:06:43,047 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,087 Request with ID 6fee6040 for model gpt2medium-355m received
2024-09-18 13:06:43,087 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,264 Request with ID 35869634 for model gpt2-124m received
2024-09-18 13:06:43,265 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,281 Request with ID 8e92f3c8 for model gpt2-124m received
2024-09-18 13:06:43,281 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,290 Request with ID c6519374 for model gpt2-124m received
2024-09-18 13:06:43,290 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,313 Request with ID a6f65bb8 for model gpt2medium-355m received
2024-09-18 13:06:43,313 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:43,484 Request with ID 18dd8787 for model gpt2medium-355m received
2024-09-18 13:06:43,484 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:43,641 Request with ID a1be3942 for model gpt2-124m received
2024-09-18 13:06:43,641 Batch size condition met for model gpt2-124m
2024-09-18 13:06:43,903 Request with ID a685966f for model gpt2medium-355m received
2024-09-18 13:06:43,904 127.0.0.1 - - [18/Sep/2024 13:06:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,095 Request with ID 705631c6 for model distilgpt2-124m received
2024-09-18 13:06:44,095 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,105 Request with ID 3f7c9b2a for model gpt2medium-355m received
2024-09-18 13:06:44,106 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,218 Request with ID 24d2a497 for model gpt2medium-355m received
2024-09-18 13:06:44,218 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,308 Request with ID 7403ca67 for model gpt2-124m received
2024-09-18 13:06:44,308 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,358 Request with ID f4a3bd22 for model gpt2medium-355m received
2024-09-18 13:06:44,358 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:44,502 Request with ID 710743c2 for model gpt2medium-355m received
2024-09-18 13:06:44,502 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,533 Request with ID 6a955839 for model gpt2medium-355m received
2024-09-18 13:06:44,533 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,590 Request with ID e8b20470 for model gpt2medium-355m received
2024-09-18 13:06:44,590 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,593 Request with ID 3138bb4c for model gpt2-124m received
2024-09-18 13:06:44,593 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,666 Request with ID 563eea5e for model gpt2-124m received
2024-09-18 13:06:44,666 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,696 Request with ID fe58abab for model gpt2medium-355m received
2024-09-18 13:06:44,696 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:44,720 Request with ID 1ebd4b79 for model gpt2medium-355m received
2024-09-18 13:06:44,720 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:44,755 Request with ID 34fafd7e for model distilgpt2-124m received
2024-09-18 13:06:44,756 Batch size condition met for model distilgpt2-124m
2024-09-18 13:06:44,855 Request with ID 4dea9c0e for model gpt2-124m received
2024-09-18 13:06:44,855 Batch size condition met for model gpt2-124m
2024-09-18 13:06:44,867 Request with ID f52f4ee4 for model distilgpt2-124m received
2024-09-18 13:06:44,867 127.0.0.1 - - [18/Sep/2024 13:06:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,083 Request with ID d49472f0 for model distilgpt2-124m received
2024-09-18 13:06:45,083 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,097 Request with ID 9b2b6534 for model gpt2medium-355m received
2024-09-18 13:06:45,097 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,336 Request with ID 1cf963ed for model gpt2-124m received
2024-09-18 13:06:45,337 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,441 Request with ID 1a7733c3 for model gpt2-124m received
2024-09-18 13:06:45,441 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,479 Request with ID d1f07df4 for model gpt2medium-355m received
2024-09-18 13:06:45,479 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,535 Request with ID cc0935ab for model gpt2medium-355m received
2024-09-18 13:06:45,535 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:45,554 Request with ID 64e4afe5 for model gpt2-124m received
2024-09-18 13:06:45,554 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,607 Request with ID fd1f8f67 for model gpt2-124m received
2024-09-18 13:06:45,607 Batch size condition met for model gpt2-124m
2024-09-18 13:06:45,638 Request with ID ff90833c for model distilgpt2-124m received
2024-09-18 13:06:45,638 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:45,759 Request with ID 073794dc for model gpt2-124m received
2024-09-18 13:06:45,760 127.0.0.1 - - [18/Sep/2024 13:06:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:46,208 Processed batch: ['3a08b55f', 'f90c8c79', '8653e176', 'ebf98742'] with model gpt2medium-355m in 1726556465.3555 seconds
2024-09-18 13:06:46,208 Latency for request 3a08b55f with model gpt2medium-355m: 5.5300 seconds
2024-09-18 13:06:46,208 Saving results without gpu monitoring
2024-09-18 13:06:46,208 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:06:46,209 Next: call load_model for gpt2-124m
2024-09-18 13:06:46,211 127.0.0.1 - - [18/Sep/2024 13:06:46] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:06:46,211 Unloaded previous model
2024-09-18 13:06:46,271 Loaded model gpt2-124m
2024-09-18 13:06:46,271 Batch processing started for model gpt2-124m
2024-09-18 13:06:46,368 Request with ID 30abfe23 for model gpt2-124m received
2024-09-18 13:06:46,369 127.0.0.1 - - [18/Sep/2024 13:06:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:46,745 Request with ID ce9f24fb for model gpt2medium-355m received
2024-09-18 13:06:46,746 127.0.0.1 - - [18/Sep/2024 13:06:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:46,779 Request with ID 4bfedd56 for model distilgpt2-124m received
2024-09-18 13:06:46,779 Batch size condition met for model distilgpt2-124m
2024-09-18 13:06:46,856 Request with ID 31ae2328 for model distilgpt2-124m received
2024-09-18 13:06:46,856 127.0.0.1 - - [18/Sep/2024 13:06:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:46,885 Request with ID 62fcd23f for model gpt2medium-355m received
2024-09-18 13:06:46,886 127.0.0.1 - - [18/Sep/2024 13:06:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,030 Request with ID cc7380e6 for model gpt2-124m received
2024-09-18 13:06:47,030 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,162 Request with ID c3dea247 for model distilgpt2-124m received
2024-09-18 13:06:47,162 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,240 Request with ID 273e8d48 for model gpt2-124m received
2024-09-18 13:06:47,240 Batch size condition met for model gpt2-124m
2024-09-18 13:06:47,280 Request with ID 4ee33202 for model gpt2medium-355m received
2024-09-18 13:06:47,280 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,307 Request with ID a07bde51 for model gpt2medium-355m received
2024-09-18 13:06:47,307 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:47,473 Request with ID 0c81c54c for model gpt2-124m received
2024-09-18 13:06:47,473 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,592 Request with ID 9e6c718a for model gpt2medium-355m received
2024-09-18 13:06:47,593 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,870 Request with ID d245dd04 for model gpt2medium-355m received
2024-09-18 13:06:47,870 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,912 Request with ID 85967c56 for model gpt2medium-355m received
2024-09-18 13:06:47,912 127.0.0.1 - - [18/Sep/2024 13:06:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:47,991 Request with ID a3c99d5b for model gpt2medium-355m received
2024-09-18 13:06:47,991 Batch size condition met for model gpt2medium-355m
2024-09-18 13:06:48,100 Request with ID 3f48685b for model gpt2-124m received
2024-09-18 13:06:48,101 127.0.0.1 - - [18/Sep/2024 13:06:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:48,148 Processed batch: ['1cf963ed', '1a7733c3', '64e4afe5', 'fd1f8f67'] with model gpt2-124m in 1726556462.4827 seconds
2024-09-18 13:06:48,148 Latency for request 1cf963ed with model gpt2-124m: 2.8120 seconds
2024-09-18 13:06:48,148 Saving results without gpu monitoring
2024-09-18 13:06:48,148 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:06:48,149 Next: call load_model for distilgpt2-124m
2024-09-18 13:06:48,149 127.0.0.1 - - [18/Sep/2024 13:06:48] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:06:48,149 Unloaded previous model
2024-09-18 13:06:48,182 Request with ID 8ce58010 for model distilgpt2-124m received
2024-09-18 13:06:48,182 127.0.0.1 - - [18/Sep/2024 13:06:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:48,205 Loaded model distilgpt2-124m
2024-09-18 13:06:48,205 Batch processing started for model distilgpt2-124m
2024-09-18 13:06:48,735 Request with ID bc417fdc for model gpt2-124m received
2024-09-18 13:06:48,735 127.0.0.1 - - [18/Sep/2024 13:06:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:48,951 Request with ID 62c7294f for model gpt2-124m received
2024-09-18 13:06:48,951 Batch size condition met for model gpt2-124m
2024-09-18 13:06:48,995 Request with ID 3a6bc205 for model distilgpt2-124m received
2024-09-18 13:06:48,995 Batch size condition met for model distilgpt2-124m
2024-09-18 13:06:49,147 Request with ID 963d1d0a for model gpt2medium-355m received
2024-09-18 13:06:49,147 127.0.0.1 - - [18/Sep/2024 13:06:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:49,213 Processed batch: ['f52f4ee4', 'd49472f0', 'ff90833c', '4bfedd56'] with model distilgpt2-124m in 1726556461.6141 seconds
2024-09-18 13:06:49,213 Latency for request f52f4ee4 with model distilgpt2-124m: 4.3460 seconds
2024-09-18 13:06:49,213 Saving results without gpu monitoring
2024-09-18 13:06:49,213 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:06:49,213 Next: call load_model for gpt2medium-355m
2024-09-18 13:06:49,214 127.0.0.1 - - [18/Sep/2024 13:06:49] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:06:49,214 Unloaded previous model
2024-09-18 13:06:49,325 Loaded model gpt2medium-355m
2024-09-18 13:06:49,325 Batch processing started for model gpt2medium-355m
2024-09-18 13:06:49,423 Request with ID 1a900eea for model distilgpt2-124m received
2024-09-18 13:06:49,423 127.0.0.1 - - [18/Sep/2024 13:06:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:49,535 Request with ID c47f7290 for model gpt2medium-355m received
2024-09-18 13:06:49,535 127.0.0.1 - - [18/Sep/2024 13:06:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:49,583 Request with ID 9abf109f for model gpt2medium-355m received
2024-09-18 13:06:49,583 127.0.0.1 - - [18/Sep/2024 13:06:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:49,948 Request with ID 8d4c5ddf for model distilgpt2-124m received
2024-09-18 13:06:49,948 127.0.0.1 - - [18/Sep/2024 13:06:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:50,029 Request with ID cb5d393a for model gpt2-124m received
2024-09-18 13:06:50,029 127.0.0.1 - - [18/Sep/2024 13:06:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:50,050 Request with ID 610170f2 for model gpt2-124m received
2024-09-18 13:06:50,050 127.0.0.1 - - [18/Sep/2024 13:06:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:06:50,119 Request with ID 91fa43fc for model gpt2medium-355m received
2024-09-18 13:06:50,119 Batch size condition met for model gpt2medium-355m
