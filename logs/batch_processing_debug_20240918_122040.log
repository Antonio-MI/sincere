2024-09-18 12:20:40,227 Using device: cpu
2024-09-18 12:20:40,227 Scheduling mode set as batchedFCFS
2024-09-18 12:20:40,251 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 12:20:40,251 [33mPress CTRL+C to quit[0m
2024-09-18 12:20:44,962 Request with ID 556cc95c for model gpt2medium-355m received
2024-09-18 12:20:44,963 127.0.0.1 - - [18/Sep/2024 12:20:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,096 Request with ID 3fdbc4e0 for model distilgpt2-124m received
2024-09-18 12:20:45,096 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,114 Request with ID c31e056b for model gpt2medium-355m received
2024-09-18 12:20:45,114 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,320 Request with ID 37b7d3a7 for model distilgpt2-124m received
2024-09-18 12:20:45,320 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,327 Request with ID f44413c0 for model distilgpt2-124m received
2024-09-18 12:20:45,328 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,352 Request with ID 339204e1 for model gpt2-124m received
2024-09-18 12:20:45,352 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,451 Request with ID a95381ab for model gpt2-124m received
2024-09-18 12:20:45,451 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,510 Request with ID 3b774e37 for model gpt2medium-355m received
2024-09-18 12:20:45,510 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,563 Request with ID f96785d1 for model gpt2medium-355m received
2024-09-18 12:20:45,564 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:45,627 Request with ID 037b15d0 for model gpt2medium-355m received
2024-09-18 12:20:45,627 127.0.0.1 - - [18/Sep/2024 12:20:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,033 Request with ID 4b6bf83a for model gpt2medium-355m received
2024-09-18 12:20:46,034 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,236 Request with ID aa1e0eda for model gpt2-124m received
2024-09-18 12:20:46,237 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,298 Request with ID 9f45568b for model gpt2-124m received
2024-09-18 12:20:46,299 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,638 Request with ID ec573760 for model distilgpt2-124m received
2024-09-18 12:20:46,638 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,675 Request with ID e90bdca0 for model gpt2medium-355m received
2024-09-18 12:20:46,676 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,793 Request with ID 731f5eb3 for model gpt2medium-355m received
2024-09-18 12:20:46,793 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:46,793 Next: call load_model for gpt2medium-355m
2024-09-18 12:20:46,814 Request with ID 7a017dc6 for model distilgpt2-124m received
2024-09-18 12:20:46,815 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,922 Request with ID 94d9bb4b for model distilgpt2-124m received
2024-09-18 12:20:46,922 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,947 Request with ID 7a92bc99 for model gpt2medium-355m received
2024-09-18 12:20:46,947 127.0.0.1 - - [18/Sep/2024 12:20:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:46,964 Loaded model gpt2medium-355m
2024-09-18 12:20:46,964 Batch processing started for model gpt2medium-355m
2024-09-18 12:20:47,132 Request with ID e3475648 for model distilgpt2-124m received
2024-09-18 12:20:47,132 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,183 Request with ID 009aaa10 for model distilgpt2-124m received
2024-09-18 12:20:47,183 Batch size condition met for model distilgpt2-124m
2024-09-18 12:20:47,314 Request with ID 923f776f for model distilgpt2-124m received
2024-09-18 12:20:47,314 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,337 Request with ID 8a2e29ff for model distilgpt2-124m received
2024-09-18 12:20:47,337 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,377 Request with ID b0daf159 for model gpt2medium-355m received
2024-09-18 12:20:47,378 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,555 Request with ID b1ebb4ab for model gpt2-124m received
2024-09-18 12:20:47,555 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,572 Request with ID 4ba51dc5 for model gpt2-124m received
2024-09-18 12:20:47,572 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,581 Request with ID b13633a6 for model gpt2-124m received
2024-09-18 12:20:47,581 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,604 Request with ID 746fa44b for model gpt2medium-355m received
2024-09-18 12:20:47,604 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,775 Request with ID a3e6f673 for model gpt2medium-355m received
2024-09-18 12:20:47,776 127.0.0.1 - - [18/Sep/2024 12:20:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:47,932 Request with ID c56cd821 for model gpt2-124m received
2024-09-18 12:20:47,932 Batch size condition met for model gpt2-124m
2024-09-18 12:20:48,193 Request with ID 19dadb4a for model gpt2medium-355m received
2024-09-18 12:20:48,193 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,384 Request with ID d2b08ca9 for model distilgpt2-124m received
2024-09-18 12:20:48,385 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,397 Request with ID 3a5fe7ce for model gpt2medium-355m received
2024-09-18 12:20:48,397 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,508 Request with ID aebf98a0 for model gpt2medium-355m received
2024-09-18 12:20:48,508 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,598 Request with ID 869dd67e for model gpt2-124m received
2024-09-18 12:20:48,598 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,648 Request with ID c132fc40 for model gpt2medium-355m received
2024-09-18 12:20:48,648 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:48,792 Request with ID 7b56b54e for model gpt2medium-355m received
2024-09-18 12:20:48,793 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,823 Request with ID 96454d40 for model gpt2medium-355m received
2024-09-18 12:20:48,824 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,881 Request with ID 86918c50 for model gpt2medium-355m received
2024-09-18 12:20:48,881 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,884 Request with ID 8c563301 for model gpt2-124m received
2024-09-18 12:20:48,884 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,957 Request with ID 7a30828b for model gpt2-124m received
2024-09-18 12:20:48,957 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:48,986 Request with ID f46723b3 for model gpt2medium-355m received
2024-09-18 12:20:48,987 127.0.0.1 - - [18/Sep/2024 12:20:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,010 Request with ID 32e81865 for model gpt2medium-355m received
2024-09-18 12:20:49,010 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,046 Request with ID 662ad66a for model distilgpt2-124m received
2024-09-18 12:20:49,046 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,145 Request with ID 15b442f9 for model gpt2-124m received
2024-09-18 12:20:49,146 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,157 Request with ID 7319481a for model distilgpt2-124m received
2024-09-18 12:20:49,157 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,373 Request with ID 2c838db1 for model distilgpt2-124m received
2024-09-18 12:20:49,373 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,387 Request with ID 23ea5e68 for model gpt2medium-355m received
2024-09-18 12:20:49,387 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,626 Request with ID 8771830a for model gpt2-124m received
2024-09-18 12:20:49,626 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,732 Request with ID 5a4359c1 for model gpt2-124m received
2024-09-18 12:20:49,732 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,770 Request with ID 0768dbae for model gpt2medium-355m received
2024-09-18 12:20:49,770 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,825 Request with ID 0e86603a for model gpt2medium-355m received
2024-09-18 12:20:49,825 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:49,846 Request with ID 044c35a8 for model gpt2-124m received
2024-09-18 12:20:49,846 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:49,899 Request with ID 44fc470d for model gpt2-124m received
2024-09-18 12:20:49,899 Batch size condition met for model gpt2-124m
2024-09-18 12:20:49,929 Request with ID 11d80540 for model distilgpt2-124m received
2024-09-18 12:20:49,929 127.0.0.1 - - [18/Sep/2024 12:20:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:50,049 Request with ID 430c2592 for model gpt2-124m received
2024-09-18 12:20:50,049 127.0.0.1 - - [18/Sep/2024 12:20:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:50,660 Request with ID f6bd359c for model gpt2-124m received
2024-09-18 12:20:50,660 127.0.0.1 - - [18/Sep/2024 12:20:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,036 Request with ID 76bc58c7 for model gpt2medium-355m received
2024-09-18 12:20:51,036 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,070 Request with ID 653f5fee for model distilgpt2-124m received
2024-09-18 12:20:51,070 Batch size condition met for model distilgpt2-124m
2024-09-18 12:20:51,148 Request with ID 066536db for model distilgpt2-124m received
2024-09-18 12:20:51,148 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,177 Request with ID 5a47e84e for model gpt2medium-355m received
2024-09-18 12:20:51,177 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,322 Request with ID c90e0b25 for model gpt2-124m received
2024-09-18 12:20:51,322 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,453 Request with ID fadac683 for model distilgpt2-124m received
2024-09-18 12:20:51,453 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,531 Request with ID 05073e3a for model gpt2-124m received
2024-09-18 12:20:51,531 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,571 Request with ID 8744a0fe for model gpt2medium-355m received
2024-09-18 12:20:51,571 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,597 Request with ID 7586fd9b for model gpt2medium-355m received
2024-09-18 12:20:51,598 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,765 Request with ID 44fc5ee6 for model gpt2-124m received
2024-09-18 12:20:51,765 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:51,885 Request with ID 08b88585 for model gpt2medium-355m received
2024-09-18 12:20:51,885 127.0.0.1 - - [18/Sep/2024 12:20:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:52,161 Request with ID 748992ea for model gpt2medium-355m received
2024-09-18 12:20:52,161 127.0.0.1 - - [18/Sep/2024 12:20:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:52,204 Request with ID d4973979 for model gpt2medium-355m received
2024-09-18 12:20:52,204 127.0.0.1 - - [18/Sep/2024 12:20:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:52,283 Request with ID 60778adf for model gpt2medium-355m received
2024-09-18 12:20:52,283 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:52,392 Request with ID 6a9638fc for model gpt2-124m received
2024-09-18 12:20:52,392 127.0.0.1 - - [18/Sep/2024 12:20:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:52,466 Request with ID 1ac740ba for model distilgpt2-124m received
2024-09-18 12:20:52,466 127.0.0.1 - - [18/Sep/2024 12:20:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,026 Request with ID 9738c42d for model gpt2-124m received
2024-09-18 12:20:53,026 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,243 Request with ID 3bf5dfeb for model gpt2-124m received
2024-09-18 12:20:53,243 Batch size condition met for model gpt2-124m
2024-09-18 12:20:53,286 Request with ID 99bf3c77 for model distilgpt2-124m received
2024-09-18 12:20:53,287 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,439 Request with ID cc86c2b4 for model gpt2medium-355m received
2024-09-18 12:20:53,439 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,715 Request with ID a2fcd588 for model distilgpt2-124m received
2024-09-18 12:20:53,715 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,826 Request with ID e8d3c9c4 for model gpt2medium-355m received
2024-09-18 12:20:53,827 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,867 Processed batch: ['556cc95c', 'c31e056b', '3b774e37', 'f96785d1', '037b15d0', '4b6bf83a', 'e90bdca0', '731f5eb3'] with model gpt2medium-355m in 6.9026 seconds
2024-09-18 12:20:53,867 Latency for request 556cc95c with model gpt2medium-355m: 8.9044 seconds
2024-09-18 12:20:53,867 Saving results without gpu monitoring
2024-09-18 12:20:53,871 Latency for request c31e056b with model gpt2medium-355m: 8.7530 seconds
2024-09-18 12:20:53,871 Saving results without gpu monitoring
2024-09-18 12:20:53,871 Latency for request 3b774e37 with model gpt2medium-355m: 8.3573 seconds
2024-09-18 12:20:53,871 Saving results without gpu monitoring
2024-09-18 12:20:53,871 Latency for request f96785d1 with model gpt2medium-355m: 8.3040 seconds
2024-09-18 12:20:53,871 Saving results without gpu monitoring
2024-09-18 12:20:53,872 Latency for request 037b15d0 with model gpt2medium-355m: 8.2401 seconds
2024-09-18 12:20:53,872 Saving results without gpu monitoring
2024-09-18 12:20:53,872 Latency for request 4b6bf83a with model gpt2medium-355m: 7.8335 seconds
2024-09-18 12:20:53,872 Saving results without gpu monitoring
2024-09-18 12:20:53,872 Latency for request e90bdca0 with model gpt2medium-355m: 7.1919 seconds
2024-09-18 12:20:53,872 Saving results without gpu monitoring
2024-09-18 12:20:53,872 Latency for request 731f5eb3 with model gpt2medium-355m: 7.0743 seconds
2024-09-18 12:20:53,872 Saving results without gpu monitoring
2024-09-18 12:20:53,873 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,873 Next: call load_model for distilgpt2-124m
2024-09-18 12:20:53,874 Request with ID 630dd0f8 for model gpt2medium-355m received
2024-09-18 12:20:53,874 127.0.0.1 - - [18/Sep/2024 12:20:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:53,885 Unloaded previous model
2024-09-18 12:20:53,937 Loaded model distilgpt2-124m
2024-09-18 12:20:53,937 Batch processing started for model distilgpt2-124m
2024-09-18 12:20:54,238 Request with ID 05cef06f for model distilgpt2-124m received
2024-09-18 12:20:54,238 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,319 Request with ID 44120c98 for model gpt2-124m received
2024-09-18 12:20:54,319 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,339 Request with ID 0c50a6cf for model gpt2-124m received
2024-09-18 12:20:54,340 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,409 Request with ID 7b9386f9 for model gpt2medium-355m received
2024-09-18 12:20:54,409 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,538 Request with ID a35ccabb for model gpt2-124m received
2024-09-18 12:20:54,538 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,616 Request with ID 15962f5a for model gpt2medium-355m received
2024-09-18 12:20:54,617 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,719 Request with ID 01a5056e for model distilgpt2-124m received
2024-09-18 12:20:54,719 127.0.0.1 - - [18/Sep/2024 12:20:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:54,810 Request with ID b0f03284 for model distilgpt2-124m received
2024-09-18 12:20:54,810 Batch size condition met for model distilgpt2-124m
2024-09-18 12:20:55,049 Request with ID c0a96653 for model distilgpt2-124m received
2024-09-18 12:20:55,049 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,092 Request with ID 7400ae83 for model gpt2-124m received
2024-09-18 12:20:55,092 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,115 Request with ID c6233eb8 for model gpt2medium-355m received
2024-09-18 12:20:55,115 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,201 Request with ID d1cf821c for model gpt2-124m received
2024-09-18 12:20:55,202 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,206 Request with ID 1962448c for model distilgpt2-124m received
2024-09-18 12:20:55,206 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,515 Request with ID 614d06d6 for model gpt2-124m received
2024-09-18 12:20:55,516 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,824 Request with ID 1b1539e3 for model gpt2-124m received
2024-09-18 12:20:55,824 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,910 Request with ID c96db37f for model gpt2-124m received
2024-09-18 12:20:55,910 Batch size condition met for model gpt2-124m
2024-09-18 12:20:55,917 Request with ID 55761cab for model gpt2medium-355m received
2024-09-18 12:20:55,918 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,951 Request with ID 50084db5 for model distilgpt2-124m received
2024-09-18 12:20:55,951 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,975 Processed batch: ['923f776f', '8a2e29ff', 'd2b08ca9', '662ad66a', '7319481a', '2c838db1', '11d80540', '653f5fee'] with model distilgpt2-124m in 2.0383 seconds
2024-09-18 12:20:55,975 Latency for request 923f776f with model distilgpt2-124m: 8.6613 seconds
2024-09-18 12:20:55,975 Saving results without gpu monitoring
2024-09-18 12:20:55,977 Latency for request 8a2e29ff with model distilgpt2-124m: 8.6384 seconds
2024-09-18 12:20:55,977 Saving results without gpu monitoring
2024-09-18 12:20:55,977 Latency for request d2b08ca9 with model distilgpt2-124m: 7.5909 seconds
2024-09-18 12:20:55,977 Saving results without gpu monitoring
2024-09-18 12:20:55,978 Latency for request 662ad66a with model distilgpt2-124m: 6.9294 seconds
2024-09-18 12:20:55,978 Saving results without gpu monitoring
2024-09-18 12:20:55,978 Latency for request 7319481a with model distilgpt2-124m: 6.8186 seconds
2024-09-18 12:20:55,978 Saving results without gpu monitoring
2024-09-18 12:20:55,979 Latency for request 2c838db1 with model distilgpt2-124m: 6.6022 seconds
2024-09-18 12:20:55,979 Saving results without gpu monitoring
2024-09-18 12:20:55,979 Latency for request 11d80540 with model distilgpt2-124m: 6.0468 seconds
2024-09-18 12:20:55,979 Saving results without gpu monitoring
2024-09-18 12:20:55,979 Latency for request 653f5fee with model distilgpt2-124m: 4.9050 seconds
2024-09-18 12:20:55,980 Saving results without gpu monitoring
2024-09-18 12:20:55,980 127.0.0.1 - - [18/Sep/2024 12:20:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:55,980 Next: call load_model for gpt2-124m
2024-09-18 12:20:55,986 Unloaded previous model
2024-09-18 12:20:56,035 Request with ID c7970078 for model distilgpt2-124m received
2024-09-18 12:20:56,035 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,055 Loaded model gpt2-124m
2024-09-18 12:20:56,055 Batch processing started for model gpt2-124m
2024-09-18 12:20:56,153 Request with ID feba2799 for model distilgpt2-124m received
2024-09-18 12:20:56,153 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,164 Request with ID 976b0761 for model distilgpt2-124m received
2024-09-18 12:20:56,164 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,197 Request with ID ecfe8fdc for model distilgpt2-124m received
2024-09-18 12:20:56,197 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,332 Request with ID a313982a for model gpt2medium-355m received
2024-09-18 12:20:56,332 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:56,367 Request with ID dc11b7ca for model gpt2medium-355m received
2024-09-18 12:20:56,367 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,398 Request with ID 0f7ecccc for model distilgpt2-124m received
2024-09-18 12:20:56,398 Batch size condition met for model distilgpt2-124m
2024-09-18 12:20:56,398 Request with ID 6c2b7441 for model gpt2-124m received
2024-09-18 12:20:56,399 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,421 Request with ID 07c53a6b for model distilgpt2-124m received
2024-09-18 12:20:56,422 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,644 Request with ID 8ee93417 for model distilgpt2-124m received
2024-09-18 12:20:56,644 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,654 Request with ID 4957bf49 for model distilgpt2-124m received
2024-09-18 12:20:56,655 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,697 Request with ID d3ec04fb for model distilgpt2-124m received
2024-09-18 12:20:56,697 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,731 Request with ID a55105f4 for model distilgpt2-124m received
2024-09-18 12:20:56,731 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,906 Request with ID 35aa41a2 for model gpt2medium-355m received
2024-09-18 12:20:56,906 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,921 Request with ID bb5d8450 for model gpt2-124m received
2024-09-18 12:20:56,921 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:56,982 Request with ID 334c9095 for model gpt2-124m received
2024-09-18 12:20:56,982 127.0.0.1 - - [18/Sep/2024 12:20:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,005 Request with ID 3de9aff6 for model gpt2medium-355m received
2024-09-18 12:20:57,005 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,108 Request with ID c9cb20f6 for model distilgpt2-124m received
2024-09-18 12:20:57,108 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,182 Request with ID 56047c7c for model gpt2medium-355m received
2024-09-18 12:20:57,182 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,341 Request with ID f3317211 for model gpt2medium-355m received
2024-09-18 12:20:57,341 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,401 Request with ID f8527cb4 for model distilgpt2-124m received
2024-09-18 12:20:57,401 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,402 Request with ID ddf45f2f for model distilgpt2-124m received
2024-09-18 12:20:57,402 Batch size condition met for model distilgpt2-124m
2024-09-18 12:20:57,585 Request with ID fb16aa5d for model gpt2-124m received
2024-09-18 12:20:57,586 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,592 Request with ID 33d6d048 for model gpt2-124m received
2024-09-18 12:20:57,593 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,652 Request with ID 3ff28773 for model distilgpt2-124m received
2024-09-18 12:20:57,652 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,663 Request with ID 8e8f1551 for model gpt2-124m received
2024-09-18 12:20:57,663 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,697 Request with ID 55ef9172 for model gpt2medium-355m received
2024-09-18 12:20:57,698 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,806 Request with ID 6a2eb00b for model distilgpt2-124m received
2024-09-18 12:20:57,806 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:57,939 Request with ID fd1855fd for model gpt2medium-355m received
2024-09-18 12:20:57,939 127.0.0.1 - - [18/Sep/2024 12:20:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,081 Request with ID 3754d4bb for model gpt2-124m received
2024-09-18 12:20:58,081 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,093 Request with ID 8575b7fe for model distilgpt2-124m received
2024-09-18 12:20:58,094 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,483 Request with ID 8782e4a1 for model distilgpt2-124m received
2024-09-18 12:20:58,483 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,628 Request with ID 5b02acfd for model gpt2-124m received
2024-09-18 12:20:58,628 Batch size condition met for model gpt2-124m
2024-09-18 12:20:58,693 Request with ID b8533ec3 for model gpt2medium-355m received
2024-09-18 12:20:58,693 Batch size condition met for model gpt2medium-355m
2024-09-18 12:20:58,698 Request with ID fdbd293d for model gpt2-124m received
2024-09-18 12:20:58,698 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,917 Request with ID c106d467 for model gpt2-124m received
2024-09-18 12:20:58,917 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,958 Processed batch: ['44120c98', '0c50a6cf', 'a35ccabb', '7400ae83', 'd1cf821c', '614d06d6', '1b1539e3', 'c96db37f'] with model gpt2-124m in 2.9022 seconds
2024-09-18 12:20:58,958 Latency for request 44120c98 with model gpt2-124m: 4.6388 seconds
2024-09-18 12:20:58,958 Saving results without gpu monitoring
2024-09-18 12:20:58,959 Latency for request 0c50a6cf with model gpt2-124m: 4.6182 seconds
2024-09-18 12:20:58,959 Saving results without gpu monitoring
2024-09-18 12:20:58,959 Latency for request a35ccabb with model gpt2-124m: 4.4198 seconds
2024-09-18 12:20:58,959 Saving results without gpu monitoring
2024-09-18 12:20:58,959 Latency for request 7400ae83 with model gpt2-124m: 3.8654 seconds
2024-09-18 12:20:58,959 Saving results without gpu monitoring
2024-09-18 12:20:58,960 Latency for request d1cf821c with model gpt2-124m: 3.7562 seconds
2024-09-18 12:20:58,960 Saving results without gpu monitoring
2024-09-18 12:20:58,960 Latency for request 614d06d6 with model gpt2-124m: 3.4421 seconds
2024-09-18 12:20:58,960 Saving results without gpu monitoring
2024-09-18 12:20:58,960 Latency for request 1b1539e3 with model gpt2-124m: 3.1338 seconds
2024-09-18 12:20:58,960 Saving results without gpu monitoring
2024-09-18 12:20:58,960 Latency for request c96db37f with model gpt2-124m: 3.0480 seconds
2024-09-18 12:20:58,960 Saving results without gpu monitoring
2024-09-18 12:20:58,961 127.0.0.1 - - [18/Sep/2024 12:20:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:58,961 Next: call load_model for gpt2medium-355m
2024-09-18 12:20:58,972 Unloaded previous model
2024-09-18 12:20:59,004 Request with ID 0ef0f69f for model gpt2medium-355m received
2024-09-18 12:20:59,004 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,095 Loaded model gpt2medium-355m
2024-09-18 12:20:59,095 Batch processing started for model gpt2medium-355m
2024-09-18 12:20:59,286 Request with ID c27c2dc7 for model gpt2medium-355m received
2024-09-18 12:20:59,287 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,449 Request with ID 7198fbca for model distilgpt2-124m received
2024-09-18 12:20:59,449 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,535 Request with ID 97eca65b for model gpt2-124m received
2024-09-18 12:20:59,536 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,703 Request with ID 5bf1d91b for model gpt2medium-355m received
2024-09-18 12:20:59,704 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,877 Request with ID 29707940 for model distilgpt2-124m received
2024-09-18 12:20:59,877 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,902 Request with ID 51eeb745 for model gpt2-124m received
2024-09-18 12:20:59,902 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,927 Request with ID 987aa42f for model gpt2medium-355m received
2024-09-18 12:20:59,927 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:20:59,969 Request with ID 885a9b32 for model gpt2-124m received
2024-09-18 12:20:59,969 127.0.0.1 - - [18/Sep/2024 12:20:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:00,157 Request with ID c4d3c012 for model distilgpt2-124m received
2024-09-18 12:21:00,157 127.0.0.1 - - [18/Sep/2024 12:21:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:00,328 Request with ID 6e1520b2 for model gpt2medium-355m received
2024-09-18 12:21:00,328 127.0.0.1 - - [18/Sep/2024 12:21:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:00,341 Request with ID a659f2a9 for model gpt2-124m received
2024-09-18 12:21:00,341 127.0.0.1 - - [18/Sep/2024 12:21:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:00,619 Request with ID d77ecdd4 for model gpt2-124m received
2024-09-18 12:21:00,619 127.0.0.1 - - [18/Sep/2024 12:21:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:00,962 Request with ID 7c4909fa for model distilgpt2-124m received
2024-09-18 12:21:00,962 Batch size condition met for model distilgpt2-124m
2024-09-18 12:21:01,018 Request with ID b69fac82 for model distilgpt2-124m received
2024-09-18 12:21:01,018 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,049 Request with ID d22ed2b9 for model gpt2medium-355m received
2024-09-18 12:21:01,049 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,317 Request with ID 695aa89b for model gpt2-124m received
2024-09-18 12:21:01,318 Batch size condition met for model gpt2-124m
2024-09-18 12:21:01,432 Request with ID 4e583bfb for model distilgpt2-124m received
2024-09-18 12:21:01,432 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,666 Request with ID 84c01f5f for model gpt2-124m received
2024-09-18 12:21:01,667 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,706 Request with ID d79e7874 for model gpt2medium-355m received
2024-09-18 12:21:01,706 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,757 Request with ID 77c5d4cf for model gpt2-124m received
2024-09-18 12:21:01,757 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:01,994 Request with ID 7f9edb28 for model gpt2-124m received
2024-09-18 12:21:01,994 127.0.0.1 - - [18/Sep/2024 12:21:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:02,034 Request with ID e0612b12 for model gpt2medium-355m received
2024-09-18 12:21:02,034 Batch size condition met for model gpt2medium-355m
2024-09-18 12:21:02,336 Request with ID 90c6a6a1 for model distilgpt2-124m received
2024-09-18 12:21:02,336 127.0.0.1 - - [18/Sep/2024 12:21:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:02,437 Request with ID 247ac32b for model gpt2-124m received
2024-09-18 12:21:02,437 127.0.0.1 - - [18/Sep/2024 12:21:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:02,474 Request with ID 1a9749c6 for model gpt2-124m received
2024-09-18 12:21:02,474 127.0.0.1 - - [18/Sep/2024 12:21:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:02,633 Request with ID 1d4b2f42 for model gpt2-124m received
2024-09-18 12:21:02,633 127.0.0.1 - - [18/Sep/2024 12:21:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:02,900 Request with ID 89ed357f for model gpt2medium-355m received
2024-09-18 12:21:02,900 127.0.0.1 - - [18/Sep/2024 12:21:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:03,451 Request with ID d2116490 for model gpt2-124m received
2024-09-18 12:21:03,451 127.0.0.1 - - [18/Sep/2024 12:21:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:03,587 Request with ID 19a5ec2f for model gpt2medium-355m received
2024-09-18 12:21:03,587 127.0.0.1 - - [18/Sep/2024 12:21:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:03,710 Request with ID 8b6f4f31 for model gpt2-124m received
2024-09-18 12:21:03,710 Batch size condition met for model gpt2-124m
2024-09-18 12:21:03,855 Request with ID 98f0bfb6 for model distilgpt2-124m received
2024-09-18 12:21:03,855 127.0.0.1 - - [18/Sep/2024 12:21:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:03,977 Request with ID 28e59706 for model distilgpt2-124m received
2024-09-18 12:21:03,977 127.0.0.1 - - [18/Sep/2024 12:21:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,201 Request with ID ab9df1d0 for model gpt2-124m received
2024-09-18 12:21:04,201 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,274 Request with ID 61f19316 for model gpt2medium-355m received
2024-09-18 12:21:04,274 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,363 Request with ID 32c7f72d for model gpt2-124m received
2024-09-18 12:21:04,363 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,576 Request with ID b987103a for model gpt2medium-355m received
2024-09-18 12:21:04,577 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,815 Request with ID d6faf8ab for model distilgpt2-124m received
2024-09-18 12:21:04,815 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,939 Request with ID 029f31b8 for model gpt2-124m received
2024-09-18 12:21:04,939 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:04,963 Total time: 13.9983 seconds
2024-09-18 12:21:04,964 Total inference time: 11.8431 seconds
2024-09-18 12:21:04,964 Inference time as percentage of total time: 84.60%
2024-09-18 12:21:04,964 END
2024-09-18 12:21:04,964 127.0.0.1 - - [18/Sep/2024 12:21:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:05,493 Processed batch: ['dc11b7ca', '35aa41a2', '3de9aff6', '56047c7c', 'f3317211', '55ef9172', 'fd1855fd', 'b8533ec3'] with model gpt2medium-355m in 6.3977 seconds
2024-09-18 12:21:05,493 Latency for request dc11b7ca with model gpt2medium-355m: 9.1253 seconds
2024-09-18 12:21:05,493 Saving results without gpu monitoring
2024-09-18 12:21:05,494 Latency for request 35aa41a2 with model gpt2medium-355m: 8.5869 seconds
2024-09-18 12:21:05,494 Saving results without gpu monitoring
2024-09-18 12:21:05,494 Latency for request 3de9aff6 with model gpt2medium-355m: 8.4875 seconds
2024-09-18 12:21:05,494 Saving results without gpu monitoring
2024-09-18 12:21:05,494 Latency for request 56047c7c with model gpt2medium-355m: 8.3107 seconds
2024-09-18 12:21:05,494 Saving results without gpu monitoring
2024-09-18 12:21:05,495 Latency for request f3317211 with model gpt2medium-355m: 8.1514 seconds
2024-09-18 12:21:05,495 Saving results without gpu monitoring
2024-09-18 12:21:05,495 Latency for request 55ef9172 with model gpt2medium-355m: 7.7953 seconds
2024-09-18 12:21:05,495 Saving results without gpu monitoring
2024-09-18 12:21:05,495 Latency for request fd1855fd with model gpt2medium-355m: 7.5533 seconds
2024-09-18 12:21:05,495 Saving results without gpu monitoring
2024-09-18 12:21:05,495 Latency for request b8533ec3 with model gpt2medium-355m: 6.7994 seconds
2024-09-18 12:21:05,495 Saving results without gpu monitoring
2024-09-18 12:21:05,496 127.0.0.1 - - [18/Sep/2024 12:21:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:05,496 Next: call load_model for gpt2medium-355m
2024-09-18 12:21:05,496 Model gpt2medium-355m already loaded
2024-09-18 12:21:05,496 Batch processing started for model gpt2medium-355m
2024-09-18 12:21:11,599 Processed batch: ['0ef0f69f', 'c27c2dc7', '5bf1d91b', '987aa42f', '6e1520b2', 'd22ed2b9', 'd79e7874', 'e0612b12'] with model gpt2medium-355m in 6.1031 seconds
2024-09-18 12:21:11,599 Latency for request 0ef0f69f with model gpt2medium-355m: 12.5951 seconds
2024-09-18 12:21:11,599 Saving results without gpu monitoring
2024-09-18 12:21:11,600 Latency for request c27c2dc7 with model gpt2medium-355m: 12.3125 seconds
2024-09-18 12:21:11,600 Saving results without gpu monitoring
2024-09-18 12:21:11,601 Latency for request 5bf1d91b with model gpt2medium-355m: 11.8955 seconds
2024-09-18 12:21:11,601 Saving results without gpu monitoring
2024-09-18 12:21:11,601 Latency for request 987aa42f with model gpt2medium-355m: 11.6724 seconds
2024-09-18 12:21:11,601 Saving results without gpu monitoring
2024-09-18 12:21:11,601 Latency for request 6e1520b2 with model gpt2medium-355m: 11.2714 seconds
2024-09-18 12:21:11,601 Saving results without gpu monitoring
2024-09-18 12:21:11,602 Latency for request d22ed2b9 with model gpt2medium-355m: 10.5504 seconds
2024-09-18 12:21:11,602 Saving results without gpu monitoring
2024-09-18 12:21:11,602 Latency for request d79e7874 with model gpt2medium-355m: 9.8927 seconds
2024-09-18 12:21:11,602 Saving results without gpu monitoring
2024-09-18 12:21:11,602 Latency for request e0612b12 with model gpt2medium-355m: 9.5652 seconds
2024-09-18 12:21:11,602 Saving results without gpu monitoring
2024-09-18 12:21:11,602 127.0.0.1 - - [18/Sep/2024 12:21:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:11,602 Next: call load_model for gpt2-124m
2024-09-18 12:21:11,615 Unloaded previous model
2024-09-18 12:21:11,684 Loaded model gpt2-124m
2024-09-18 12:21:11,684 Batch processing started for model gpt2-124m
2024-09-18 12:21:13,995 Processed batch: ['84c01f5f', '77c5d4cf', '7f9edb28', '247ac32b', '1a9749c6', '1d4b2f42', 'd2116490', '8b6f4f31'] with model gpt2-124m in 2.3108 seconds
2024-09-18 12:21:13,995 Latency for request 84c01f5f with model gpt2-124m: 12.3286 seconds
2024-09-18 12:21:13,995 Saving results without gpu monitoring
2024-09-18 12:21:13,996 Latency for request 77c5d4cf with model gpt2-124m: 12.2378 seconds
2024-09-18 12:21:13,996 Saving results without gpu monitoring
2024-09-18 12:21:13,996 Latency for request 7f9edb28 with model gpt2-124m: 12.0007 seconds
2024-09-18 12:21:13,996 Saving results without gpu monitoring
2024-09-18 12:21:13,997 Latency for request 247ac32b with model gpt2-124m: 11.5583 seconds
2024-09-18 12:21:13,997 Saving results without gpu monitoring
2024-09-18 12:21:13,997 Latency for request 1a9749c6 with model gpt2-124m: 11.5211 seconds
2024-09-18 12:21:13,997 Saving results without gpu monitoring
2024-09-18 12:21:13,997 Latency for request 1d4b2f42 with model gpt2-124m: 11.3625 seconds
2024-09-18 12:21:13,997 Saving results without gpu monitoring
2024-09-18 12:21:13,997 Latency for request d2116490 with model gpt2-124m: 10.5437 seconds
2024-09-18 12:21:13,997 Saving results without gpu monitoring
2024-09-18 12:21:13,998 Latency for request 8b6f4f31 with model gpt2-124m: 10.2848 seconds
2024-09-18 12:21:13,998 Saving results without gpu monitoring
2024-09-18 12:21:13,998 127.0.0.1 - - [18/Sep/2024 12:21:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:13,998 Next: call load_model for distilgpt2-124m
2024-09-18 12:21:14,008 Unloaded previous model
2024-09-18 12:21:14,115 Loaded model distilgpt2-124m
2024-09-18 12:21:14,115 Batch processing started for model distilgpt2-124m
2024-09-18 12:21:15,886 Processed batch: ['3ff28773', '6a2eb00b', '8575b7fe', '8782e4a1', '7198fbca', '29707940', 'c4d3c012', '7c4909fa'] with model distilgpt2-124m in 1.7705 seconds
2024-09-18 12:21:15,886 Latency for request 3ff28773 with model distilgpt2-124m: 18.2338 seconds
2024-09-18 12:21:15,886 Saving results without gpu monitoring
2024-09-18 12:21:15,887 Latency for request 6a2eb00b with model distilgpt2-124m: 18.0800 seconds
2024-09-18 12:21:15,887 Saving results without gpu monitoring
2024-09-18 12:21:15,887 Latency for request 8575b7fe with model distilgpt2-124m: 17.7926 seconds
2024-09-18 12:21:15,887 Saving results without gpu monitoring
2024-09-18 12:21:15,888 Latency for request 8782e4a1 with model distilgpt2-124m: 17.4033 seconds
2024-09-18 12:21:15,888 Saving results without gpu monitoring
2024-09-18 12:21:15,888 Latency for request 7198fbca with model distilgpt2-124m: 16.4370 seconds
2024-09-18 12:21:15,888 Saving results without gpu monitoring
2024-09-18 12:21:15,888 Latency for request 29707940 with model distilgpt2-124m: 16.0091 seconds
2024-09-18 12:21:15,888 Saving results without gpu monitoring
2024-09-18 12:21:15,888 Latency for request c4d3c012 with model distilgpt2-124m: 15.7291 seconds
2024-09-18 12:21:15,888 Saving results without gpu monitoring
2024-09-18 12:21:15,889 Latency for request 7c4909fa with model distilgpt2-124m: 14.9238 seconds
2024-09-18 12:21:15,889 Saving results without gpu monitoring
2024-09-18 12:21:15,889 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,889 No batch to process for model gpt2medium-355m
2024-09-18 12:21:15,889 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,889 No batch to process for model gpt2-124m
2024-09-18 12:21:15,890 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,890 No batch to process for model distilgpt2-124m
2024-09-18 12:21:15,890 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,890 No batch to process for model gpt2-124m
2024-09-18 12:21:15,890 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,890 No batch to process for model gpt2medium-355m
2024-09-18 12:21:15,890 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,890 No batch to process for model distilgpt2-124m
2024-09-18 12:21:15,890 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model distilgpt2-124m
2024-09-18 12:21:15,891 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model gpt2-124m
2024-09-18 12:21:15,891 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model gpt2medium-355m
2024-09-18 12:21:15,891 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model distilgpt2-124m
2024-09-18 12:21:15,891 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model gpt2-124m
2024-09-18 12:21:15,891 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,891 No batch to process for model gpt2medium-355m
2024-09-18 12:21:15,892 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:21:15,892 No batch to process for model gpt2-124m
2024-09-18 12:21:15,892 127.0.0.1 - - [18/Sep/2024 12:21:15] "POST /inference HTTP/1.1" 200 -
