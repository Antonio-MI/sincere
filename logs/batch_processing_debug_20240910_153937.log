2024-09-10 15:39:42,408 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 15:39:42,408 [33mPress CTRL+C to quit[0m
2024-09-10 15:39:43,759 Request with ID c7c6cfae for model gpt2-124m received
2024-09-10 15:39:43,759 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:39:43,759 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 15:39:43,759 127.0.0.1 - - [10/Sep/2024 15:39:43] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:44,258 Request with ID 9b6283dc for model gpt2-124m received
2024-09-10 15:39:44,259 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:39:44,259 127.0.0.1 - - [10/Sep/2024 15:39:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:44,459 Request with ID dc70cb91 for model gpt2medium-355m received
2024-09-10 15:39:44,459 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:39:44,459 Adjusted time limit for model gpt2medium-355m: 11.8866 seconds
2024-09-10 15:39:44,459 127.0.0.1 - - [10/Sep/2024 15:39:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:44,495 Request with ID e67f31ad for model gpt2-124m received
2024-09-10 15:39:44,495 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:44,495 127.0.0.1 - - [10/Sep/2024 15:39:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:44,720 Request with ID 905d54c7 for model gpt2-124m received
2024-09-10 15:39:44,720 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:44,721 Batch size condition met for model gpt2-124m
2024-09-10 15:39:44,721 Updated batch size:4
2024-09-10 15:39:44,721 Loading model gpt2-124m
2024-09-10 15:39:45,117 Request with ID eca5b0ad for model distilgpt2-124m received
2024-09-10 15:39:45,117 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:39:45,117 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 15:39:45,117 127.0.0.1 - - [10/Sep/2024 15:39:45] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:45,483 Request with ID f5abab13 for model distilgpt2-124m received
2024-09-10 15:39:45,483 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:39:45,483 127.0.0.1 - - [10/Sep/2024 15:39:45] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:45,534 Processed batch: ['c7c6cfae', '9b6283dc', 'e67f31ad', '905d54c7'] with model gpt2-124m in 0.6988 seconds
2024-09-10 15:39:45,534 Latency for request c7c6cfae with model gpt2-124m: 1.7743 seconds
2024-09-10 15:39:45,535 Latency for request 9b6283dc with model gpt2-124m: 1.2753 seconds
2024-09-10 15:39:45,536 Latency for request e67f31ad with model gpt2-124m: 1.0385 seconds
2024-09-10 15:39:45,536 Latency for request 905d54c7 with model gpt2-124m: 0.8135 seconds
2024-09-10 15:39:45,536 127.0.0.1 - - [10/Sep/2024 15:39:45] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:45,824 Request with ID b438c4d9 for model gpt2medium-355m received
2024-09-10 15:39:45,824 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:45,824 127.0.0.1 - - [10/Sep/2024 15:39:45] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:46,062 Request with ID 40411dde for model distilgpt2-124m received
2024-09-10 15:39:46,063 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:46,063 127.0.0.1 - - [10/Sep/2024 15:39:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:46,338 Request with ID 188c50a9 for model gpt2medium-355m received
2024-09-10 15:39:46,338 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:39:46,339 127.0.0.1 - - [10/Sep/2024 15:39:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:46,442 Time limit condition met for model gpt2medium-355m
2024-09-10 15:39:46,442 Updated batch size:4
2024-09-10 15:39:46,442 Loading model gpt2medium-355m
2024-09-10 15:39:46,629 Request with ID 6b50ec90 for model gpt2-124m received
2024-09-10 15:39:46,629 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:46,629 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:39:46,629 127.0.0.1 - - [10/Sep/2024 15:39:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:46,996 Request with ID a04cc0c7 for model gpt2medium-355m received
2024-09-10 15:39:46,996 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:46,996 127.0.0.1 - - [10/Sep/2024 15:39:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:47,214 Request with ID a67a8dbc for model gpt2-124m received
2024-09-10 15:39:47,214 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:39:47,214 127.0.0.1 - - [10/Sep/2024 15:39:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:47,417 Request with ID fedbe575 for model distilgpt2-124m received
2024-09-10 15:39:47,417 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:39:47,417 Batch size condition met for model distilgpt2-124m
2024-09-10 15:39:48,061 Request with ID 5169b246 for model gpt2-124m received
2024-09-10 15:39:48,061 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:48,061 127.0.0.1 - - [10/Sep/2024 15:39:48] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:48,781 Request with ID ef24728c for model gpt2-124m received
2024-09-10 15:39:48,781 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:48,781 Batch size condition met for model gpt2-124m
2024-09-10 15:39:49,153 Processed batch: ['dc70cb91', 'b438c4d9', '188c50a9', 'ea6a'] with model gpt2medium-355m in 2.5780 seconds
2024-09-10 15:39:49,153 Latency for request dc70cb91 with model gpt2medium-355m: 4.6939 seconds
2024-09-10 15:39:49,153 Latency for request b438c4d9 with model gpt2medium-355m: 3.3285 seconds
2024-09-10 15:39:49,154 Latency for request 188c50a9 with model gpt2medium-355m: 2.8150 seconds
2024-09-10 15:39:49,154 Latency for request ea6a with model gpt2medium-355m: 2.7104 seconds
2024-09-10 15:39:49,154 Updated batch size:4
2024-09-10 15:39:49,154 Loading model distilgpt2-124m
2024-09-10 15:39:49,222 Request with ID 95bda981 for model distilgpt2-124m received
2024-09-10 15:39:49,222 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:39:49,222 127.0.0.1 - - [10/Sep/2024 15:39:49] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:49,799 Processed batch: ['eca5b0ad', 'f5abab13', '40411dde', 'fedbe575'] with model distilgpt2-124m in 0.5906 seconds
2024-09-10 15:39:49,799 Latency for request eca5b0ad with model distilgpt2-124m: 4.6821 seconds
2024-09-10 15:39:49,800 Latency for request f5abab13 with model distilgpt2-124m: 4.3162 seconds
2024-09-10 15:39:49,800 Latency for request 40411dde with model distilgpt2-124m: 3.7370 seconds
2024-09-10 15:39:49,800 Latency for request fedbe575 with model distilgpt2-124m: 2.3823 seconds
2024-09-10 15:39:49,801 127.0.0.1 - - [10/Sep/2024 15:39:49] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:49,801 Updated batch size:4
2024-09-10 15:39:49,801 Loading model gpt2-124m
2024-09-10 15:39:50,716 Processed batch: ['6b50ec90', 'a67a8dbc', '5169b246', 'ef24728c'] with model gpt2-124m in 0.8511 seconds
2024-09-10 15:39:50,716 Latency for request 6b50ec90 with model gpt2-124m: 4.0877 seconds
2024-09-10 15:39:50,717 Latency for request a67a8dbc with model gpt2-124m: 3.5022 seconds
2024-09-10 15:39:50,717 Latency for request 5169b246 with model gpt2-124m: 2.6556 seconds
2024-09-10 15:39:50,718 Latency for request ef24728c with model gpt2-124m: 1.9356 seconds
2024-09-10 15:39:50,718 127.0.0.1 - - [10/Sep/2024 15:39:50] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:50,912 Request with ID c58f50b1 for model gpt2-124m received
2024-09-10 15:39:50,912 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:39:50,912 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:39:50,912 127.0.0.1 - - [10/Sep/2024 15:39:50] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:51,662 Request with ID 1bda8821 for model gpt2medium-355m received
2024-09-10 15:39:51,662 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:51,662 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:39:51,663 127.0.0.1 - - [10/Sep/2024 15:39:51] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:52,228 Request with ID 322c4ea7 for model gpt2medium-355m received
2024-09-10 15:39:52,228 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:52,229 127.0.0.1 - - [10/Sep/2024 15:39:52] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:52,265 Time limit condition met for model gpt2medium-355m
2024-09-10 15:39:52,265 Updated batch size:4
2024-09-10 15:39:52,265 Loading model gpt2medium-355m
2024-09-10 15:39:52,606 Request with ID a2a76182 for model gpt2medium-355m received
2024-09-10 15:39:52,606 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:39:52,606 127.0.0.1 - - [10/Sep/2024 15:39:52] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:53,301 Request with ID 5e9208bf for model gpt2medium-355m received
2024-09-10 15:39:53,301 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:53,301 127.0.0.1 - - [10/Sep/2024 15:39:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:53,726 Request with ID e0a783b9 for model gpt2-124m received
2024-09-10 15:39:53,726 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:53,726 127.0.0.1 - - [10/Sep/2024 15:39:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:53,971 Request with ID 96b45158 for model distilgpt2-124m received
2024-09-10 15:39:53,971 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:39:53,971 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:39:53,971 127.0.0.1 - - [10/Sep/2024 15:39:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:54,608 Request with ID e7204455 for model distilgpt2-124m received
2024-09-10 15:39:54,608 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:39:54,608 127.0.0.1 - - [10/Sep/2024 15:39:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:55,098 Processed batch: ['a04cc0c7', '1bda8821', '322c4ea7', '5fc8'] with model gpt2medium-355m in 2.6631 seconds
2024-09-10 15:39:55,099 Latency for request a04cc0c7 with model gpt2medium-355m: 8.1027 seconds
2024-09-10 15:39:55,100 Latency for request 1bda8821 with model gpt2medium-355m: 3.4369 seconds
2024-09-10 15:39:55,100 Latency for request 322c4ea7 with model gpt2medium-355m: 2.8707 seconds
2024-09-10 15:39:55,101 Latency for request 5fc8 with model gpt2medium-355m: 2.8330 seconds
2024-09-10 15:39:55,150 Request with ID 090c4856 for model distilgpt2-124m received
2024-09-10 15:39:55,150 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:39:55,150 Batch size condition met for model distilgpt2-124m
2024-09-10 15:39:55,150 Updated batch size:4
2024-09-10 15:39:55,150 Loading model distilgpt2-124m
2024-09-10 15:39:55,252 Request with ID 968eb03e for model gpt2-124m received
2024-09-10 15:39:55,252 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:55,252 127.0.0.1 - - [10/Sep/2024 15:39:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:55,934 Request with ID b11376ad for model gpt2medium-355m received
2024-09-10 15:39:55,935 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:39:55,935 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:39:55,935 127.0.0.1 - - [10/Sep/2024 15:39:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:55,975 Processed batch: ['95bda981', '96b45158', 'e7204455', '090c4856'] with model distilgpt2-124m in 0.7186 seconds
2024-09-10 15:39:55,975 Latency for request 95bda981 with model distilgpt2-124m: 6.7533 seconds
2024-09-10 15:39:55,977 Latency for request 96b45158 with model distilgpt2-124m: 2.0040 seconds
2024-09-10 15:39:55,977 Latency for request e7204455 with model distilgpt2-124m: 1.3671 seconds
2024-09-10 15:39:55,977 Latency for request 090c4856 with model distilgpt2-124m: 0.8252 seconds
2024-09-10 15:39:55,978 127.0.0.1 - - [10/Sep/2024 15:39:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:56,404 Request with ID f3461135 for model gpt2medium-355m received
2024-09-10 15:39:56,406 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:39:56,407 Batch size condition met for model gpt2medium-355m
2024-09-10 15:39:56,407 Updated batch size:4
2024-09-10 15:39:56,407 Loading model gpt2medium-355m
2024-09-10 15:39:56,483 Request with ID 68cb3a0c for model gpt2medium-355m received
2024-09-10 15:39:56,483 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:56,484 127.0.0.1 - - [10/Sep/2024 15:39:56] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:56,506 Time limit condition met for model gpt2medium-355m
2024-09-10 15:39:57,353 Request with ID 7e994618 for model gpt2medium-355m received
2024-09-10 15:39:57,353 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:57,353 127.0.0.1 - - [10/Sep/2024 15:39:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:57,782 Request with ID 811443b5 for model gpt2-124m received
2024-09-10 15:39:57,782 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:57,782 Batch size condition met for model gpt2-124m
2024-09-10 15:39:58,140 Request with ID 8c9b4c8f for model gpt2medium-355m received
2024-09-10 15:39:58,140 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:39:58,140 127.0.0.1 - - [10/Sep/2024 15:39:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:58,403 Request with ID 7a1fdbb8 for model gpt2medium-355m received
2024-09-10 15:39:58,403 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:39:58,404 127.0.0.1 - - [10/Sep/2024 15:39:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:58,573 Request with ID b68f39d0 for model gpt2-124m received
2024-09-10 15:39:58,573 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:39:58,573 127.0.0.1 - - [10/Sep/2024 15:39:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:59,139 Request with ID b6699b6c for model gpt2medium-355m received
2024-09-10 15:39:59,139 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:39:59,139 Batch size condition met for model gpt2medium-355m
2024-09-10 15:39:59,341 Processed batch: ['a2a76182', '5e9208bf', 'b11376ad', 'f3461135'] with model gpt2medium-355m in 2.7596 seconds
2024-09-10 15:39:59,341 Latency for request a2a76182 with model gpt2medium-355m: 6.7349 seconds
2024-09-10 15:39:59,342 Latency for request 5e9208bf with model gpt2medium-355m: 6.0400 seconds
2024-09-10 15:39:59,342 Latency for request b11376ad with model gpt2medium-355m: 3.4061 seconds
2024-09-10 15:39:59,342 Latency for request f3461135 with model gpt2medium-355m: 2.9362 seconds
2024-09-10 15:39:59,342 127.0.0.1 - - [10/Sep/2024 15:39:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:39:59,342 Updated batch size:4
2024-09-10 15:39:59,343 Loading model gpt2medium-355m
2024-09-10 15:39:59,770 Request with ID 81162fb6 for model distilgpt2-124m received
2024-09-10 15:39:59,770 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:39:59,770 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:39:59,770 127.0.0.1 - - [10/Sep/2024 15:39:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:00,198 Request with ID a8f03a72 for model gpt2-124m received
2024-09-10 15:40:00,198 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:00,198 127.0.0.1 - - [10/Sep/2024 15:40:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:00,369 Request with ID 12338096 for model gpt2medium-355m received
2024-09-10 15:40:00,369 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:00,369 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:40:00,370 127.0.0.1 - - [10/Sep/2024 15:40:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:00,619 Request with ID 2e27abaa for model gpt2-124m received
2024-09-10 15:40:00,620 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:00,620 127.0.0.1 - - [10/Sep/2024 15:40:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:00,632 Request with ID 8d4a4486 for model gpt2-124m received
2024-09-10 15:40:00,632 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:00,632 Batch size condition met for model gpt2-124m
2024-09-10 15:40:01,064 Request with ID 351b0320 for model gpt2-124m received
2024-09-10 15:40:01,064 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:01,064 127.0.0.1 - - [10/Sep/2024 15:40:01] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:01,288 Request with ID da69217f for model distilgpt2-124m received
2024-09-10 15:40:01,288 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:01,289 127.0.0.1 - - [10/Sep/2024 15:40:01] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:01,497 Request with ID 56973fa5 for model distilgpt2-124m received
2024-09-10 15:40:01,497 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:01,497 127.0.0.1 - - [10/Sep/2024 15:40:01] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:02,090 Request with ID f3c94bfe for model distilgpt2-124m received
2024-09-10 15:40:02,090 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:02,090 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:02,220 Request with ID 986a1204 for model distilgpt2-124m received
2024-09-10 15:40:02,220 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:02,220 127.0.0.1 - - [10/Sep/2024 15:40:02] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:02,247 Processed batch: ['7e994618', '8c9b4c8f', '7a1fdbb8', 'b6699b6c'] with model gpt2medium-355m in 2.9043 seconds
2024-09-10 15:40:02,247 Latency for request 7e994618 with model gpt2medium-355m: 4.8943 seconds
2024-09-10 15:40:02,248 Latency for request 8c9b4c8f with model gpt2medium-355m: 4.1069 seconds
2024-09-10 15:40:02,248 Latency for request 7a1fdbb8 with model gpt2medium-355m: 3.8436 seconds
2024-09-10 15:40:02,248 Latency for request b6699b6c with model gpt2medium-355m: 3.1079 seconds
2024-09-10 15:40:02,248 Updated batch size:4
2024-09-10 15:40:02,248 Loading model gpt2-124m
2024-09-10 15:40:02,353 Time limit condition met for model gpt2-124m
2024-09-10 15:40:02,570 Request with ID cf220e28 for model distilgpt2-124m received
2024-09-10 15:40:02,570 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:02,571 127.0.0.1 - - [10/Sep/2024 15:40:02] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:02,778 Request with ID 663b2f86 for model distilgpt2-124m received
2024-09-10 15:40:02,778 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:02,778 127.0.0.1 - - [10/Sep/2024 15:40:02] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,015 Request with ID 845a87a1 for model distilgpt2-124m received
2024-09-10 15:40:03,015 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:03,015 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:03,150 Processed batch: ['b68f39d0', 'a8f03a72', '2e27abaa', '8d4a4486'] with model gpt2-124m in 0.8205 seconds
2024-09-10 15:40:03,150 Latency for request b68f39d0 with model gpt2-124m: 4.5777 seconds
2024-09-10 15:40:03,152 Latency for request a8f03a72 with model gpt2-124m: 2.9527 seconds
2024-09-10 15:40:03,152 Latency for request 2e27abaa with model gpt2-124m: 2.5309 seconds
2024-09-10 15:40:03,152 Latency for request 8d4a4486 with model gpt2-124m: 2.5188 seconds
2024-09-10 15:40:03,153 127.0.0.1 - - [10/Sep/2024 15:40:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,153 No batch to process for model gpt2medium-355m
2024-09-10 15:40:03,153 127.0.0.1 - - [10/Sep/2024 15:40:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,153 Updated batch size:1
2024-09-10 15:40:03,153 Loading model gpt2-124m
2024-09-10 15:40:03,297 Request with ID 05d6f4c6 for model distilgpt2-124m received
2024-09-10 15:40:03,297 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:03,298 127.0.0.1 - - [10/Sep/2024 15:40:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,485 Request with ID 887c7b9f for model distilgpt2-124m received
2024-09-10 15:40:03,485 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:03,485 127.0.0.1 - - [10/Sep/2024 15:40:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,581 Processed batch: ['351b0320'] with model gpt2-124m in 0.4279 seconds
2024-09-10 15:40:03,581 Latency for request 351b0320 with model gpt2-124m: 2.5171 seconds
2024-09-10 15:40:03,582 127.0.0.1 - - [10/Sep/2024 15:40:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:03,582 Updated batch size:4
2024-09-10 15:40:03,582 Loading model distilgpt2-124m
2024-09-10 15:40:04,154 Processed batch: ['986a1204', 'cf220e28', '663b2f86', '845a87a1'] with model distilgpt2-124m in 0.5167 seconds
2024-09-10 15:40:04,154 Latency for request 986a1204 with model distilgpt2-124m: 1.9341 seconds
2024-09-10 15:40:04,155 Latency for request cf220e28 with model distilgpt2-124m: 1.5834 seconds
2024-09-10 15:40:04,155 Latency for request 663b2f86 with model distilgpt2-124m: 1.3761 seconds
2024-09-10 15:40:04,155 Latency for request 845a87a1 with model distilgpt2-124m: 1.1388 seconds
2024-09-10 15:40:04,156 127.0.0.1 - - [10/Sep/2024 15:40:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:04,156 No batch to process for model gpt2-124m
2024-09-10 15:40:04,156 No batch to process for model distilgpt2-124m
2024-09-10 15:40:04,156 127.0.0.1 - - [10/Sep/2024 15:40:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:04,243 Request with ID e91356ca for model gpt2medium-355m received
2024-09-10 15:40:04,243 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:04,243 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:40:04,243 127.0.0.1 - - [10/Sep/2024 15:40:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:04,487 Request with ID 6bbbe197 for model gpt2-124m received
2024-09-10 15:40:04,487 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:04,487 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:40:04,487 127.0.0.1 - - [10/Sep/2024 15:40:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:05,485 Request with ID 312debed for model gpt2-124m received
2024-09-10 15:40:05,485 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:05,485 127.0.0.1 - - [10/Sep/2024 15:40:05] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:06,102 Request with ID dd0222ae for model distilgpt2-124m received
2024-09-10 15:40:06,103 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:06,103 Adjusted time limit for model distilgpt2-124m: 14.1819 seconds
2024-09-10 15:40:06,103 127.0.0.1 - - [10/Sep/2024 15:40:06] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:06,596 Request with ID 30c527e4 for model distilgpt2-124m received
2024-09-10 15:40:06,597 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:06,597 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:06,597 Updated batch size:4
2024-09-10 15:40:06,597 Loading model distilgpt2-124m
2024-09-10 15:40:06,744 Request with ID 7d3c5c52 for model gpt2-124m received
2024-09-10 15:40:06,744 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:06,744 127.0.0.1 - - [10/Sep/2024 15:40:06] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:07,201 Processed batch: ['05d6f4c6', '887c7b9f', 'dd0222ae', '30c527e4'] with model distilgpt2-124m in 0.6035 seconds
2024-09-10 15:40:07,201 Latency for request 05d6f4c6 with model distilgpt2-124m: 3.9038 seconds
2024-09-10 15:40:07,202 Latency for request 887c7b9f with model distilgpt2-124m: 3.7164 seconds
2024-09-10 15:40:07,202 Latency for request dd0222ae with model distilgpt2-124m: 1.0989 seconds
2024-09-10 15:40:07,202 Latency for request 30c527e4 with model distilgpt2-124m: 0.6049 seconds
2024-09-10 15:40:07,203 127.0.0.1 - - [10/Sep/2024 15:40:07] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:07,336 Request with ID 382498bc for model gpt2-124m received
2024-09-10 15:40:07,336 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:07,336 Batch size condition met for model gpt2-124m
2024-09-10 15:40:07,336 Updated batch size:4
2024-09-10 15:40:07,336 Loading model gpt2-124m
2024-09-10 15:40:07,443 Request with ID 180df203 for model gpt2-124m received
2024-09-10 15:40:07,443 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:07,443 127.0.0.1 - - [10/Sep/2024 15:40:07] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:07,701 Request with ID e5dd7d66 for model gpt2-124m received
2024-09-10 15:40:07,701 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:07,701 127.0.0.1 - - [10/Sep/2024 15:40:07] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:07,985 Request with ID 39aeb7ed for model gpt2medium-355m received
2024-09-10 15:40:07,985 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:07,985 127.0.0.1 - - [10/Sep/2024 15:40:07] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:08,004 Time limit condition met for model gpt2medium-355m
2024-09-10 15:40:08,182 Processed batch: ['6bbbe197', '312debed', '7d3c5c52', '382498bc'] with model gpt2-124m in 0.7782 seconds
2024-09-10 15:40:08,182 Latency for request 6bbbe197 with model gpt2-124m: 3.6951 seconds
2024-09-10 15:40:08,183 Latency for request 312debed with model gpt2-124m: 2.6969 seconds
2024-09-10 15:40:08,184 Latency for request 7d3c5c52 with model gpt2-124m: 1.4376 seconds
2024-09-10 15:40:08,184 Latency for request 382498bc with model gpt2-124m: 0.8461 seconds
2024-09-10 15:40:08,184 127.0.0.1 - - [10/Sep/2024 15:40:08] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:08,184 Updated batch size:4
2024-09-10 15:40:08,184 Loading model gpt2medium-355m
2024-09-10 15:40:08,636 Request with ID a667a086 for model gpt2medium-355m received
2024-09-10 15:40:08,636 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:08,636 127.0.0.1 - - [10/Sep/2024 15:40:08] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:08,765 Request with ID 4bf5d6ee for model gpt2-124m received
2024-09-10 15:40:08,765 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:08,765 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:40:08,765 127.0.0.1 - - [10/Sep/2024 15:40:08] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:08,958 Request with ID 8a0630d4 for model gpt2-124m received
2024-09-10 15:40:08,958 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:08,958 Batch size condition met for model gpt2-124m
2024-09-10 15:40:09,353 Request with ID 64c06239 for model distilgpt2-124m received
2024-09-10 15:40:09,353 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:09,353 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:40:09,353 127.0.0.1 - - [10/Sep/2024 15:40:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:09,735 Request with ID d0daded8 for model gpt2-124m received
2024-09-10 15:40:09,735 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:09,735 127.0.0.1 - - [10/Sep/2024 15:40:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:09,844 Request with ID e093f771 for model gpt2medium-355m received
2024-09-10 15:40:09,844 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:09,844 127.0.0.1 - - [10/Sep/2024 15:40:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:10,177 Request with ID 34a3ec30 for model distilgpt2-124m received
2024-09-10 15:40:10,177 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:10,178 127.0.0.1 - - [10/Sep/2024 15:40:10] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:10,422 Request with ID 09a30d6f for model gpt2-124m received
2024-09-10 15:40:10,423 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:10,423 127.0.0.1 - - [10/Sep/2024 15:40:10] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:10,566 Request with ID f1fa9bc1 for model gpt2medium-355m received
2024-09-10 15:40:10,566 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:10,566 127.0.0.1 - - [10/Sep/2024 15:40:10] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:11,275 Request with ID 5e64e991 for model gpt2-124m received
2024-09-10 15:40:11,275 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:11,275 127.0.0.1 - - [10/Sep/2024 15:40:11] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:11,298 Processed batch: ['12338096', 'e91356ca', '39aeb7ed', '1433'] with model gpt2medium-355m in 2.9821 seconds
2024-09-10 15:40:11,298 Latency for request 12338096 with model gpt2medium-355m: 10.9287 seconds
2024-09-10 15:40:11,299 Latency for request e91356ca with model gpt2medium-355m: 7.0550 seconds
2024-09-10 15:40:11,299 Latency for request 39aeb7ed with model gpt2medium-355m: 3.3130 seconds
2024-09-10 15:40:11,299 Latency for request 1433 with model gpt2medium-355m: 3.1137 seconds
2024-09-10 15:40:11,300 Updated batch size:4
2024-09-10 15:40:11,300 Loading model gpt2-124m
2024-09-10 15:40:11,892 Request with ID f791f065 for model gpt2-124m received
2024-09-10 15:40:11,892 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-10 15:40:11,892 Batch size condition met for model gpt2-124m
2024-09-10 15:40:12,168 Request with ID 2bfc1fc6 for model gpt2medium-355m received
2024-09-10 15:40:12,168 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:12,168 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:40:12,169 Batch size condition met for model gpt2medium-355m
2024-09-10 15:40:12,215 Processed batch: ['180df203', 'e5dd7d66', '4bf5d6ee', '8a0630d4'] with model gpt2-124m in 0.8385 seconds
2024-09-10 15:40:12,215 Latency for request 180df203 with model gpt2-124m: 4.7717 seconds
2024-09-10 15:40:12,216 Latency for request e5dd7d66 with model gpt2-124m: 4.5139 seconds
2024-09-10 15:40:12,216 Latency for request 4bf5d6ee with model gpt2-124m: 3.4502 seconds
2024-09-10 15:40:12,216 Latency for request 8a0630d4 with model gpt2-124m: 3.2568 seconds
2024-09-10 15:40:12,217 127.0.0.1 - - [10/Sep/2024 15:40:12] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:12,217 Updated batch size:4
2024-09-10 15:40:12,217 Loading model gpt2-124m
2024-09-10 15:40:12,415 Request with ID 02c3ed40 for model gpt2medium-355m received
2024-09-10 15:40:12,415 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:12,416 127.0.0.1 - - [10/Sep/2024 15:40:12] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:12,897 Request with ID 200ab110 for model distilgpt2-124m received
2024-09-10 15:40:12,897 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:12,897 127.0.0.1 - - [10/Sep/2024 15:40:12] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:13,221 Processed batch: ['d0daded8', '09a30d6f', '5e64e991', 'f791f065'] with model gpt2-124m in 1.0043 seconds
2024-09-10 15:40:13,221 Latency for request d0daded8 with model gpt2-124m: 3.4861 seconds
2024-09-10 15:40:13,222 Latency for request 09a30d6f with model gpt2-124m: 2.7987 seconds
2024-09-10 15:40:13,222 Latency for request 5e64e991 with model gpt2-124m: 1.9464 seconds
2024-09-10 15:40:13,223 Latency for request f791f065 with model gpt2-124m: 1.3295 seconds
2024-09-10 15:40:13,223 127.0.0.1 - - [10/Sep/2024 15:40:13] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:13,223 Updated batch size:4
2024-09-10 15:40:13,223 Loading model gpt2medium-355m
2024-09-10 15:40:13,232 Request with ID fd1c1285 for model gpt2medium-355m received
2024-09-10 15:40:13,232 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:13,232 127.0.0.1 - - [10/Sep/2024 15:40:13] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:13,266 Time limit condition met for model gpt2medium-355m
2024-09-10 15:40:13,614 Request with ID 0fee0ace for model distilgpt2-124m received
2024-09-10 15:40:13,614 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:13,614 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:13,918 Request with ID e902e59d for model distilgpt2-124m received
2024-09-10 15:40:13,918 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:40:13,918 127.0.0.1 - - [10/Sep/2024 15:40:13] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:14,482 Request with ID 0a49be1e for model gpt2-124m received
2024-09-10 15:40:14,483 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:14,483 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:40:14,483 127.0.0.1 - - [10/Sep/2024 15:40:14] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:14,627 Request with ID edc101da for model distilgpt2-124m received
2024-09-10 15:40:14,627 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:14,627 127.0.0.1 - - [10/Sep/2024 15:40:14] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:14,830 Request with ID 24f296e3 for model distilgpt2-124m received
2024-09-10 15:40:14,830 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:14,830 127.0.0.1 - - [10/Sep/2024 15:40:14] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:14,947 Request with ID bd53554c for model distilgpt2-124m received
2024-09-10 15:40:14,947 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:14,947 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:15,784 Request with ID b89adcf9 for model distilgpt2-124m received
2024-09-10 15:40:15,784 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:15,784 127.0.0.1 - - [10/Sep/2024 15:40:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:15,874 Request with ID 44b35645 for model distilgpt2-124m received
2024-09-10 15:40:15,874 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:15,874 127.0.0.1 - - [10/Sep/2024 15:40:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:16,021 Request with ID 8fff1425 for model gpt2medium-355m received
2024-09-10 15:40:16,021 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:16,021 127.0.0.1 - - [10/Sep/2024 15:40:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:16,134 Request with ID dc61499b for model gpt2-124m received
2024-09-10 15:40:16,134 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:16,134 127.0.0.1 - - [10/Sep/2024 15:40:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:16,388 Processed batch: ['a667a086', 'e093f771', 'f1fa9bc1', '2bfc1fc6'] with model gpt2medium-355m in 3.0407 seconds
2024-09-10 15:40:16,388 Latency for request a667a086 with model gpt2medium-355m: 7.7516 seconds
2024-09-10 15:40:16,389 Latency for request e093f771 with model gpt2medium-355m: 6.5438 seconds
2024-09-10 15:40:16,389 Latency for request f1fa9bc1 with model gpt2medium-355m: 5.8217 seconds
2024-09-10 15:40:16,390 Latency for request 2bfc1fc6 with model gpt2medium-355m: 4.2193 seconds
2024-09-10 15:40:16,390 127.0.0.1 - - [10/Sep/2024 15:40:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:16,390 Updated batch size:2
2024-09-10 15:40:16,390 Loading model gpt2medium-355m
2024-09-10 15:40:16,613 Request with ID a5a75713 for model gpt2-124m received
2024-09-10 15:40:16,613 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:16,613 127.0.0.1 - - [10/Sep/2024 15:40:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:16,730 Request with ID 62ab99a2 for model gpt2-124m received
2024-09-10 15:40:16,730 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:16,730 Batch size condition met for model gpt2-124m
2024-09-10 15:40:16,817 Request with ID 76aefa06 for model gpt2medium-355m received
2024-09-10 15:40:16,817 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:16,817 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:40:16,817 127.0.0.1 - - [10/Sep/2024 15:40:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:17,055 Request with ID a4b7d44c for model gpt2-124m received
2024-09-10 15:40:17,056 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:17,056 127.0.0.1 - - [10/Sep/2024 15:40:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:17,569 Request with ID e3edde24 for model distilgpt2-124m received
2024-09-10 15:40:17,569 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:17,569 127.0.0.1 - - [10/Sep/2024 15:40:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:18,067 Request with ID 37c3f971 for model gpt2-124m received
2024-09-10 15:40:18,067 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:18,067 127.0.0.1 - - [10/Sep/2024 15:40:18] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:18,169 Request with ID 64ecd7b3 for model gpt2medium-355m received
2024-09-10 15:40:18,169 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:18,169 127.0.0.1 - - [10/Sep/2024 15:40:18] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:18,342 Request with ID 79c824b3 for model distilgpt2-124m received
2024-09-10 15:40:18,342 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-10 15:40:18,342 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:18,798 Request with ID 1e61d08f for model distilgpt2-124m received
2024-09-10 15:40:18,798 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:18,798 127.0.0.1 - - [10/Sep/2024 15:40:18] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:19,106 Request with ID 809696c0 for model distilgpt2-124m received
2024-09-10 15:40:19,106 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:19,106 127.0.0.1 - - [10/Sep/2024 15:40:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:19,130 Processed batch: ['02c3ed40', 'fd1c1285'] with model gpt2medium-355m in 2.7391 seconds
2024-09-10 15:40:19,130 Latency for request 02c3ed40 with model gpt2medium-355m: 6.7143 seconds
2024-09-10 15:40:19,131 Latency for request fd1c1285 with model gpt2medium-355m: 5.8973 seconds
2024-09-10 15:40:19,132 Updated batch size:4
2024-09-10 15:40:19,132 Loading model distilgpt2-124m
2024-09-10 15:40:19,330 Request with ID 066c76d3 for model gpt2medium-355m received
2024-09-10 15:40:19,330 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:19,330 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:40:19,330 Batch size condition met for model gpt2medium-355m
2024-09-10 15:40:19,857 Processed batch: ['b89adcf9', '44b35645', 'e3edde24', '79c824b3'] with model distilgpt2-124m in 0.6601 seconds
2024-09-10 15:40:19,857 Latency for request b89adcf9 with model distilgpt2-124m: 4.0730 seconds
2024-09-10 15:40:19,858 Latency for request 44b35645 with model distilgpt2-124m: 3.9827 seconds
2024-09-10 15:40:19,858 Latency for request e3edde24 with model distilgpt2-124m: 2.2880 seconds
2024-09-10 15:40:19,859 Latency for request 79c824b3 with model distilgpt2-124m: 1.5150 seconds
2024-09-10 15:40:19,859 127.0.0.1 - - [10/Sep/2024 15:40:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:19,859 No batch to process for model distilgpt2-124m
2024-09-10 15:40:19,859 127.0.0.1 - - [10/Sep/2024 15:40:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:19,859 Updated batch size:4
2024-09-10 15:40:19,859 Loading model gpt2-124m
2024-09-10 15:40:20,369 Request with ID 31706c17 for model distilgpt2-124m received
2024-09-10 15:40:20,369 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:20,369 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 15:40:20,369 127.0.0.1 - - [10/Sep/2024 15:40:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:20,558 Request with ID 92ef80b2 for model gpt2medium-355m received
2024-09-10 15:40:20,558 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:20,559 127.0.0.1 - - [10/Sep/2024 15:40:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:21,208 Processed batch: ['0a49be1e', 'dc61499b', 'a5a75713', '62ab99a2'] with model gpt2-124m in 1.2757 seconds
2024-09-10 15:40:21,208 Latency for request 0a49be1e with model gpt2-124m: 6.7255 seconds
2024-09-10 15:40:21,209 Latency for request dc61499b with model gpt2-124m: 5.0742 seconds
2024-09-10 15:40:21,210 Latency for request a5a75713 with model gpt2-124m: 4.5947 seconds
2024-09-10 15:40:21,210 Latency for request 62ab99a2 with model gpt2-124m: 4.4776 seconds
2024-09-10 15:40:21,210 127.0.0.1 - - [10/Sep/2024 15:40:21] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:21,210 No batch to process for model distilgpt2-124m
2024-09-10 15:40:21,210 127.0.0.1 - - [10/Sep/2024 15:40:21] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:21,210 Updated batch size:4
2024-09-10 15:40:21,211 Loading model gpt2medium-355m
2024-09-10 15:40:21,587 Request with ID 03cef8fc for model distilgpt2-124m received
2024-09-10 15:40:21,587 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:21,587 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:22,038 Request with ID 7d1a0083 for model distilgpt2-124m received
2024-09-10 15:40:22,038 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:22,038 127.0.0.1 - - [10/Sep/2024 15:40:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:23,276 Request with ID ba5d523f for model gpt2medium-355m received
2024-09-10 15:40:23,276 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:23,276 127.0.0.1 - - [10/Sep/2024 15:40:23] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:23,373 Time limit condition met for model gpt2medium-355m
2024-09-10 15:40:23,954 Request with ID cd17c7b5 for model gpt2-124m received
2024-09-10 15:40:23,954 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:23,954 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:40:23,954 127.0.0.1 - - [10/Sep/2024 15:40:23] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:24,181 Processed batch: ['8fff1425', '76aefa06', '64ecd7b3', '066c76d3'] with model gpt2medium-355m in 2.8459 seconds
2024-09-10 15:40:24,181 Latency for request 8fff1425 with model gpt2medium-355m: 8.1605 seconds
2024-09-10 15:40:24,182 Latency for request 76aefa06 with model gpt2medium-355m: 7.3647 seconds
2024-09-10 15:40:24,182 Latency for request 64ecd7b3 with model gpt2medium-355m: 6.0126 seconds
2024-09-10 15:40:24,183 Latency for request 066c76d3 with model gpt2medium-355m: 4.8514 seconds
2024-09-10 15:40:24,183 127.0.0.1 - - [10/Sep/2024 15:40:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:24,183 Updated batch size:4
2024-09-10 15:40:24,183 Loading model distilgpt2-124m
2024-09-10 15:40:24,232 Request with ID c7a6d4c1 for model gpt2-124m received
2024-09-10 15:40:24,232 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:24,232 Batch size condition met for model gpt2-124m
2024-09-10 15:40:24,374 Request with ID 0b9f738d for model distilgpt2-124m received
2024-09-10 15:40:24,374 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:24,374 127.0.0.1 - - [10/Sep/2024 15:40:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:24,651 Request with ID d43740a6 for model gpt2medium-355m received
2024-09-10 15:40:24,651 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:24,651 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:40:24,651 127.0.0.1 - - [10/Sep/2024 15:40:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:24,895 Processed batch: ['1e61d08f', '809696c0', '31706c17', '03cef8fc'] with model distilgpt2-124m in 0.6441 seconds
2024-09-10 15:40:24,895 Latency for request 1e61d08f with model distilgpt2-124m: 6.0970 seconds
2024-09-10 15:40:24,896 Latency for request 809696c0 with model distilgpt2-124m: 5.7889 seconds
2024-09-10 15:40:24,896 Latency for request 31706c17 with model distilgpt2-124m: 4.5262 seconds
2024-09-10 15:40:24,897 Latency for request 03cef8fc with model distilgpt2-124m: 3.3078 seconds
2024-09-10 15:40:24,897 127.0.0.1 - - [10/Sep/2024 15:40:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:24,897 Updated batch size:2
2024-09-10 15:40:24,897 Loading model gpt2medium-355m
2024-09-10 15:40:25,741 Request with ID 22344ef5 for model distilgpt2-124m received
2024-09-10 15:40:25,741 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:25,741 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:40:25,741 127.0.0.1 - - [10/Sep/2024 15:40:25] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:25,978 Request with ID 44a4e857 for model gpt2medium-355m received
2024-09-10 15:40:25,978 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:25,979 127.0.0.1 - - [10/Sep/2024 15:40:25] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:26,351 Request with ID 2efdd73c for model distilgpt2-124m received
2024-09-10 15:40:26,351 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:26,351 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:27,005 Request with ID 3d03e05b for model gpt2medium-355m received
2024-09-10 15:40:27,005 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:27,005 127.0.0.1 - - [10/Sep/2024 15:40:27] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:27,834 Processed batch: ['92ef80b2', 'ba5d523f'] with model gpt2medium-355m in 2.8134 seconds
2024-09-10 15:40:27,834 Latency for request 92ef80b2 with model gpt2medium-355m: 7.2758 seconds
2024-09-10 15:40:27,835 Latency for request ba5d523f with model gpt2medium-355m: 4.5582 seconds
2024-09-10 15:40:27,835 Updated batch size:4
2024-09-10 15:40:27,835 Loading model gpt2-124m
2024-09-10 15:40:27,919 Request with ID 23cd30e4 for model distilgpt2-124m received
2024-09-10 15:40:27,920 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:27,920 127.0.0.1 - - [10/Sep/2024 15:40:27] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:27,966 Request with ID a833952d for model gpt2-124m received
2024-09-10 15:40:27,966 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:27,966 127.0.0.1 - - [10/Sep/2024 15:40:27] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:28,751 Request with ID 6cc67ffb for model distilgpt2-124m received
2024-09-10 15:40:28,752 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:28,752 127.0.0.1 - - [10/Sep/2024 15:40:28] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:29,085 Processed batch: ['a4b7d44c', '37c3f971', 'cd17c7b5', 'c7a6d4c1'] with model gpt2-124m in 1.1688 seconds
2024-09-10 15:40:29,085 Latency for request a4b7d44c with model gpt2-124m: 12.0290 seconds
2024-09-10 15:40:29,086 Latency for request 37c3f971 with model gpt2-124m: 11.0176 seconds
2024-09-10 15:40:29,087 Latency for request cd17c7b5 with model gpt2-124m: 5.1304 seconds
2024-09-10 15:40:29,087 Latency for request c7a6d4c1 with model gpt2-124m: 4.8528 seconds
2024-09-10 15:40:29,087 127.0.0.1 - - [10/Sep/2024 15:40:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:29,087 Updated batch size:4
2024-09-10 15:40:29,087 Loading model distilgpt2-124m
2024-09-10 15:40:29,475 Request with ID 3c509f49 for model gpt2medium-355m received
2024-09-10 15:40:29,475 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:29,475 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:40:29,475 Batch size condition met for model gpt2medium-355m
2024-09-10 15:40:29,742 Processed batch: ['7d1a0083', '0b9f738d', '22344ef5', '2efdd73c'] with model distilgpt2-124m in 0.5918 seconds
2024-09-10 15:40:29,742 Latency for request 7d1a0083 with model distilgpt2-124m: 7.7046 seconds
2024-09-10 15:40:29,743 Latency for request 0b9f738d with model distilgpt2-124m: 5.3683 seconds
2024-09-10 15:40:29,743 Latency for request 22344ef5 with model distilgpt2-124m: 4.0015 seconds
2024-09-10 15:40:29,744 Latency for request 2efdd73c with model distilgpt2-124m: 3.3916 seconds
2024-09-10 15:40:29,744 127.0.0.1 - - [10/Sep/2024 15:40:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:29,744 Updated batch size:4
2024-09-10 15:40:29,744 Loading model gpt2medium-355m
2024-09-10 15:40:29,938 Request with ID d5109912 for model distilgpt2-124m received
2024-09-10 15:40:29,938 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:29,938 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:40:29,938 127.0.0.1 - - [10/Sep/2024 15:40:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:30,351 Request with ID 9d0caba9 for model distilgpt2-124m received
2024-09-10 15:40:30,351 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:30,351 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:30,549 Request with ID 3b3b726d for model gpt2medium-355m received
2024-09-10 15:40:30,549 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:30,549 127.0.0.1 - - [10/Sep/2024 15:40:30] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:30,655 Request with ID 09f8694d for model distilgpt2-124m received
2024-09-10 15:40:30,655 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:30,655 127.0.0.1 - - [10/Sep/2024 15:40:30] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:30,820 Request with ID 58d1250e for model gpt2medium-355m received
2024-09-10 15:40:30,820 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:30,820 127.0.0.1 - - [10/Sep/2024 15:40:30] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:30,849 Time limit condition met for model gpt2medium-355m
2024-09-10 15:40:31,284 Request with ID 3962dd8c for model gpt2-124m received
2024-09-10 15:40:31,284 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:31,284 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:40:31,284 127.0.0.1 - - [10/Sep/2024 15:40:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:31,599 Request with ID b75768d3 for model gpt2-124m received
2024-09-10 15:40:31,599 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:31,599 127.0.0.1 - - [10/Sep/2024 15:40:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:31,652 Request with ID da1236ef for model gpt2medium-355m received
2024-09-10 15:40:31,652 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:31,652 127.0.0.1 - - [10/Sep/2024 15:40:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:31,761 Request with ID 71dada9c for model gpt2-124m received
2024-09-10 15:40:31,761 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:31,761 Batch size condition met for model gpt2-124m
2024-09-10 15:40:32,334 Request with ID 8fd6dc8e for model gpt2medium-355m received
2024-09-10 15:40:32,334 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:32,334 127.0.0.1 - - [10/Sep/2024 15:40:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:32,457 Request with ID 99b7ce9a for model gpt2-124m received
2024-09-10 15:40:32,457 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:32,457 127.0.0.1 - - [10/Sep/2024 15:40:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:32,474 Request with ID 90ddb16e for model distilgpt2-124m received
2024-09-10 15:40:32,474 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:32,475 127.0.0.1 - - [10/Sep/2024 15:40:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:32,789 Request with ID 442f718d for model gpt2medium-355m received
2024-09-10 15:40:32,789 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:32,790 127.0.0.1 - - [10/Sep/2024 15:40:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:33,154 Processed batch: ['d43740a6', '44a4e857', '3d03e05b', '3c509f49'] with model gpt2medium-355m in 3.2836 seconds
2024-09-10 15:40:33,155 Latency for request d43740a6 with model gpt2medium-355m: 8.5035 seconds
2024-09-10 15:40:33,156 Latency for request 44a4e857 with model gpt2medium-355m: 7.1762 seconds
2024-09-10 15:40:33,156 Latency for request 3d03e05b with model gpt2medium-355m: 6.1498 seconds
2024-09-10 15:40:33,156 Latency for request 3c509f49 with model gpt2medium-355m: 3.6794 seconds
2024-09-10 15:40:33,157 127.0.0.1 - - [10/Sep/2024 15:40:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:33,157 Updated batch size:4
2024-09-10 15:40:33,157 Loading model distilgpt2-124m
2024-09-10 15:40:33,236 Request with ID 41cce398 for model distilgpt2-124m received
2024-09-10 15:40:33,236 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:33,236 127.0.0.1 - - [10/Sep/2024 15:40:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:33,540 Request with ID 36ad7623 for model distilgpt2-124m received
2024-09-10 15:40:33,540 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:33,540 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:33,592 Request with ID 44bcb81c for model gpt2-124m received
2024-09-10 15:40:33,592 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:33,592 127.0.0.1 - - [10/Sep/2024 15:40:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:33,820 Processed batch: ['23cd30e4', '6cc67ffb', 'd5109912', '9d0caba9'] with model distilgpt2-124m in 0.5996 seconds
2024-09-10 15:40:33,820 Latency for request 23cd30e4 with model distilgpt2-124m: 5.9001 seconds
2024-09-10 15:40:33,820 Latency for request 6cc67ffb with model distilgpt2-124m: 5.0681 seconds
2024-09-10 15:40:33,821 Latency for request d5109912 with model distilgpt2-124m: 3.8815 seconds
2024-09-10 15:40:33,821 Latency for request 9d0caba9 with model distilgpt2-124m: 3.4683 seconds
2024-09-10 15:40:33,822 127.0.0.1 - - [10/Sep/2024 15:40:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:33,822 Updated batch size:2
2024-09-10 15:40:33,822 Loading model gpt2medium-355m
2024-09-10 15:40:33,823 Request with ID f65bdfd0 for model gpt2-124m received
2024-09-10 15:40:33,824 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:33,824 127.0.0.1 - - [10/Sep/2024 15:40:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:34,009 Request with ID 9fab5e11 for model gpt2-124m received
2024-09-10 15:40:34,009 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:34,009 Batch size condition met for model gpt2-124m
2024-09-10 15:40:34,341 Request with ID 76820460 for model gpt2-124m received
2024-09-10 15:40:34,341 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:34,341 127.0.0.1 - - [10/Sep/2024 15:40:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:34,504 Request with ID c27ebad7 for model gpt2medium-355m received
2024-09-10 15:40:34,505 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:34,505 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:40:34,505 Batch size condition met for model gpt2medium-355m
2024-09-10 15:40:34,763 Request with ID 81036f51 for model gpt2medium-355m received
2024-09-10 15:40:34,763 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:34,763 127.0.0.1 - - [10/Sep/2024 15:40:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:34,847 Request with ID b5dd3295 for model gpt2-124m received
2024-09-10 15:40:34,848 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:34,848 127.0.0.1 - - [10/Sep/2024 15:40:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:35,092 Request with ID 52b1a48b for model gpt2-124m received
2024-09-10 15:40:35,093 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:35,093 127.0.0.1 - - [10/Sep/2024 15:40:35] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:35,823 Request with ID bb3fbe61 for model gpt2-124m received
2024-09-10 15:40:35,823 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:35,823 Batch size condition met for model gpt2-124m
2024-09-10 15:40:36,049 Request with ID f729b5f4 for model gpt2medium-355m received
2024-09-10 15:40:36,049 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:36,049 127.0.0.1 - - [10/Sep/2024 15:40:36] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:36,230 Request with ID 6a41e83e for model gpt2medium-355m received
2024-09-10 15:40:36,231 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:36,231 127.0.0.1 - - [10/Sep/2024 15:40:36] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:36,514 Request with ID b977c095 for model gpt2-124m received
2024-09-10 15:40:36,514 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:36,514 127.0.0.1 - - [10/Sep/2024 15:40:36] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:36,776 Request with ID 03dd3a69 for model distilgpt2-124m received
2024-09-10 15:40:36,776 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:36,776 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:40:36,777 127.0.0.1 - - [10/Sep/2024 15:40:36] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:36,817 Processed batch: ['3b3b726d', '58d1250e'] with model gpt2medium-355m in 2.8542 seconds
2024-09-10 15:40:36,817 Latency for request 3b3b726d with model gpt2medium-355m: 6.2675 seconds
2024-09-10 15:40:36,817 Latency for request 58d1250e with model gpt2medium-355m: 5.9966 seconds
2024-09-10 15:40:36,818 Updated batch size:4
2024-09-10 15:40:36,818 Loading model gpt2-124m
2024-09-10 15:40:37,109 Request with ID 9eaedf26 for model gpt2medium-355m received
2024-09-10 15:40:37,109 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:37,109 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:40:37,109 Batch size condition met for model gpt2medium-355m
2024-09-10 15:40:37,534 Request with ID bde1f834 for model gpt2-124m received
2024-09-10 15:40:37,535 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:37,535 127.0.0.1 - - [10/Sep/2024 15:40:37] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:37,698 Request with ID 1d01b75c for model distilgpt2-124m received
2024-09-10 15:40:37,698 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:37,699 127.0.0.1 - - [10/Sep/2024 15:40:37] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:37,782 Request with ID 271db80a for model distilgpt2-124m received
2024-09-10 15:40:37,783 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:37,783 127.0.0.1 - - [10/Sep/2024 15:40:37] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:37,942 Request with ID 30eb36df for model gpt2-124m received
2024-09-10 15:40:37,942 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:37,942 127.0.0.1 - - [10/Sep/2024 15:40:37] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:37,949 Time limit condition met for model gpt2-124m
2024-09-10 15:40:38,060 Processed batch: ['76820460', 'b5dd3295', '52b1a48b', 'bb3fbe61'] with model gpt2-124m in 1.1610 seconds
2024-09-10 15:40:38,060 Latency for request 76820460 with model gpt2-124m: 3.7197 seconds
2024-09-10 15:40:38,061 Latency for request b5dd3295 with model gpt2-124m: 3.2128 seconds
2024-09-10 15:40:38,061 Latency for request 52b1a48b with model gpt2-124m: 2.9679 seconds
2024-09-10 15:40:38,062 Latency for request bb3fbe61 with model gpt2-124m: 2.2373 seconds
2024-09-10 15:40:38,062 127.0.0.1 - - [10/Sep/2024 15:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:38,062 Updated batch size:4
2024-09-10 15:40:38,062 Loading model distilgpt2-124m
2024-09-10 15:40:38,337 Request with ID f4a19d0c for model distilgpt2-124m received
2024-09-10 15:40:38,337 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:38,337 Batch size condition met for model distilgpt2-124m
2024-09-10 15:40:38,674 Request with ID fd7e916f for model gpt2-124m received
2024-09-10 15:40:38,675 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:40:38,675 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:40:38,675 127.0.0.1 - - [10/Sep/2024 15:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:38,716 Processed batch: ['09f8694d', '90ddb16e', '41cce398', '36ad7623'] with model distilgpt2-124m in 0.5929 seconds
2024-09-10 15:40:38,716 Latency for request 09f8694d with model distilgpt2-124m: 8.0609 seconds
2024-09-10 15:40:38,717 Latency for request 90ddb16e with model distilgpt2-124m: 6.2419 seconds
2024-09-10 15:40:38,717 Latency for request 41cce398 with model distilgpt2-124m: 5.4799 seconds
2024-09-10 15:40:38,717 Latency for request 36ad7623 with model distilgpt2-124m: 5.1766 seconds
2024-09-10 15:40:38,718 127.0.0.1 - - [10/Sep/2024 15:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:38,718 Updated batch size:4
2024-09-10 15:40:38,718 Loading model gpt2-124m
2024-09-10 15:40:39,542 Request with ID e1c8e144 for model gpt2-124m received
2024-09-10 15:40:39,542 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:40:39,542 127.0.0.1 - - [10/Sep/2024 15:40:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:39,968 Processed batch: ['b977c095', 'bde1f834', '30eb36df', '388f'] with model gpt2-124m in 1.1723 seconds
2024-09-10 15:40:39,968 Latency for request b977c095 with model gpt2-124m: 3.4539 seconds
2024-09-10 15:40:39,969 Latency for request bde1f834 with model gpt2-124m: 2.4336 seconds
2024-09-10 15:40:39,970 Latency for request 30eb36df with model gpt2-124m: 2.0258 seconds
2024-09-10 15:40:39,970 Latency for request 388f with model gpt2-124m: 1.2501 seconds
2024-09-10 15:40:39,970 127.0.0.1 - - [10/Sep/2024 15:40:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:39,970 Updated batch size:4
2024-09-10 15:40:39,970 Loading model gpt2medium-355m
2024-09-10 15:40:40,192 Request with ID ae7a5d23 for model gpt2medium-355m received
2024-09-10 15:40:40,192 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 15:40:40,192 127.0.0.1 - - [10/Sep/2024 15:40:40] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:41,471 Request with ID 1cbe3948 for model distilgpt2-124m received
2024-09-10 15:40:41,471 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 15:40:41,471 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:40:41,471 127.0.0.1 - - [10/Sep/2024 15:40:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:41,637 Request with ID 8887441d for model distilgpt2-124m received
2024-09-10 15:40:41,637 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:41,638 127.0.0.1 - - [10/Sep/2024 15:40:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:41,682 Request with ID fb75fbf0 for model distilgpt2-124m received
2024-09-10 15:40:41,682 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-10 15:40:41,682 127.0.0.1 - - [10/Sep/2024 15:40:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:42,119 Request with ID fe1662f5 for model gpt2-124m received
2024-09-10 15:40:42,119 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-10 15:40:42,119 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:40:42,120 127.0.0.1 - - [10/Sep/2024 15:40:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:42,216 Request with ID e9054f50 for model gpt2-124m received
2024-09-10 15:40:42,216 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-10 15:40:42,216 Batch size condition met for model gpt2-124m
2024-09-10 15:40:42,975 Processed batch: ['81036f51', 'f729b5f4', '6a41e83e', '9eaedf26'] with model gpt2medium-355m in 2.8819 seconds
2024-09-10 15:40:42,976 Latency for request 81036f51 with model gpt2medium-355m: 8.2125 seconds
2024-09-10 15:40:42,977 Latency for request f729b5f4 with model gpt2medium-355m: 6.9266 seconds
2024-09-10 15:40:42,977 Latency for request 6a41e83e with model gpt2medium-355m: 6.7451 seconds
2024-09-10 15:40:42,977 Latency for request 9eaedf26 with model gpt2medium-355m: 5.8662 seconds
2024-09-10 15:40:42,978 127.0.0.1 - - [10/Sep/2024 15:40:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:42,978 Updated batch size:4
2024-09-10 15:40:42,978 Loading model gpt2-124m
2024-09-10 15:40:43,282 Request with ID 34c03c87 for model gpt2-124m received
2024-09-10 15:40:43,282 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 15:40:43,283 127.0.0.1 - - [10/Sep/2024 15:40:43] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:44,278 Processed batch: ['fd7e916f', 'e1c8e144', 'fe1662f5', 'e9054f50'] with model gpt2-124m in 1.2163 seconds
2024-09-10 15:40:44,278 Latency for request fd7e916f with model gpt2-124m: 5.6035 seconds
2024-09-10 15:40:44,280 Latency for request e1c8e144 with model gpt2-124m: 4.7361 seconds
2024-09-10 15:40:44,280 Latency for request fe1662f5 with model gpt2-124m: 2.1586 seconds
2024-09-10 15:40:44,280 Latency for request e9054f50 with model gpt2-124m: 2.0618 seconds
2024-09-10 15:40:44,281 127.0.0.1 - - [10/Sep/2024 15:40:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:44,281 No batch to process for model gpt2medium-355m
2024-09-10 15:40:44,281 127.0.0.1 - - [10/Sep/2024 15:40:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:44,281 No batch to process for model gpt2-124m
2024-09-10 15:40:44,281 Updated batch size:4
2024-09-10 15:40:44,281 Loading model distilgpt2-124m
2024-09-10 15:40:44,984 Processed batch: ['03dd3a69', '1d01b75c', '271db80a', 'f4a19d0c'] with model distilgpt2-124m in 0.6389 seconds
2024-09-10 15:40:44,984 Latency for request 03dd3a69 with model distilgpt2-124m: 8.2075 seconds
2024-09-10 15:40:44,985 Latency for request 1d01b75c with model distilgpt2-124m: 7.2854 seconds
2024-09-10 15:40:44,985 Latency for request 271db80a with model distilgpt2-124m: 7.2012 seconds
2024-09-10 15:40:44,986 Latency for request f4a19d0c with model distilgpt2-124m: 6.6465 seconds
2024-09-10 15:40:44,986 127.0.0.1 - - [10/Sep/2024 15:40:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:40:44,986 No batch to process for model gpt2-124m
2024-09-10 15:40:44,986 127.0.0.1 - - [10/Sep/2024 15:40:44] "POST /inference HTTP/1.1" 200 -
