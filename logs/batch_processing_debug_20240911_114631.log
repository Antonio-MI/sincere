2024-09-11 11:46:31,579 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-11 11:46:31,579 [33mPress CTRL+C to quit[0m
2024-09-11 11:46:33,514 Request with ID 1366280d for model gpt2-124m received
2024-09-11 11:46:33,514 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:46:33,514 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-11 11:46:33,514 127.0.0.1 - - [11/Sep/2024 11:46:33] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:33,577 Remaining requests condition met for model gpt2-124m
2024-09-11 11:46:33,578 Updated batch size:1
2024-09-11 11:46:33,578 Loading model gpt2-124m
2024-09-11 11:46:34,053 Processed batch: ['1366280d'] with model gpt2-124m in 0.3954 seconds
2024-09-11 11:46:34,053 Latency for request 1366280d with model gpt2-124m: 0.5391 seconds
2024-09-11 11:46:34,055 Total time: 0.5407 seconds
2024-09-11 11:46:34,055 Total inference time: 0.3954 seconds
2024-09-11 11:46:34,055 Inference time as percentage of total time: 73.12%
2024-09-11 11:46:34,340 Request with ID 30d5b99f for model gpt2-124m received
2024-09-11 11:46:34,340 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:46:34,340 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-11 11:46:34,340 127.0.0.1 - - [11/Sep/2024 11:46:34] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:34,366 Remaining requests condition met for model gpt2-124m
2024-09-11 11:46:34,366 Updated batch size:1
2024-09-11 11:46:34,366 Loading model gpt2-124m
2024-09-11 11:46:34,438 Processed batch: ['30d5b99f'] with model gpt2-124m in 0.0717 seconds
2024-09-11 11:46:34,438 Latency for request 30d5b99f with model gpt2-124m: 0.0983 seconds
2024-09-11 11:46:34,671 Request with ID f92f6e44 for model gpt2medium-355m received
2024-09-11 11:46:34,672 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:46:34,672 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-11 11:46:34,672 127.0.0.1 - - [11/Sep/2024 11:46:34] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:34,735 Request with ID 1326e312 for model gpt2-124m received
2024-09-11 11:46:34,735 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:46:34,735 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-11 11:46:34,735 127.0.0.1 - - [11/Sep/2024 11:46:34] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:34,753 Remaining requests condition met for model gpt2-124m
2024-09-11 11:46:34,753 Updated batch size:1
2024-09-11 11:46:34,753 Loading model gpt2-124m
2024-09-11 11:46:35,106 Request with ID 2d3d6c98 for model gpt2-124m received
2024-09-11 11:46:35,106 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:46:35,106 127.0.0.1 - - [11/Sep/2024 11:46:35] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:35,116 Processed batch: ['1326e312'] with model gpt2-124m in 0.3630 seconds
2024-09-11 11:46:35,116 Latency for request 1326e312 with model gpt2-124m: 0.3811 seconds
2024-09-11 11:46:35,117 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:46:35,117 Updated batch size:1
2024-09-11 11:46:35,117 Loading model gpt2medium-355m
2024-09-11 11:46:35,769 Request with ID c978f6dc for model distilgpt2-124m received
2024-09-11 11:46:35,769 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:46:35,769 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-11 11:46:35,769 127.0.0.1 - - [11/Sep/2024 11:46:35] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:36,378 Request with ID 8c2b28e1 for model distilgpt2-124m received
2024-09-11 11:46:36,378 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-11 11:46:36,378 127.0.0.1 - - [11/Sep/2024 11:46:36] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:36,944 Request with ID da47c4f9 for model gpt2medium-355m received
2024-09-11 11:46:36,944 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:46:36,945 127.0.0.1 - - [11/Sep/2024 11:46:36] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:37,337 Request with ID d6dde210 for model distilgpt2-124m received
2024-09-11 11:46:37,337 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-11 11:46:37,338 127.0.0.1 - - [11/Sep/2024 11:46:37] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:37,793 Request with ID 611e7efc for model gpt2medium-355m received
2024-09-11 11:46:37,793 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:46:37,793 127.0.0.1 - - [11/Sep/2024 11:46:37] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:38,282 Request with ID 9ea5a132 for model gpt2-124m received
2024-09-11 11:46:38,282 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-11 11:46:38,282 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-11 11:46:38,282 127.0.0.1 - - [11/Sep/2024 11:46:38] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:38,447 Processed batch: ['f92f6e44'] with model gpt2medium-355m in 3.2373 seconds
2024-09-11 11:46:38,447 Latency for request f92f6e44 with model gpt2medium-355m: 3.7750 seconds
2024-09-11 11:46:38,550 Remaining requests condition met for model gpt2-124m
2024-09-11 11:46:38,550 Updated batch size:2
2024-09-11 11:46:38,551 Loading model gpt2-124m
2024-09-11 11:46:38,893 Request with ID 47e53e43 for model gpt2medium-355m received
2024-09-11 11:46:38,893 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:46:38,893 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-11 11:46:38,893 127.0.0.1 - - [11/Sep/2024 11:46:38] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:39,257 Request with ID fdfa5441 for model gpt2-124m received
2024-09-11 11:46:39,257 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-11 11:46:39,257 127.0.0.1 - - [11/Sep/2024 11:46:39] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:39,298 Processed batch: ['2d3d6c98', '9ea5a132'] with model gpt2-124m in 0.6753 seconds
2024-09-11 11:46:39,298 Latency for request 2d3d6c98 with model gpt2-124m: 4.1923 seconds
2024-09-11 11:46:39,299 Latency for request 9ea5a132 with model gpt2-124m: 1.0157 seconds
2024-09-11 11:46:39,299 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:46:39,299 Updated batch size:4
2024-09-11 11:46:39,299 Loading model gpt2medium-355m
2024-09-11 11:46:39,594 Request with ID c56b12d6 for model distilgpt2-124m received
2024-09-11 11:46:39,594 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-11 11:46:39,594 127.0.0.1 - - [11/Sep/2024 11:46:39] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:40,666 Request with ID f57089a2 for model gpt2-124m received
2024-09-11 11:46:40,666 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:46:40,666 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-11 11:46:40,666 127.0.0.1 - - [11/Sep/2024 11:46:40] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:41,865 Request with ID 63f4f381 for model gpt2-124m received
2024-09-11 11:46:41,865 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-11 11:46:41,865 127.0.0.1 - - [11/Sep/2024 11:46:41] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:42,054 Processed batch: ['da47c4f9', '611e7efc', '47e53e43', '422c'] with model gpt2medium-355m in 2.5852 seconds
2024-09-11 11:46:42,054 Latency for request da47c4f9 with model gpt2medium-355m: 5.1095 seconds
2024-09-11 11:46:42,055 Latency for request 611e7efc with model gpt2medium-355m: 4.2609 seconds
2024-09-11 11:46:42,055 Latency for request 47e53e43 with model gpt2medium-355m: 3.1608 seconds
2024-09-11 11:46:42,056 Latency for request 422c with model gpt2medium-355m: 2.7548 seconds
2024-09-11 11:46:42,056 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:46:42,056 Updated batch size:4
2024-09-11 11:46:42,056 Loading model distilgpt2-124m
2024-09-11 11:46:42,599 Request with ID 939dab8e for model distilgpt2-124m received
2024-09-11 11:46:42,599 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:46:42,599 127.0.0.1 - - [11/Sep/2024 11:46:42] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:46:43,345 Processed batch: ['c978f6dc', '8c2b28e1', 'd6dde210', 'c56b12d6'] with model distilgpt2-124m in 1.2258 seconds
2024-09-11 11:46:43,345 Latency for request c978f6dc with model distilgpt2-124m: 7.5761 seconds
2024-09-11 11:46:43,346 Latency for request 8c2b28e1 with model distilgpt2-124m: 6.9668 seconds
2024-09-11 11:46:43,346 Latency for request d6dde210 with model distilgpt2-124m: 6.0076 seconds
2024-09-11 11:46:43,346 Latency for request c56b12d6 with model distilgpt2-124m: 3.7512 seconds
2024-09-11 11:46:43,447 Remaining requests condition met for model gpt2-124m
2024-09-11 11:46:43,447 Updated batch size:4
2024-09-11 11:46:43,447 Loading model gpt2-124m
2024-09-11 11:46:44,324 Processed batch: ['fdfa5441', 'f57089a2', '63f4f381', '1ed2'] with model gpt2-124m in 0.8127 seconds
2024-09-11 11:46:44,324 Latency for request fdfa5441 with model gpt2-124m: 5.0673 seconds
2024-09-11 11:46:44,325 Latency for request f57089a2 with model gpt2-124m: 3.6584 seconds
2024-09-11 11:46:44,325 Latency for request 63f4f381 with model gpt2-124m: 2.4594 seconds
2024-09-11 11:46:44,326 Latency for request 1ed2 with model gpt2-124m: 0.8767 seconds
2024-09-11 11:46:44,326 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:46:44,326 Updated batch size:1
2024-09-11 11:46:44,326 Loading model distilgpt2-124m
2024-09-11 11:46:44,682 Processed batch: ['939dab8e'] with model distilgpt2-124m in 0.3037 seconds
2024-09-11 11:46:44,682 Latency for request 939dab8e with model distilgpt2-124m: 2.0834 seconds
