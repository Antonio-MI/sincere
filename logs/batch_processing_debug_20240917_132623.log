2024-09-17 13:26:23,895 Using device: cuda
2024-09-17 13:26:23,895 Scheduling mode set as FCFS
2024-09-17 13:26:23,895 Monitoring status set to True
2024-09-17 13:26:38,965 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.122.143:5000
2024-09-17 13:26:38,965 [33mPress CTRL+C to quit[0m
2024-09-17 13:26:47,846 Request with ID ec4fb1d3 for model llama3-8b received
2024-09-17 13:26:47,846 Batch size condition met for model llama3-8b
2024-09-17 13:26:47,846 Next: call load_model for llama3-8b
2024-09-17 13:26:49,177 Request with ID 078026a2 for model llama3-8b received
2024-09-17 13:26:49,177 Batch size condition met for model llama3-8b
2024-09-17 13:26:51,431 Request with ID 4eea224f for model granite-7b received
2024-09-17 13:26:51,432 Batch size condition met for model granite-7b
2024-09-17 13:26:53,761 Request with ID 812674d5 for model gemma-7b received
2024-09-17 13:26:53,762 Batch size condition met for model gemma-7b
2024-09-17 13:26:55,197 Request with ID 576433f6 for model granite-7b received
2024-09-17 13:26:55,197 Batch size condition met for model granite-7b
2024-09-17 13:26:57,380 Request with ID fa573b43 for model llama3-8b received
2024-09-17 13:26:57,380 Batch size condition met for model llama3-8b
2024-09-17 13:26:59,214 Loaded model llama3-8b
2024-09-17 13:26:59,217 Batch processing started for model llama3-8b
2024-09-17 13:26:59,658 Request with ID f67c1609 for model llama3-8b received
2024-09-17 13:26:59,658 Batch size condition met for model llama3-8b
2024-09-17 13:27:01,522 Processed batch: ['ec4fb1d3'] with model llama3-8b in 2.3051 seconds
2024-09-17 13:27:01,522 Saving sys info
2024-09-17 13:27:01,561 Latency for request ec4fb1d3 with model llama3-8b: 13.6756 seconds
2024-09-17 13:27:01,561 Saving results with gpu monitoring
2024-09-17 13:27:01,561 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 500, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 282, in process_batch
    save_measurements_and_monitor(request_id, model_alias, current_batch_size, latency, batch_throughput, sys_info)
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 165, in save_measurements_and_monitor
    for key in df["sys_info"][0].keys()
UnboundLocalError: local variable 'df' referenced before assignment
2024-09-17 13:27:01,562 Next: call load_model for llama3-8b
2024-09-17 13:27:01,563 127.0.0.1 - - [17/Sep/2024 13:27:01] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 13:27:01,563 Model llama3-8b already loaded
2024-09-17 13:27:01,569 Batch processing started for model llama3-8b
2024-09-17 13:27:01,842 Request with ID 7c04b35b for model granite-7b received
2024-09-17 13:27:01,842 Batch size condition met for model granite-7b
2024-09-17 13:27:02,542 Request with ID 3b630d36 for model granite-7b received
2024-09-17 13:27:02,542 Batch size condition met for model granite-7b
2024-09-17 13:27:03,202 Request with ID 0bd08a21 for model gemma-7b received
2024-09-17 13:27:03,202 Batch size condition met for model gemma-7b
2024-09-17 13:27:03,435 Processed batch: ['f67c1609'] with model llama3-8b in 1.8658 seconds
2024-09-17 13:27:03,435 Saving sys info
2024-09-17 13:27:03,467 Latency for request f67c1609 with model llama3-8b: 3.7770 seconds
2024-09-17 13:27:03,467 Saving results with gpu monitoring
2024-09-17 13:27:03,467 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 500, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 282, in process_batch
    save_measurements_and_monitor(request_id, model_alias, current_batch_size, latency, batch_throughput, sys_info)
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 165, in save_measurements_and_monitor
    for key in df["sys_info"][0].keys()
UnboundLocalError: local variable 'df' referenced before assignment
2024-09-17 13:27:03,467 Next: call load_model for granite-7b
2024-09-17 13:27:03,468 127.0.0.1 - - [17/Sep/2024 13:27:03] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 13:27:03,468 Unloaded previous model
2024-09-17 13:27:04,080 Request with ID 1132d6f0 for model llama3-8b received
2024-09-17 13:27:04,165 Batch size condition met for model llama3-8b
2024-09-17 13:27:05,628 Request with ID 89976390 for model gemma-7b received
2024-09-17 13:27:05,629 Batch size condition met for model gemma-7b
