2024-09-18 12:58:59,716 Using device: cpu
2024-09-18 12:58:59,717 Scheduling mode set as batchedFCFS
2024-09-18 12:58:59,836 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 12:58:59,836 [33mPress CTRL+C to quit[0m
2024-09-18 12:59:04,738 Request with ID 17371d0f for model llama3-8b received
2024-09-18 12:59:04,738 127.0.0.1 - - [18/Sep/2024 12:59:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:04,871 Request with ID 407f8b33 for model gemma-7b received
2024-09-18 12:59:04,871 127.0.0.1 - - [18/Sep/2024 12:59:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:04,889 Request with ID 87e9421c for model llama3-8b received
2024-09-18 12:59:04,889 127.0.0.1 - - [18/Sep/2024 12:59:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,095 Request with ID 075049a9 for model gemma-7b received
2024-09-18 12:59:05,096 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,105 Request with ID 0527dded for model gemma-7b received
2024-09-18 12:59:05,105 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,127 Request with ID 47d40a77 for model granite-7b received
2024-09-18 12:59:05,128 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,226 Request with ID 220c5d28 for model granite-7b received
2024-09-18 12:59:05,227 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,286 Request with ID 7832820a for model llama3-8b received
2024-09-18 12:59:05,287 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,338 Request with ID c283d53d for model llama3-8b received
2024-09-18 12:59:05,339 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,399 Request with ID 0a44773d for model llama3-8b received
2024-09-18 12:59:05,399 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:05,812 Request with ID fb20566a for model llama3-8b received
2024-09-18 12:59:05,812 127.0.0.1 - - [18/Sep/2024 12:59:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,011 Request with ID 4a70243b for model granite-7b received
2024-09-18 12:59:06,012 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,073 Request with ID e8dbe57d for model granite-7b received
2024-09-18 12:59:06,074 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,412 Request with ID 4ac911be for model gemma-7b received
2024-09-18 12:59:06,413 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,451 Request with ID 60950e86 for model llama3-8b received
2024-09-18 12:59:06,452 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,568 Request with ID 3a847506 for model llama3-8b received
2024-09-18 12:59:06,569 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,591 Request with ID 76358b75 for model gemma-7b received
2024-09-18 12:59:06,591 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,701 Request with ID 32dadd86 for model gemma-7b received
2024-09-18 12:59:06,702 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,726 Request with ID 1a4eea5e for model llama3-8b received
2024-09-18 12:59:06,727 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,914 Request with ID d32da300 for model gemma-7b received
2024-09-18 12:59:06,915 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:06,964 Request with ID a5da99d6 for model gemma-7b received
2024-09-18 12:59:06,965 127.0.0.1 - - [18/Sep/2024 12:59:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,095 Request with ID 60bab94c for model gemma-7b received
2024-09-18 12:59:07,095 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,137 Request with ID 4d1c4b86 for model gemma-7b received
2024-09-18 12:59:07,137 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,161 Request with ID 310e904f for model llama3-8b received
2024-09-18 12:59:07,161 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,339 Request with ID a4c38978 for model granite-7b received
2024-09-18 12:59:07,340 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,356 Request with ID 05a50d28 for model granite-7b received
2024-09-18 12:59:07,357 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,364 Request with ID 7ed4dc05 for model granite-7b received
2024-09-18 12:59:07,364 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,388 Request with ID 4873e64d for model llama3-8b received
2024-09-18 12:59:07,388 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,559 Request with ID 88079352 for model llama3-8b received
2024-09-18 12:59:07,559 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,720 Request with ID 3c717cda for model granite-7b received
2024-09-18 12:59:07,721 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:07,980 Request with ID ff0693bb for model llama3-8b received
2024-09-18 12:59:07,981 127.0.0.1 - - [18/Sep/2024 12:59:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,174 Request with ID 4f77da86 for model gemma-7b received
2024-09-18 12:59:08,174 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,185 Request with ID aaeee3b9 for model llama3-8b received
2024-09-18 12:59:08,186 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,298 Request with ID 2ceff619 for model llama3-8b received
2024-09-18 12:59:08,299 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,390 Request with ID 5f1257af for model granite-7b received
2024-09-18 12:59:08,391 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,440 Request with ID 1455e7c6 for model llama3-8b received
2024-09-18 12:59:08,441 Batch size condition met for model llama3-8b
2024-09-18 12:59:08,441 Next: call load_model for llama3-8b
2024-09-18 12:59:08,443 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 402, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/llama3-8b'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 230, in process_batch
    load_model(model_alias)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 107, in load_model
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 834, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 666, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 466, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './models/llama3-8b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
2024-09-18 12:59:08,452 127.0.0.1 - - [18/Sep/2024 12:59:08] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 12:59:08,586 Request with ID c6449e80 for model llama3-8b received
2024-09-18 12:59:08,587 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,618 Request with ID 483b8e11 for model llama3-8b received
2024-09-18 12:59:08,619 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,676 Request with ID e33fab52 for model llama3-8b received
2024-09-18 12:59:08,677 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,681 Request with ID 88cec95b for model granite-7b received
2024-09-18 12:59:08,682 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,754 Request with ID d53f1279 for model granite-7b received
2024-09-18 12:59:08,755 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,784 Request with ID 01ba71d7 for model llama3-8b received
2024-09-18 12:59:08,785 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,809 Request with ID 066160bb for model llama3-8b received
2024-09-18 12:59:08,809 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,845 Request with ID fa65c97a for model gemma-7b received
2024-09-18 12:59:08,845 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,945 Request with ID b93d9417 for model granite-7b received
2024-09-18 12:59:08,946 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:08,958 Request with ID c24aaeec for model gemma-7b received
2024-09-18 12:59:08,959 127.0.0.1 - - [18/Sep/2024 12:59:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,176 Request with ID 0f48d261 for model gemma-7b received
2024-09-18 12:59:09,177 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,190 Request with ID 7df5767f for model llama3-8b received
2024-09-18 12:59:09,190 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,430 Request with ID 0d0be547 for model granite-7b received
2024-09-18 12:59:09,431 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,536 Request with ID 240702ff for model granite-7b received
2024-09-18 12:59:09,537 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,574 Request with ID 74500b3f for model llama3-8b received
2024-09-18 12:59:09,575 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,631 Request with ID 600fce6b for model llama3-8b received
2024-09-18 12:59:09,632 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,651 Request with ID 706309bb for model granite-7b received
2024-09-18 12:59:09,651 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,705 Request with ID e29ab6e6 for model granite-7b received
2024-09-18 12:59:09,706 Batch size condition met for model granite-7b
2024-09-18 12:59:09,706 Next: call load_model for granite-7b
2024-09-18 12:59:09,707 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 402, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/granite-7b'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 230, in process_batch
    load_model(model_alias)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 107, in load_model
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 834, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 666, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 466, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './models/granite-7b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
2024-09-18 12:59:09,710 127.0.0.1 - - [18/Sep/2024 12:59:09] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 12:59:09,734 Request with ID 7bf90db0 for model gemma-7b received
2024-09-18 12:59:09,735 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:09,856 Request with ID f7a67cc4 for model granite-7b received
2024-09-18 12:59:09,857 127.0.0.1 - - [18/Sep/2024 12:59:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:10,468 Request with ID b1f0d343 for model granite-7b received
2024-09-18 12:59:10,469 127.0.0.1 - - [18/Sep/2024 12:59:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:10,846 Request with ID bd233bc7 for model llama3-8b received
2024-09-18 12:59:10,847 127.0.0.1 - - [18/Sep/2024 12:59:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:10,881 Request with ID 64260d94 for model gemma-7b received
2024-09-18 12:59:10,881 Batch size condition met for model gemma-7b
2024-09-18 12:59:10,881 Next: call load_model for gemma-7b
2024-09-18 12:59:10,882 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 402, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/gemma-7b'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 230, in process_batch
    load_model(model_alias)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 107, in load_model
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 834, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 666, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/transformers/utils/hub.py", line 466, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './models/gemma-7b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
2024-09-18 12:59:10,884 127.0.0.1 - - [18/Sep/2024 12:59:10] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 12:59:10,958 Request with ID d9828151 for model gemma-7b received
2024-09-18 12:59:10,958 127.0.0.1 - - [18/Sep/2024 12:59:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:10,987 Request with ID 770efe31 for model llama3-8b received
2024-09-18 12:59:10,988 127.0.0.1 - - [18/Sep/2024 12:59:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,133 Request with ID 563b3290 for model granite-7b received
2024-09-18 12:59:11,134 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,265 Request with ID 5757967e for model gemma-7b received
2024-09-18 12:59:11,266 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,344 Request with ID c6532bf6 for model granite-7b received
2024-09-18 12:59:11,345 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,386 Request with ID d5c06aab for model llama3-8b received
2024-09-18 12:59:11,386 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,412 Request with ID 0cac9457 for model llama3-8b received
2024-09-18 12:59:11,413 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,581 Request with ID 425398a5 for model granite-7b received
2024-09-18 12:59:11,581 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:59:11,700 Request with ID 7e6576eb for model llama3-8b received
2024-09-18 12:59:11,700 127.0.0.1 - - [18/Sep/2024 12:59:11] "POST /inference HTTP/1.1" 200 -
