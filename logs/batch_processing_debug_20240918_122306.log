2024-09-18 12:23:06,655 Using device: cpu
2024-09-18 12:23:06,655 Scheduling mode set as batchedFCFS
2024-09-18 12:23:06,679 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 12:23:06,679 [33mPress CTRL+C to quit[0m
2024-09-18 12:23:10,298 Request with ID 6fa320c3 for model gpt2medium-355m received
2024-09-18 12:23:10,298 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,431 Request with ID 8a993dcd for model distilgpt2-124m received
2024-09-18 12:23:10,431 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,449 Request with ID 32b2324d for model gpt2medium-355m received
2024-09-18 12:23:10,450 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,654 Request with ID 6cff9b16 for model distilgpt2-124m received
2024-09-18 12:23:10,654 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,660 Request with ID 0f091ad9 for model distilgpt2-124m received
2024-09-18 12:23:10,661 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,686 Request with ID 08d06b0f for model gpt2-124m received
2024-09-18 12:23:10,686 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,784 Request with ID 0028cc10 for model gpt2-124m received
2024-09-18 12:23:10,784 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,842 Request with ID 58e6000c for model gpt2medium-355m received
2024-09-18 12:23:10,842 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,895 Request with ID 3a446e76 for model gpt2medium-355m received
2024-09-18 12:23:10,895 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:10,958 Request with ID d28c9407 for model gpt2medium-355m received
2024-09-18 12:23:10,958 127.0.0.1 - - [18/Sep/2024 12:23:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:11,369 Request with ID 5f695fcf for model gpt2medium-355m received
2024-09-18 12:23:11,369 127.0.0.1 - - [18/Sep/2024 12:23:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:11,567 Request with ID c6a4185c for model gpt2-124m received
2024-09-18 12:23:11,567 127.0.0.1 - - [18/Sep/2024 12:23:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:11,630 Request with ID 933c8ed0 for model gpt2-124m received
2024-09-18 12:23:11,631 127.0.0.1 - - [18/Sep/2024 12:23:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:11,970 Request with ID ff179ff2 for model distilgpt2-124m received
2024-09-18 12:23:11,971 127.0.0.1 - - [18/Sep/2024 12:23:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,010 Request with ID b626eb05 for model gpt2medium-355m received
2024-09-18 12:23:12,011 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,126 Request with ID acbe83b8 for model gpt2medium-355m received
2024-09-18 12:23:12,127 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:12,127 Next: call load_model for gpt2medium-355m
2024-09-18 12:23:12,147 Request with ID bc7806cf for model distilgpt2-124m received
2024-09-18 12:23:12,148 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,254 Request with ID f8bc5708 for model distilgpt2-124m received
2024-09-18 12:23:12,254 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,279 Request with ID be051a07 for model gpt2medium-355m received
2024-09-18 12:23:12,279 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,282 Loaded model gpt2medium-355m
2024-09-18 12:23:12,282 Batch processing started for model gpt2medium-355m
2024-09-18 12:23:12,464 Request with ID 7de8159c for model distilgpt2-124m received
2024-09-18 12:23:12,464 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,515 Request with ID febef879 for model distilgpt2-124m received
2024-09-18 12:23:12,516 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:12,647 Request with ID 544004c3 for model distilgpt2-124m received
2024-09-18 12:23:12,647 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,670 Request with ID e70b3104 for model distilgpt2-124m received
2024-09-18 12:23:12,670 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,711 Request with ID 3ece79ce for model gpt2medium-355m received
2024-09-18 12:23:12,711 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,888 Request with ID f5213f2a for model gpt2-124m received
2024-09-18 12:23:12,889 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,905 Request with ID cb1a77a2 for model gpt2-124m received
2024-09-18 12:23:12,905 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,913 Request with ID dc1452f2 for model gpt2-124m received
2024-09-18 12:23:12,913 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:12,937 Request with ID bdd26cf6 for model gpt2medium-355m received
2024-09-18 12:23:12,937 127.0.0.1 - - [18/Sep/2024 12:23:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,108 Request with ID daab37c3 for model gpt2medium-355m received
2024-09-18 12:23:13,108 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,265 Request with ID 15e57b29 for model gpt2-124m received
2024-09-18 12:23:13,265 Batch size condition met for model gpt2-124m
2024-09-18 12:23:13,527 Request with ID 1bfff8b2 for model gpt2medium-355m received
2024-09-18 12:23:13,527 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,719 Request with ID b47961c3 for model distilgpt2-124m received
2024-09-18 12:23:13,719 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,729 Request with ID e1aedb6e for model gpt2medium-355m received
2024-09-18 12:23:13,730 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,841 Request with ID b0c07604 for model gpt2medium-355m received
2024-09-18 12:23:13,841 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,932 Request with ID ec759f30 for model gpt2-124m received
2024-09-18 12:23:13,932 127.0.0.1 - - [18/Sep/2024 12:23:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:13,982 Request with ID 657919c1 for model gpt2medium-355m received
2024-09-18 12:23:13,982 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:14,126 Request with ID 27f6617c for model gpt2medium-355m received
2024-09-18 12:23:14,126 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,157 Request with ID 9002f76d for model gpt2medium-355m received
2024-09-18 12:23:14,157 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,214 Request with ID 5eb37626 for model gpt2medium-355m received
2024-09-18 12:23:14,214 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,217 Request with ID fe611901 for model gpt2-124m received
2024-09-18 12:23:14,217 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,291 Request with ID 3bf21f60 for model gpt2-124m received
2024-09-18 12:23:14,291 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,320 Request with ID 151c8b51 for model gpt2medium-355m received
2024-09-18 12:23:14,320 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,344 Request with ID 3f8681ee for model gpt2medium-355m received
2024-09-18 12:23:14,344 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,380 Request with ID 5fcf5541 for model distilgpt2-124m received
2024-09-18 12:23:14,380 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,479 Request with ID a6534da2 for model gpt2-124m received
2024-09-18 12:23:14,479 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,491 Request with ID 0d270f8f for model distilgpt2-124m received
2024-09-18 12:23:14,492 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,708 Request with ID c77d693f for model distilgpt2-124m received
2024-09-18 12:23:14,708 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,722 Request with ID 8f7966b1 for model gpt2medium-355m received
2024-09-18 12:23:14,722 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:14,961 Request with ID 1170592c for model gpt2-124m received
2024-09-18 12:23:14,961 127.0.0.1 - - [18/Sep/2024 12:23:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,066 Request with ID ffc73970 for model gpt2-124m received
2024-09-18 12:23:15,067 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,105 Request with ID 5e38f02a for model gpt2medium-355m received
2024-09-18 12:23:15,105 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,159 Request with ID 5aa3ad76 for model gpt2medium-355m received
2024-09-18 12:23:15,160 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:15,179 Request with ID db0e502b for model gpt2-124m received
2024-09-18 12:23:15,179 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,232 Request with ID 30eea8fd for model gpt2-124m received
2024-09-18 12:23:15,232 Batch size condition met for model gpt2-124m
2024-09-18 12:23:15,263 Request with ID b937aace for model distilgpt2-124m received
2024-09-18 12:23:15,263 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,384 Request with ID 01f05f26 for model gpt2-124m received
2024-09-18 12:23:15,384 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:15,994 Request with ID 5368fb17 for model gpt2-124m received
2024-09-18 12:23:15,994 127.0.0.1 - - [18/Sep/2024 12:23:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,371 Request with ID a370cb18 for model gpt2medium-355m received
2024-09-18 12:23:16,371 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,405 Request with ID 46fe78a6 for model distilgpt2-124m received
2024-09-18 12:23:16,405 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:16,482 Request with ID f1dbdc13 for model distilgpt2-124m received
2024-09-18 12:23:16,483 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,511 Request with ID 14730f8c for model gpt2medium-355m received
2024-09-18 12:23:16,511 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,656 Request with ID a406d9b6 for model gpt2-124m received
2024-09-18 12:23:16,656 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,788 Request with ID fa06ad82 for model distilgpt2-124m received
2024-09-18 12:23:16,788 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,866 Request with ID 0b816c96 for model gpt2-124m received
2024-09-18 12:23:16,866 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,906 Request with ID 15fa3568 for model gpt2medium-355m received
2024-09-18 12:23:16,906 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:16,932 Request with ID d153e44f for model gpt2medium-355m received
2024-09-18 12:23:16,932 127.0.0.1 - - [18/Sep/2024 12:23:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,100 Request with ID 2b305159 for model gpt2-124m received
2024-09-18 12:23:17,100 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,218 Request with ID e379bf31 for model gpt2medium-355m received
2024-09-18 12:23:17,218 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,496 Request with ID d1e4de86 for model gpt2medium-355m received
2024-09-18 12:23:17,496 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,539 Request with ID f3ec3ba9 for model gpt2medium-355m received
2024-09-18 12:23:17,539 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,618 Request with ID a7a7a879 for model gpt2medium-355m received
2024-09-18 12:23:17,618 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:17,728 Request with ID 113bd6be for model gpt2-124m received
2024-09-18 12:23:17,728 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:17,802 Request with ID 19f8d429 for model distilgpt2-124m received
2024-09-18 12:23:17,802 127.0.0.1 - - [18/Sep/2024 12:23:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:18,362 Request with ID 28bfef6c for model gpt2-124m received
2024-09-18 12:23:18,362 127.0.0.1 - - [18/Sep/2024 12:23:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:18,515 Processed batch: ['6fa320c3', '32b2324d', '58e6000c', '3a446e76', 'd28c9407', '5f695fcf', 'b626eb05', 'acbe83b8'] with model gpt2medium-355m in 6.2326 seconds
2024-09-18 12:23:18,515 Latency for request 6fa320c3 with model gpt2medium-355m: 8.2165 seconds
2024-09-18 12:23:18,515 Saving results without gpu monitoring
2024-09-18 12:23:18,517 Latency for request 32b2324d with model gpt2medium-355m: 8.0655 seconds
2024-09-18 12:23:18,517 Saving results without gpu monitoring
2024-09-18 12:23:18,518 Latency for request 58e6000c with model gpt2medium-355m: 7.6731 seconds
2024-09-18 12:23:18,518 Saving results without gpu monitoring
2024-09-18 12:23:18,518 Latency for request 3a446e76 with model gpt2medium-355m: 7.6200 seconds
2024-09-18 12:23:18,518 Saving results without gpu monitoring
2024-09-18 12:23:18,518 Latency for request d28c9407 with model gpt2medium-355m: 7.5570 seconds
2024-09-18 12:23:18,518 Saving results without gpu monitoring
2024-09-18 12:23:18,518 Latency for request 5f695fcf with model gpt2medium-355m: 7.1462 seconds
2024-09-18 12:23:18,519 Saving results without gpu monitoring
2024-09-18 12:23:18,519 Latency for request b626eb05 with model gpt2medium-355m: 6.5048 seconds
2024-09-18 12:23:18,519 Saving results without gpu monitoring
2024-09-18 12:23:18,519 Latency for request acbe83b8 with model gpt2medium-355m: 6.3889 seconds
2024-09-18 12:23:18,519 Saving results without gpu monitoring
2024-09-18 12:23:18,519 127.0.0.1 - - [18/Sep/2024 12:23:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:18,519 Next: call load_model for distilgpt2-124m
2024-09-18 12:23:18,532 Unloaded previous model
2024-09-18 12:23:18,577 Request with ID c190f452 for model gpt2-124m received
2024-09-18 12:23:18,577 Batch size condition met for model gpt2-124m
2024-09-18 12:23:18,584 Loaded model distilgpt2-124m
2024-09-18 12:23:18,584 Batch processing started for model distilgpt2-124m
2024-09-18 12:23:18,621 Request with ID 52247f49 for model distilgpt2-124m received
2024-09-18 12:23:18,622 127.0.0.1 - - [18/Sep/2024 12:23:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:18,774 Request with ID da456267 for model gpt2medium-355m received
2024-09-18 12:23:18,774 127.0.0.1 - - [18/Sep/2024 12:23:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,050 Request with ID bcee407c for model distilgpt2-124m received
2024-09-18 12:23:19,050 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,162 Request with ID 0f8140b9 for model gpt2medium-355m received
2024-09-18 12:23:19,162 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,210 Request with ID 88dbb4ec for model gpt2medium-355m received
2024-09-18 12:23:19,210 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,575 Request with ID 9dd01191 for model distilgpt2-124m received
2024-09-18 12:23:19,575 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,656 Request with ID 1cd81b2c for model gpt2-124m received
2024-09-18 12:23:19,656 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,677 Request with ID 27faf4fe for model gpt2-124m received
2024-09-18 12:23:19,677 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,746 Request with ID be4c3e09 for model gpt2medium-355m received
2024-09-18 12:23:19,747 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,875 Request with ID 5fa9594a for model gpt2-124m received
2024-09-18 12:23:19,876 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:19,955 Request with ID b61b022f for model gpt2medium-355m received
2024-09-18 12:23:19,955 127.0.0.1 - - [18/Sep/2024 12:23:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,057 Request with ID 9c26f909 for model distilgpt2-124m received
2024-09-18 12:23:20,057 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,147 Request with ID 7bf3e740 for model distilgpt2-124m received
2024-09-18 12:23:20,147 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:20,220 Processed batch: ['544004c3', 'e70b3104', 'b47961c3', '5fcf5541', '0d270f8f', 'c77d693f', 'b937aace', '46fe78a6'] with model distilgpt2-124m in 1.6361 seconds
2024-09-18 12:23:20,220 Latency for request 544004c3 with model distilgpt2-124m: 7.5729 seconds
2024-09-18 12:23:20,220 Saving results without gpu monitoring
2024-09-18 12:23:20,221 Latency for request e70b3104 with model distilgpt2-124m: 7.5499 seconds
2024-09-18 12:23:20,221 Saving results without gpu monitoring
2024-09-18 12:23:20,221 Latency for request b47961c3 with model distilgpt2-124m: 6.5013 seconds
2024-09-18 12:23:20,221 Saving results without gpu monitoring
2024-09-18 12:23:20,221 Latency for request 5fcf5541 with model distilgpt2-124m: 5.8399 seconds
2024-09-18 12:23:20,222 Saving results without gpu monitoring
2024-09-18 12:23:20,222 Latency for request 0d270f8f with model distilgpt2-124m: 5.7285 seconds
2024-09-18 12:23:20,222 Saving results without gpu monitoring
2024-09-18 12:23:20,222 Latency for request c77d693f with model distilgpt2-124m: 5.5119 seconds
2024-09-18 12:23:20,222 Saving results without gpu monitoring
2024-09-18 12:23:20,222 Latency for request b937aace with model distilgpt2-124m: 4.9573 seconds
2024-09-18 12:23:20,222 Saving results without gpu monitoring
2024-09-18 12:23:20,223 Latency for request 46fe78a6 with model distilgpt2-124m: 3.8146 seconds
2024-09-18 12:23:20,223 Saving results without gpu monitoring
2024-09-18 12:23:20,223 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,223 Next: call load_model for gpt2-124m
2024-09-18 12:23:20,229 Unloaded previous model
2024-09-18 12:23:20,296 Loaded model gpt2-124m
2024-09-18 12:23:20,296 Batch processing started for model gpt2-124m
2024-09-18 12:23:20,385 Request with ID fa6c1306 for model distilgpt2-124m received
2024-09-18 12:23:20,385 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,429 Request with ID a7d3bd6c for model gpt2-124m received
2024-09-18 12:23:20,429 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,452 Request with ID a71c9dc0 for model gpt2medium-355m received
2024-09-18 12:23:20,452 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,538 Request with ID 6f44c72a for model gpt2-124m received
2024-09-18 12:23:20,539 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,543 Request with ID fdb21335 for model distilgpt2-124m received
2024-09-18 12:23:20,543 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:20,853 Request with ID 2b4858b0 for model gpt2-124m received
2024-09-18 12:23:20,853 127.0.0.1 - - [18/Sep/2024 12:23:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,160 Request with ID d99de3ed for model gpt2-124m received
2024-09-18 12:23:21,160 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,247 Request with ID 8da1782f for model gpt2-124m received
2024-09-18 12:23:21,247 Batch size condition met for model gpt2-124m
2024-09-18 12:23:21,256 Request with ID 9c80b580 for model gpt2medium-355m received
2024-09-18 12:23:21,256 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,288 Request with ID 6ecf96c0 for model distilgpt2-124m received
2024-09-18 12:23:21,288 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,370 Request with ID d4c39967 for model distilgpt2-124m received
2024-09-18 12:23:21,370 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,491 Request with ID 13902646 for model distilgpt2-124m received
2024-09-18 12:23:21,491 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,502 Request with ID 988e5412 for model distilgpt2-124m received
2024-09-18 12:23:21,502 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,533 Request with ID 350a626c for model distilgpt2-124m received
2024-09-18 12:23:21,533 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,669 Request with ID 05394eda for model gpt2medium-355m received
2024-09-18 12:23:21,670 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:21,706 Request with ID 1933c3c3 for model gpt2medium-355m received
2024-09-18 12:23:21,706 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,737 Request with ID 0a652c9e for model distilgpt2-124m received
2024-09-18 12:23:21,737 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:21,737 Request with ID 6c095fea for model gpt2-124m received
2024-09-18 12:23:21,737 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,760 Request with ID ad75ad81 for model distilgpt2-124m received
2024-09-18 12:23:21,761 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,983 Request with ID 67c325d8 for model distilgpt2-124m received
2024-09-18 12:23:21,983 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:21,993 Request with ID 95a70c8b for model distilgpt2-124m received
2024-09-18 12:23:21,993 127.0.0.1 - - [18/Sep/2024 12:23:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,036 Request with ID 1f56909d for model distilgpt2-124m received
2024-09-18 12:23:22,036 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,070 Request with ID e881de69 for model distilgpt2-124m received
2024-09-18 12:23:22,070 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,245 Request with ID ffb5dbf8 for model gpt2medium-355m received
2024-09-18 12:23:22,245 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,260 Request with ID 3df267b8 for model gpt2-124m received
2024-09-18 12:23:22,260 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,321 Request with ID 251c934f for model gpt2-124m received
2024-09-18 12:23:22,321 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,344 Request with ID e9b09574 for model gpt2medium-355m received
2024-09-18 12:23:22,345 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,447 Request with ID 9dc8929d for model distilgpt2-124m received
2024-09-18 12:23:22,447 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,521 Request with ID d41d8da9 for model gpt2medium-355m received
2024-09-18 12:23:22,521 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,681 Request with ID e9184567 for model gpt2medium-355m received
2024-09-18 12:23:22,681 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,741 Request with ID 71eb6b3f for model distilgpt2-124m received
2024-09-18 12:23:22,741 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,743 Request with ID bc7ae38c for model distilgpt2-124m received
2024-09-18 12:23:22,743 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:22,900 Processed batch: ['01f05f26', '5368fb17', 'a406d9b6', '0b816c96', '2b305159', '113bd6be', '28bfef6c', 'c190f452'] with model gpt2-124m in 2.6041 seconds
2024-09-18 12:23:22,900 Latency for request 01f05f26 with model gpt2-124m: 7.5163 seconds
2024-09-18 12:23:22,900 Saving results without gpu monitoring
2024-09-18 12:23:22,901 Latency for request 5368fb17 with model gpt2-124m: 6.9059 seconds
2024-09-18 12:23:22,901 Saving results without gpu monitoring
2024-09-18 12:23:22,901 Latency for request a406d9b6 with model gpt2-124m: 6.2439 seconds
2024-09-18 12:23:22,901 Saving results without gpu monitoring
2024-09-18 12:23:22,901 Latency for request 0b816c96 with model gpt2-124m: 6.0338 seconds
2024-09-18 12:23:22,901 Saving results without gpu monitoring
2024-09-18 12:23:22,901 Latency for request 2b305159 with model gpt2-124m: 5.8002 seconds
2024-09-18 12:23:22,902 Saving results without gpu monitoring
2024-09-18 12:23:22,902 Latency for request 113bd6be with model gpt2-124m: 5.1720 seconds
2024-09-18 12:23:22,902 Saving results without gpu monitoring
2024-09-18 12:23:22,902 Latency for request 28bfef6c with model gpt2-124m: 4.5380 seconds
2024-09-18 12:23:22,902 Saving results without gpu monitoring
2024-09-18 12:23:22,902 Latency for request c190f452 with model gpt2-124m: 4.3231 seconds
2024-09-18 12:23:22,902 Saving results without gpu monitoring
2024-09-18 12:23:22,903 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,903 Next: call load_model for gpt2medium-355m
2024-09-18 12:23:22,911 Unloaded previous model
2024-09-18 12:23:22,944 Request with ID bad19190 for model gpt2-124m received
2024-09-18 12:23:22,944 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:22,945 Request with ID 92a26d55 for model gpt2-124m received
2024-09-18 12:23:22,946 127.0.0.1 - - [18/Sep/2024 12:23:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,015 Request with ID 0f2c68f7 for model distilgpt2-124m received
2024-09-18 12:23:23,015 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,015 Request with ID bf4a58e0 for model gpt2-124m received
2024-09-18 12:23:23,015 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,037 Request with ID 6a03f6a2 for model gpt2medium-355m received
2024-09-18 12:23:23,037 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,054 Loaded model gpt2medium-355m
2024-09-18 12:23:23,054 Batch processing started for model gpt2medium-355m
2024-09-18 12:23:23,145 Request with ID 43f4562e for model distilgpt2-124m received
2024-09-18 12:23:23,145 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,277 Request with ID 3367ce39 for model gpt2medium-355m received
2024-09-18 12:23:23,277 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,421 Request with ID 12076001 for model gpt2-124m received
2024-09-18 12:23:23,421 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,431 Request with ID 10aba4ae for model distilgpt2-124m received
2024-09-18 12:23:23,432 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,821 Request with ID 13ce3e31 for model distilgpt2-124m received
2024-09-18 12:23:23,822 127.0.0.1 - - [18/Sep/2024 12:23:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:23,967 Request with ID 1015fb8e for model gpt2-124m received
2024-09-18 12:23:23,967 Batch size condition met for model gpt2-124m
2024-09-18 12:23:24,031 Request with ID 813fd101 for model gpt2medium-355m received
2024-09-18 12:23:24,031 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:24,036 Request with ID 59a35ba6 for model gpt2-124m received
2024-09-18 12:23:24,036 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:24,256 Request with ID 978f5488 for model gpt2-124m received
2024-09-18 12:23:24,256 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:24,335 Request with ID cd93cd31 for model gpt2medium-355m received
2024-09-18 12:23:24,335 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:24,625 Request with ID 123221ae for model gpt2medium-355m received
2024-09-18 12:23:24,625 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:24,787 Request with ID ce0905c5 for model distilgpt2-124m received
2024-09-18 12:23:24,787 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:24,873 Request with ID 16f18d14 for model gpt2-124m received
2024-09-18 12:23:24,873 127.0.0.1 - - [18/Sep/2024 12:23:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,042 Request with ID f2b51e5e for model gpt2medium-355m received
2024-09-18 12:23:25,042 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,215 Request with ID 97a03c4a for model distilgpt2-124m received
2024-09-18 12:23:25,215 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,240 Request with ID 3d6c9c3b for model gpt2-124m received
2024-09-18 12:23:25,240 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,264 Request with ID 28db938f for model gpt2medium-355m received
2024-09-18 12:23:25,264 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,306 Request with ID e526275f for model gpt2-124m received
2024-09-18 12:23:25,306 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,494 Request with ID 6f1f1167 for model distilgpt2-124m received
2024-09-18 12:23:25,494 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,664 Request with ID 1a351188 for model gpt2medium-355m received
2024-09-18 12:23:25,664 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,678 Request with ID 468663e8 for model gpt2-124m received
2024-09-18 12:23:25,678 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:25,955 Request with ID 0214b4e8 for model gpt2-124m received
2024-09-18 12:23:25,955 127.0.0.1 - - [18/Sep/2024 12:23:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:26,299 Request with ID 58c4a693 for model distilgpt2-124m received
2024-09-18 12:23:26,300 Batch size condition met for model distilgpt2-124m
2024-09-18 12:23:26,356 Request with ID 366b143e for model distilgpt2-124m received
2024-09-18 12:23:26,356 127.0.0.1 - - [18/Sep/2024 12:23:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:26,386 Request with ID 62ebf739 for model gpt2medium-355m received
2024-09-18 12:23:26,386 127.0.0.1 - - [18/Sep/2024 12:23:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:26,654 Request with ID 1c3c3974 for model gpt2-124m received
2024-09-18 12:23:26,654 Batch size condition met for model gpt2-124m
2024-09-18 12:23:26,770 Request with ID daf493d4 for model distilgpt2-124m received
2024-09-18 12:23:26,770 127.0.0.1 - - [18/Sep/2024 12:23:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,003 Request with ID b107e93b for model gpt2-124m received
2024-09-18 12:23:27,004 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,044 Request with ID b9d874ac for model gpt2medium-355m received
2024-09-18 12:23:27,044 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,095 Request with ID b6c85e0a for model gpt2-124m received
2024-09-18 12:23:27,095 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,331 Request with ID 387d3e4d for model gpt2-124m received
2024-09-18 12:23:27,332 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,372 Request with ID 936f71f4 for model gpt2medium-355m received
2024-09-18 12:23:27,372 Batch size condition met for model gpt2medium-355m
2024-09-18 12:23:27,674 Request with ID c079103c for model distilgpt2-124m received
2024-09-18 12:23:27,674 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,775 Request with ID 46caf3c1 for model gpt2-124m received
2024-09-18 12:23:27,775 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,810 Request with ID 92cb2b07 for model gpt2-124m received
2024-09-18 12:23:27,811 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:27,969 Request with ID 8ea8e11b for model gpt2-124m received
2024-09-18 12:23:27,970 127.0.0.1 - - [18/Sep/2024 12:23:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:28,238 Request with ID 718aca63 for model gpt2medium-355m received
2024-09-18 12:23:28,238 127.0.0.1 - - [18/Sep/2024 12:23:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:28,790 Request with ID b69414e2 for model gpt2-124m received
2024-09-18 12:23:28,790 127.0.0.1 - - [18/Sep/2024 12:23:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:28,926 Request with ID 824cad6f for model gpt2medium-355m received
2024-09-18 12:23:28,926 127.0.0.1 - - [18/Sep/2024 12:23:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,049 Request with ID 11a90627 for model gpt2-124m received
2024-09-18 12:23:29,050 Batch size condition met for model gpt2-124m
2024-09-18 12:23:29,194 Request with ID 3d3fd6b0 for model distilgpt2-124m received
2024-09-18 12:23:29,194 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,317 Request with ID d618068f for model distilgpt2-124m received
2024-09-18 12:23:29,317 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,540 Request with ID ce3117af for model gpt2-124m received
2024-09-18 12:23:29,540 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,613 Request with ID 6c683335 for model gpt2medium-355m received
2024-09-18 12:23:29,614 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,702 Request with ID d3096450 for model gpt2-124m received
2024-09-18 12:23:29,702 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:29,915 Request with ID c2cac56c for model gpt2medium-355m received
2024-09-18 12:23:29,915 127.0.0.1 - - [18/Sep/2024 12:23:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:30,155 Request with ID 6c7715e9 for model distilgpt2-124m received
2024-09-18 12:23:30,156 127.0.0.1 - - [18/Sep/2024 12:23:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:30,167 Processed batch: ['da456267', '0f8140b9', '88dbb4ec', 'be4c3e09', 'b61b022f', 'a71c9dc0', '9c80b580', '05394eda'] with model gpt2medium-355m in 7.1129 seconds
2024-09-18 12:23:30,167 Latency for request da456267 with model gpt2medium-355m: 11.3925 seconds
2024-09-18 12:23:30,167 Saving results without gpu monitoring
2024-09-18 12:23:30,167 Latency for request 0f8140b9 with model gpt2medium-355m: 11.0047 seconds
2024-09-18 12:23:30,167 Saving results without gpu monitoring
2024-09-18 12:23:30,168 Latency for request 88dbb4ec with model gpt2medium-355m: 10.9570 seconds
2024-09-18 12:23:30,168 Saving results without gpu monitoring
2024-09-18 12:23:30,168 Latency for request be4c3e09 with model gpt2medium-355m: 10.4203 seconds
2024-09-18 12:23:30,168 Saving results without gpu monitoring
2024-09-18 12:23:30,168 Latency for request b61b022f with model gpt2medium-355m: 10.2113 seconds
2024-09-18 12:23:30,168 Saving results without gpu monitoring
2024-09-18 12:23:30,169 Latency for request a71c9dc0 with model gpt2medium-355m: 9.7144 seconds
2024-09-18 12:23:30,169 Saving results without gpu monitoring
2024-09-18 12:23:30,169 Latency for request 9c80b580 with model gpt2medium-355m: 8.9111 seconds
2024-09-18 12:23:30,169 Saving results without gpu monitoring
2024-09-18 12:23:30,169 Latency for request 05394eda with model gpt2medium-355m: 8.4972 seconds
2024-09-18 12:23:30,169 Saving results without gpu monitoring
2024-09-18 12:23:30,170 127.0.0.1 - - [18/Sep/2024 12:23:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:30,170 Next: call load_model for gpt2medium-355m
2024-09-18 12:23:30,170 Model gpt2medium-355m already loaded
2024-09-18 12:23:30,170 Batch processing started for model gpt2medium-355m
2024-09-18 12:23:30,279 Request with ID 1818bf11 for model gpt2-124m received
2024-09-18 12:23:30,279 127.0.0.1 - - [18/Sep/2024 12:23:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:30,299 Waiting for running processes to finish
2024-09-18 12:23:31,302 Waiting for running processes to finish
2024-09-18 12:23:32,307 Waiting for running processes to finish
2024-09-18 12:23:33,312 Waiting for running processes to finish
2024-09-18 12:23:34,317 Waiting for running processes to finish
2024-09-18 12:23:35,318 Waiting for running processes to finish
2024-09-18 12:23:36,239 Processed batch: ['cd93cd31', '123221ae', 'f2b51e5e', '28db938f', '1a351188', '62ebf739', 'b9d874ac', '936f71f4'] with model gpt2medium-355m in 6.0689 seconds
2024-09-18 12:23:36,239 Latency for request cd93cd31 with model gpt2medium-355m: 11.9042 seconds
2024-09-18 12:23:36,239 Saving results without gpu monitoring
2024-09-18 12:23:36,240 Latency for request 123221ae with model gpt2medium-355m: 11.6142 seconds
2024-09-18 12:23:36,240 Saving results without gpu monitoring
2024-09-18 12:23:36,240 Latency for request f2b51e5e with model gpt2medium-355m: 11.1972 seconds
2024-09-18 12:23:36,240 Saving results without gpu monitoring
2024-09-18 12:23:36,241 Latency for request 28db938f with model gpt2medium-355m: 10.9749 seconds
2024-09-18 12:23:36,241 Saving results without gpu monitoring
2024-09-18 12:23:36,241 Latency for request 1a351188 with model gpt2medium-355m: 10.5745 seconds
2024-09-18 12:23:36,241 Saving results without gpu monitoring
2024-09-18 12:23:36,241 Latency for request 62ebf739 with model gpt2medium-355m: 9.8526 seconds
2024-09-18 12:23:36,241 Saving results without gpu monitoring
2024-09-18 12:23:36,241 Latency for request b9d874ac with model gpt2medium-355m: 9.1951 seconds
2024-09-18 12:23:36,241 Saving results without gpu monitoring
2024-09-18 12:23:36,242 Latency for request 936f71f4 with model gpt2medium-355m: 8.8672 seconds
2024-09-18 12:23:36,242 Saving results without gpu monitoring
2024-09-18 12:23:36,242 127.0.0.1 - - [18/Sep/2024 12:23:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:36,242 Next: call load_model for gpt2-124m
2024-09-18 12:23:36,259 Unloaded previous model
2024-09-18 12:23:36,325 Waiting for running processes to finish
2024-09-18 12:23:36,334 Loaded model gpt2-124m
2024-09-18 12:23:36,334 Batch processing started for model gpt2-124m
2024-09-18 12:23:37,331 Waiting for running processes to finish
2024-09-18 12:23:38,336 Waiting for running processes to finish
2024-09-18 12:23:38,623 Processed batch: ['b107e93b', 'b6c85e0a', '387d3e4d', '46caf3c1', '92cb2b07', '8ea8e11b', 'b69414e2', '11a90627'] with model gpt2-124m in 2.2897 seconds
2024-09-18 12:23:38,623 Latency for request b107e93b with model gpt2-124m: 11.6199 seconds
2024-09-18 12:23:38,623 Saving results without gpu monitoring
2024-09-18 12:23:38,624 Latency for request b6c85e0a with model gpt2-124m: 11.5287 seconds
2024-09-18 12:23:38,624 Saving results without gpu monitoring
2024-09-18 12:23:38,625 Latency for request 387d3e4d with model gpt2-124m: 11.2919 seconds
2024-09-18 12:23:38,625 Saving results without gpu monitoring
2024-09-18 12:23:38,625 Latency for request 46caf3c1 with model gpt2-124m: 10.8483 seconds
2024-09-18 12:23:38,625 Saving results without gpu monitoring
2024-09-18 12:23:38,625 Latency for request 92cb2b07 with model gpt2-124m: 10.8129 seconds
2024-09-18 12:23:38,625 Saving results without gpu monitoring
2024-09-18 12:23:38,626 Latency for request 8ea8e11b with model gpt2-124m: 10.6540 seconds
2024-09-18 12:23:38,626 Saving results without gpu monitoring
2024-09-18 12:23:38,626 Latency for request b69414e2 with model gpt2-124m: 9.8332 seconds
2024-09-18 12:23:38,626 Saving results without gpu monitoring
2024-09-18 12:23:38,626 Latency for request 11a90627 with model gpt2-124m: 9.5739 seconds
2024-09-18 12:23:38,626 Saving results without gpu monitoring
2024-09-18 12:23:38,627 127.0.0.1 - - [18/Sep/2024 12:23:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:38,627 Next: call load_model for distilgpt2-124m
2024-09-18 12:23:38,637 Unloaded previous model
2024-09-18 12:23:38,747 Loaded model distilgpt2-124m
2024-09-18 12:23:38,747 Batch processing started for model distilgpt2-124m
2024-09-18 12:23:39,341 Waiting for running processes to finish
2024-09-18 12:23:40,344 Total time: 28.3282 seconds
2024-09-18 12:23:40,344 Total inference time: 25.9442 seconds
2024-09-18 12:23:40,344 Inference time as percentage of total time: 91.58%
2024-09-18 12:23:40,344 END
2024-09-18 12:23:40,345 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,615 Processed batch: ['0f2c68f7', '43f4562e', '10aba4ae', '13ce3e31', 'ce0905c5', '97a03c4a', '6f1f1167', '58c4a693'] with model distilgpt2-124m in 1.8671 seconds
2024-09-18 12:23:40,615 Latency for request 0f2c68f7 with model distilgpt2-124m: 17.5998 seconds
2024-09-18 12:23:40,615 Saving results without gpu monitoring
2024-09-18 12:23:40,616 Latency for request 43f4562e with model distilgpt2-124m: 17.4697 seconds
2024-09-18 12:23:40,616 Saving results without gpu monitoring
2024-09-18 12:23:40,616 Latency for request 10aba4ae with model distilgpt2-124m: 17.1832 seconds
2024-09-18 12:23:40,616 Saving results without gpu monitoring
2024-09-18 12:23:40,617 Latency for request 13ce3e31 with model distilgpt2-124m: 16.7931 seconds
2024-09-18 12:23:40,617 Saving results without gpu monitoring
2024-09-18 12:23:40,617 Latency for request ce0905c5 with model distilgpt2-124m: 15.8273 seconds
2024-09-18 12:23:40,617 Saving results without gpu monitoring
2024-09-18 12:23:40,617 Latency for request 97a03c4a with model distilgpt2-124m: 15.3993 seconds
2024-09-18 12:23:40,617 Saving results without gpu monitoring
2024-09-18 12:23:40,617 Latency for request 6f1f1167 with model distilgpt2-124m: 15.1205 seconds
2024-09-18 12:23:40,617 Saving results without gpu monitoring
2024-09-18 12:23:40,618 Latency for request 58c4a693 with model distilgpt2-124m: 14.3152 seconds
2024-09-18 12:23:40,618 Saving results without gpu monitoring
2024-09-18 12:23:40,618 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,618 No batch to process for model gpt2medium-355m
2024-09-18 12:23:40,618 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,618 No batch to process for model gpt2-124m
2024-09-18 12:23:40,618 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,618 No batch to process for model distilgpt2-124m
2024-09-18 12:23:40,619 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,619 No batch to process for model gpt2-124m
2024-09-18 12:23:40,619 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,619 No batch to process for model gpt2medium-355m
2024-09-18 12:23:40,619 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,619 No batch to process for model distilgpt2-124m
2024-09-18 12:23:40,619 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model distilgpt2-124m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model gpt2-124m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model gpt2medium-355m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model distilgpt2-124m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model gpt2-124m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,620 No batch to process for model gpt2medium-355m
2024-09-18 12:23:40,620 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:23:40,621 No batch to process for model gpt2-124m
2024-09-18 12:23:40,621 127.0.0.1 - - [18/Sep/2024 12:23:40] "POST /inference HTTP/1.1" 200 -
