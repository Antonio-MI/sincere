2024-09-11 11:41:02,126 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-11 11:41:02,127 [33mPress CTRL+C to quit[0m
2024-09-11 11:41:07,720 Request with ID 8b5d6b8a for model gpt2-124m received
2024-09-11 11:41:07,720 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:07,720 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-11 11:41:07,720 127.0.0.1 - - [11/Sep/2024 11:41:07] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:07,748 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:07,749 Updated batch size:1
2024-09-11 11:41:07,749 Loading model gpt2-124m
2024-09-11 11:41:08,197 Processed batch: ['8b5d6b8a'] with model gpt2-124m in 0.3798 seconds
2024-09-11 11:41:08,198 Latency for request 8b5d6b8a with model gpt2-124m: 0.4774 seconds
2024-09-11 11:41:08,199 Total time: 0.4787 seconds
2024-09-11 11:41:08,199 Total inference time: 0.3798 seconds
2024-09-11 11:41:08,199 Inference time as percentage of total time: 79.33%
2024-09-11 11:41:08,199 Total time: 0.4788 seconds
2024-09-11 11:41:08,199 Total inference time: 0.3798 seconds
2024-09-11 11:41:08,199 Inference time as percentage of total time: 79.32%
2024-09-11 11:41:08,304 Total time: 0.5838 seconds
2024-09-11 11:41:08,304 Total inference time: 0.3798 seconds
2024-09-11 11:41:08,304 Inference time as percentage of total time: 65.04%
2024-09-11 11:41:08,409 Total time: 0.6889 seconds
2024-09-11 11:41:08,409 Total inference time: 0.3798 seconds
2024-09-11 11:41:08,409 Inference time as percentage of total time: 55.12%
2024-09-11 11:41:08,514 Total time: 0.7941 seconds
2024-09-11 11:41:08,514 Total inference time: 0.3798 seconds
2024-09-11 11:41:08,514 Inference time as percentage of total time: 47.82%
2024-09-11 11:41:08,546 Request with ID 6c97460e for model gpt2-124m received
2024-09-11 11:41:08,546 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:08,546 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-11 11:41:08,547 127.0.0.1 - - [11/Sep/2024 11:41:08] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:08,619 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:08,619 Updated batch size:1
2024-09-11 11:41:08,619 Loading model gpt2-124m
2024-09-11 11:41:08,877 Request with ID abc78c35 for model gpt2medium-355m received
2024-09-11 11:41:08,878 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:08,878 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-11 11:41:08,878 127.0.0.1 - - [11/Sep/2024 11:41:08] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:08,940 Request with ID 08ab089e for model gpt2-124m received
2024-09-11 11:41:08,941 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:08,941 127.0.0.1 - - [11/Sep/2024 11:41:08] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:08,962 Processed batch: ['6c97460e'] with model gpt2-124m in 0.3426 seconds
2024-09-11 11:41:08,962 Latency for request 6c97460e with model gpt2-124m: 0.4158 seconds
2024-09-11 11:41:09,068 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:09,068 Updated batch size:1
2024-09-11 11:41:09,068 Loading model gpt2-124m
2024-09-11 11:41:09,311 Request with ID d8e8c514 for model gpt2-124m received
2024-09-11 11:41:09,311 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:09,311 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-11 11:41:09,311 127.0.0.1 - - [11/Sep/2024 11:41:09] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:09,402 Processed batch: ['08ab089e'] with model gpt2-124m in 0.3344 seconds
2024-09-11 11:41:09,402 Latency for request 08ab089e with model gpt2-124m: 0.4619 seconds
2024-09-11 11:41:09,403 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:09,403 Updated batch size:1
2024-09-11 11:41:09,403 Loading model gpt2medium-355m
2024-09-11 11:41:09,974 Request with ID 3688d875 for model distilgpt2-124m received
2024-09-11 11:41:09,974 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:09,974 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-11 11:41:09,974 127.0.0.1 - - [11/Sep/2024 11:41:09] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:10,584 Request with ID 19def686 for model distilgpt2-124m received
2024-09-11 11:41:10,584 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-11 11:41:10,584 127.0.0.1 - - [11/Sep/2024 11:41:10] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:11,150 Request with ID 60b6b39c for model gpt2medium-355m received
2024-09-11 11:41:11,150 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:11,150 127.0.0.1 - - [11/Sep/2024 11:41:11] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:11,284 Processed batch: ['abc78c35'] with model gpt2medium-355m in 1.7752 seconds
2024-09-11 11:41:11,284 Latency for request abc78c35 with model gpt2medium-355m: 2.4066 seconds
2024-09-11 11:41:11,390 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:11,390 Updated batch size:1
2024-09-11 11:41:11,390 Loading model gpt2-124m
2024-09-11 11:41:11,543 Request with ID c76f120a for model distilgpt2-124m received
2024-09-11 11:41:11,543 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:11,543 127.0.0.1 - - [11/Sep/2024 11:41:11] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:11,852 Processed batch: ['d8e8c514'] with model gpt2-124m in 0.4008 seconds
2024-09-11 11:41:11,852 Latency for request d8e8c514 with model gpt2-124m: 2.5414 seconds
2024-09-11 11:41:11,853 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:11,853 Updated batch size:1
2024-09-11 11:41:11,853 Loading model gpt2medium-355m
2024-09-11 11:41:11,998 Request with ID 52881326 for model gpt2medium-355m received
2024-09-11 11:41:11,998 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:11,998 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-11 11:41:11,999 127.0.0.1 - - [11/Sep/2024 11:41:11] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:12,488 Request with ID 16b1e026 for model gpt2-124m received
2024-09-11 11:41:12,488 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-11 11:41:12,488 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-11 11:41:12,488 127.0.0.1 - - [11/Sep/2024 11:41:12] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:13,099 Request with ID c0005759 for model gpt2medium-355m received
2024-09-11 11:41:13,099 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:41:13,099 127.0.0.1 - - [11/Sep/2024 11:41:13] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:13,143 Processed batch: ['60b6b39c'] with model gpt2medium-355m in 1.1462 seconds
2024-09-11 11:41:13,143 Latency for request 60b6b39c with model gpt2medium-355m: 1.9924 seconds
2024-09-11 11:41:13,143 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:41:13,143 Updated batch size:4
2024-09-11 11:41:13,143 Loading model distilgpt2-124m
2024-09-11 11:41:13,462 Request with ID 3727cce6 for model gpt2-124m received
2024-09-11 11:41:13,462 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:13,462 127.0.0.1 - - [11/Sep/2024 11:41:13] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:13,799 Request with ID 126ad0d5 for model distilgpt2-124m received
2024-09-11 11:41:13,799 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-11 11:41:13,799 127.0.0.1 - - [11/Sep/2024 11:41:13] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:14,069 Processed batch: ['3688d875', '19def686', 'c76f120a', '3082'] with model distilgpt2-124m in 0.8691 seconds
2024-09-11 11:41:14,069 Latency for request 3688d875 with model distilgpt2-124m: 4.0949 seconds
2024-09-11 11:41:14,070 Latency for request 19def686 with model distilgpt2-124m: 3.4852 seconds
2024-09-11 11:41:14,070 Latency for request c76f120a with model distilgpt2-124m: 2.5263 seconds
2024-09-11 11:41:14,070 Latency for request 3082 with model distilgpt2-124m: 0.9255 seconds
2024-09-11 11:41:14,175 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:14,175 Updated batch size:2
2024-09-11 11:41:14,175 Loading model gpt2-124m
2024-09-11 11:41:14,870 Processed batch: ['16b1e026', '3727cce6'] with model gpt2-124m in 0.6362 seconds
2024-09-11 11:41:14,870 Latency for request 16b1e026 with model gpt2-124m: 2.3824 seconds
2024-09-11 11:41:14,871 Request with ID 69197691 for model gpt2-124m received
2024-09-11 11:41:14,871 Latency for request 3727cce6 with model gpt2-124m: 1.4081 seconds
2024-09-11 11:41:14,871 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:14,871 127.0.0.1 - - [11/Sep/2024 11:41:14] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:14,872 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:14,872 Updated batch size:2
2024-09-11 11:41:14,872 Loading model gpt2medium-355m
2024-09-11 11:41:16,070 Request with ID 7e88f27e for model gpt2-124m received
2024-09-11 11:41:16,070 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-11 11:41:16,070 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-11 11:41:16,070 127.0.0.1 - - [11/Sep/2024 11:41:16] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:16,803 Request with ID 831274da for model distilgpt2-124m received
2024-09-11 11:41:16,804 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:41:16,804 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-11 11:41:16,804 127.0.0.1 - - [11/Sep/2024 11:41:16] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:17,228 Processed batch: ['52881326', 'c0005759'] with model gpt2medium-355m in 2.2471 seconds
2024-09-11 11:41:17,228 Latency for request 52881326 with model gpt2medium-355m: 5.2295 seconds
2024-09-11 11:41:17,228 Latency for request c0005759 with model gpt2medium-355m: 4.1288 seconds
2024-09-11 11:41:17,229 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:41:17,229 Updated batch size:2
2024-09-11 11:41:17,229 Loading model distilgpt2-124m
2024-09-11 11:41:17,726 Processed batch: ['126ad0d5', '831274da'] with model distilgpt2-124m in 0.4437 seconds
2024-09-11 11:41:17,726 Latency for request 126ad0d5 with model distilgpt2-124m: 3.9274 seconds
2024-09-11 11:41:17,727 Latency for request 831274da with model distilgpt2-124m: 0.9224 seconds
2024-09-11 11:41:17,827 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:17,827 Updated batch size:2
2024-09-11 11:41:17,827 Loading model gpt2-124m
2024-09-11 11:41:18,512 Processed batch: ['69197691', '7e88f27e'] with model gpt2-124m in 0.6231 seconds
2024-09-11 11:41:18,512 Latency for request 69197691 with model gpt2-124m: 3.6411 seconds
2024-09-11 11:41:18,513 Latency for request 7e88f27e with model gpt2-124m: 2.4421 seconds
2024-09-11 11:41:18,513 Total time: 10.7932 seconds
2024-09-11 11:41:18,513 Total inference time: 9.1982 seconds
2024-09-11 11:41:18,513 Inference time as percentage of total time: 85.22%
2024-09-11 11:41:18,616 Total time: 10.8959 seconds
2024-09-11 11:41:18,616 Total inference time: 9.1982 seconds
2024-09-11 11:41:18,616 Inference time as percentage of total time: 84.42%
2024-09-11 11:41:18,721 Total time: 11.0010 seconds
2024-09-11 11:41:18,721 Total inference time: 9.1982 seconds
2024-09-11 11:41:18,721 Inference time as percentage of total time: 83.61%
2024-09-11 11:41:18,826 Total time: 11.1062 seconds
2024-09-11 11:41:18,826 Total inference time: 9.1982 seconds
2024-09-11 11:41:18,826 Inference time as percentage of total time: 82.82%
2024-09-11 11:41:18,931 Total time: 11.2113 seconds
2024-09-11 11:41:18,932 Total inference time: 9.1982 seconds
2024-09-11 11:41:18,932 Inference time as percentage of total time: 82.04%
2024-09-11 11:41:19,037 Total time: 11.3171 seconds
2024-09-11 11:41:19,037 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,037 Inference time as percentage of total time: 81.28%
2024-09-11 11:41:19,143 Total time: 11.4229 seconds
2024-09-11 11:41:19,143 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,143 Inference time as percentage of total time: 80.52%
2024-09-11 11:41:19,248 Total time: 11.5284 seconds
2024-09-11 11:41:19,248 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,249 Inference time as percentage of total time: 79.79%
2024-09-11 11:41:19,354 Total time: 11.6340 seconds
2024-09-11 11:41:19,354 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,354 Inference time as percentage of total time: 79.06%
2024-09-11 11:41:19,460 Total time: 11.7397 seconds
2024-09-11 11:41:19,460 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,460 Inference time as percentage of total time: 78.35%
2024-09-11 11:41:19,561 Total time: 11.8410 seconds
2024-09-11 11:41:19,561 Total inference time: 9.1982 seconds
2024-09-11 11:41:19,561 Inference time as percentage of total time: 77.68%
2024-09-11 11:41:19,621 Request with ID 420e3355 for model gpt2-124m received
2024-09-11 11:41:19,621 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:19,622 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-11 11:41:19,622 127.0.0.1 - - [11/Sep/2024 11:41:19] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:19,667 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:19,667 Updated batch size:1
2024-09-11 11:41:19,667 Loading model gpt2-124m
2024-09-11 11:41:20,048 Processed batch: ['420e3355'] with model gpt2-124m in 0.3806 seconds
2024-09-11 11:41:20,048 Latency for request 420e3355 with model gpt2-124m: 0.4268 seconds
2024-09-11 11:41:20,049 Total time: 12.3289 seconds
2024-09-11 11:41:20,049 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,049 Inference time as percentage of total time: 77.69%
2024-09-11 11:41:20,152 Total time: 12.4319 seconds
2024-09-11 11:41:20,152 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,152 Inference time as percentage of total time: 77.05%
2024-09-11 11:41:20,257 Total time: 12.5370 seconds
2024-09-11 11:41:20,257 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,257 Inference time as percentage of total time: 76.40%
2024-09-11 11:41:20,362 Total time: 12.6421 seconds
2024-09-11 11:41:20,362 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,362 Inference time as percentage of total time: 75.77%
2024-09-11 11:41:20,467 Total time: 12.7473 seconds
2024-09-11 11:41:20,467 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,468 Inference time as percentage of total time: 75.14%
2024-09-11 11:41:20,573 Total time: 12.8530 seconds
2024-09-11 11:41:20,573 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,573 Inference time as percentage of total time: 74.53%
2024-09-11 11:41:20,674 Total time: 12.9545 seconds
2024-09-11 11:41:20,674 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,674 Inference time as percentage of total time: 73.94%
2024-09-11 11:41:20,780 Total time: 13.0597 seconds
2024-09-11 11:41:20,780 Total inference time: 9.5787 seconds
2024-09-11 11:41:20,780 Inference time as percentage of total time: 73.35%
2024-09-11 11:41:20,865 Request with ID 3f8fac87 for model gpt2medium-355m received
2024-09-11 11:41:20,865 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:20,865 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-11 11:41:20,865 127.0.0.1 - - [11/Sep/2024 11:41:20] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:20,885 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:20,885 Updated batch size:1
2024-09-11 11:41:20,885 Loading model gpt2medium-355m
2024-09-11 11:41:21,804 Request with ID 137683df for model gpt2medium-355m received
2024-09-11 11:41:21,804 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:21,804 127.0.0.1 - - [11/Sep/2024 11:41:21] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:22,279 Processed batch: ['3f8fac87'] with model gpt2medium-355m in 1.2418 seconds
2024-09-11 11:41:22,279 Latency for request 3f8fac87 with model gpt2medium-355m: 1.4134 seconds
2024-09-11 11:41:22,383 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:22,383 Updated batch size:1
2024-09-11 11:41:22,383 Loading model gpt2medium-355m
2024-09-11 11:41:22,438 Request with ID d26626ac for model gpt2medium-355m received
2024-09-11 11:41:22,438 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:22,438 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-11 11:41:22,438 127.0.0.1 - - [11/Sep/2024 11:41:22] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:23,520 Processed batch: ['137683df'] with model gpt2medium-355m in 1.1369 seconds
2024-09-11 11:41:23,520 Latency for request 137683df with model gpt2medium-355m: 1.7156 seconds
2024-09-11 11:41:23,595 Request with ID 75d34e6c for model gpt2medium-355m received
2024-09-11 11:41:23,595 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:23,595 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-11 11:41:23,595 127.0.0.1 - - [11/Sep/2024 11:41:23] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:23,626 Remaining requests condition met for model gpt2medium-355m
2024-09-11 11:41:23,626 Updated batch size:2
2024-09-11 11:41:23,626 Loading model gpt2medium-355m
2024-09-11 11:41:24,303 Request with ID aec71bfb for model gpt2-124m received
2024-09-11 11:41:24,303 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:24,303 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-11 11:41:24,304 127.0.0.1 - - [11/Sep/2024 11:41:24] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:24,709 Request with ID e92cf0ac for model distilgpt2-124m received
2024-09-11 11:41:24,710 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:24,710 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-11 11:41:24,710 127.0.0.1 - - [11/Sep/2024 11:41:24] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:25,770 Request with ID eea62178 for model distilgpt2-124m received
2024-09-11 11:41:25,770 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-11 11:41:25,770 127.0.0.1 - - [11/Sep/2024 11:41:25] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:25,880 Processed batch: ['d26626ac', '75d34e6c'] with model gpt2medium-355m in 2.2537 seconds
2024-09-11 11:41:25,880 Latency for request d26626ac with model gpt2medium-355m: 3.4420 seconds
2024-09-11 11:41:25,880 Latency for request 75d34e6c with model gpt2medium-355m: 2.2852 seconds
2024-09-11 11:41:25,881 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:41:25,881 Updated batch size:2
2024-09-11 11:41:25,881 Loading model distilgpt2-124m
2024-09-11 11:41:26,494 Processed batch: ['e92cf0ac', 'eea62178'] with model distilgpt2-124m in 0.5503 seconds
2024-09-11 11:41:26,494 Latency for request e92cf0ac with model distilgpt2-124m: 1.7842 seconds
2024-09-11 11:41:26,495 Latency for request eea62178 with model distilgpt2-124m: 0.7234 seconds
2024-09-11 11:41:26,599 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:26,600 Updated batch size:1
2024-09-11 11:41:26,600 Loading model gpt2-124m
2024-09-11 11:41:26,673 Request with ID 72ef5ec8 for model distilgpt2-124m received
2024-09-11 11:41:26,673 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:41:26,673 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-11 11:41:26,673 127.0.0.1 - - [11/Sep/2024 11:41:26] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:26,800 Request with ID e76f1cc8 for model gpt2-124m received
2024-09-11 11:41:26,800 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:41:26,801 127.0.0.1 - - [11/Sep/2024 11:41:26] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:41:27,158 Processed batch: ['aec71bfb'] with model gpt2-124m in 0.4956 seconds
2024-09-11 11:41:27,158 Latency for request aec71bfb with model gpt2-124m: 2.8549 seconds
2024-09-11 11:41:27,159 Remaining requests condition met for model distilgpt2-124m
2024-09-11 11:41:27,159 Updated batch size:1
2024-09-11 11:41:27,159 Loading model distilgpt2-124m
2024-09-11 11:41:27,519 Processed batch: ['72ef5ec8'] with model distilgpt2-124m in 0.3043 seconds
2024-09-11 11:41:27,519 Latency for request 72ef5ec8 with model distilgpt2-124m: 0.8465 seconds
2024-09-11 11:41:27,626 Remaining requests condition met for model gpt2-124m
2024-09-11 11:41:27,626 Updated batch size:1
2024-09-11 11:41:27,626 Loading model gpt2-124m
2024-09-11 11:41:28,106 Processed batch: ['e76f1cc8'] with model gpt2-124m in 0.4136 seconds
2024-09-11 11:41:28,106 Latency for request e76f1cc8 with model gpt2-124m: 1.3059 seconds
2024-09-11 11:41:28,107 Total time: 20.3874 seconds
2024-09-11 11:41:28,107 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,107 Inference time as percentage of total time: 78.36%
2024-09-11 11:41:28,212 Total time: 20.4924 seconds
2024-09-11 11:41:28,212 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,212 Inference time as percentage of total time: 77.95%
2024-09-11 11:41:28,317 Total time: 20.5976 seconds
2024-09-11 11:41:28,317 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,317 Inference time as percentage of total time: 77.56%
2024-09-11 11:41:28,422 Total time: 20.7027 seconds
2024-09-11 11:41:28,422 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,422 Inference time as percentage of total time: 77.16%
2024-09-11 11:41:28,528 Total time: 20.8079 seconds
2024-09-11 11:41:28,528 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,528 Inference time as percentage of total time: 76.77%
2024-09-11 11:41:28,634 Total time: 20.9139 seconds
2024-09-11 11:41:28,634 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,634 Inference time as percentage of total time: 76.38%
2024-09-11 11:41:28,739 Total time: 21.0196 seconds
2024-09-11 11:41:28,740 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,740 Inference time as percentage of total time: 76.00%
2024-09-11 11:41:28,842 Total time: 21.1219 seconds
2024-09-11 11:41:28,842 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,842 Inference time as percentage of total time: 75.63%
2024-09-11 11:41:28,947 Total time: 21.2274 seconds
2024-09-11 11:41:28,947 Total inference time: 15.9748 seconds
2024-09-11 11:41:28,947 Inference time as percentage of total time: 75.26%
2024-09-11 11:41:29,051 Total time: 21.3309 seconds
2024-09-11 11:41:29,051 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,051 Inference time as percentage of total time: 74.89%
2024-09-11 11:41:29,154 Total time: 21.4339 seconds
2024-09-11 11:41:29,154 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,154 Inference time as percentage of total time: 74.53%
2024-09-11 11:41:29,260 Total time: 21.5397 seconds
2024-09-11 11:41:29,260 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,260 Inference time as percentage of total time: 74.16%
2024-09-11 11:41:29,365 Total time: 21.6451 seconds
2024-09-11 11:41:29,365 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,365 Inference time as percentage of total time: 73.80%
2024-09-11 11:41:29,470 Total time: 21.7505 seconds
2024-09-11 11:41:29,470 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,470 Inference time as percentage of total time: 73.45%
2024-09-11 11:41:29,576 Total time: 21.8558 seconds
2024-09-11 11:41:29,576 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,576 Inference time as percentage of total time: 73.09%
2024-09-11 11:41:29,681 Total time: 21.9617 seconds
2024-09-11 11:41:29,682 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,682 Inference time as percentage of total time: 72.74%
2024-09-11 11:41:29,787 Total time: 22.0672 seconds
2024-09-11 11:41:29,787 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,787 Inference time as percentage of total time: 72.39%
2024-09-11 11:41:29,892 Total time: 22.1727 seconds
2024-09-11 11:41:29,893 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,893 Inference time as percentage of total time: 72.05%
2024-09-11 11:41:29,998 Total time: 22.2784 seconds
2024-09-11 11:41:29,999 Total inference time: 15.9748 seconds
2024-09-11 11:41:29,999 Inference time as percentage of total time: 71.71%
2024-09-11 11:41:30,102 Total time: 22.3818 seconds
2024-09-11 11:41:30,102 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,102 Inference time as percentage of total time: 71.37%
2024-09-11 11:41:30,207 Total time: 22.4877 seconds
2024-09-11 11:41:30,208 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,208 Inference time as percentage of total time: 71.04%
2024-09-11 11:41:30,314 Total time: 22.5940 seconds
2024-09-11 11:41:30,314 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,314 Inference time as percentage of total time: 70.70%
2024-09-11 11:41:30,415 Total time: 22.6951 seconds
2024-09-11 11:41:30,416 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,416 Inference time as percentage of total time: 70.39%
2024-09-11 11:41:30,518 Total time: 22.7981 seconds
2024-09-11 11:41:30,518 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,518 Inference time as percentage of total time: 70.07%
2024-09-11 11:41:30,622 Total time: 22.9022 seconds
2024-09-11 11:41:30,622 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,623 Inference time as percentage of total time: 69.75%
2024-09-11 11:41:30,728 Total time: 23.0082 seconds
2024-09-11 11:41:30,728 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,728 Inference time as percentage of total time: 69.43%
2024-09-11 11:41:30,834 Total time: 23.1140 seconds
2024-09-11 11:41:30,834 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,834 Inference time as percentage of total time: 69.11%
2024-09-11 11:41:30,935 Total time: 23.2152 seconds
2024-09-11 11:41:30,935 Total inference time: 15.9748 seconds
2024-09-11 11:41:30,935 Inference time as percentage of total time: 68.81%
2024-09-11 11:41:31,040 Total time: 23.3204 seconds
2024-09-11 11:41:31,040 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,040 Inference time as percentage of total time: 68.50%
2024-09-11 11:41:31,145 Total time: 23.4257 seconds
2024-09-11 11:41:31,146 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,146 Inference time as percentage of total time: 68.19%
2024-09-11 11:41:31,251 Total time: 23.5310 seconds
2024-09-11 11:41:31,251 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,251 Inference time as percentage of total time: 67.89%
2024-09-11 11:41:31,353 Total time: 23.6338 seconds
2024-09-11 11:41:31,353 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,354 Inference time as percentage of total time: 67.59%
2024-09-11 11:41:31,457 Total time: 23.7370 seconds
2024-09-11 11:41:31,457 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,457 Inference time as percentage of total time: 67.30%
2024-09-11 11:41:31,563 Total time: 23.8430 seconds
2024-09-11 11:41:31,563 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,563 Inference time as percentage of total time: 67.00%
2024-09-11 11:41:31,665 Total time: 23.9450 seconds
2024-09-11 11:41:31,665 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,665 Inference time as percentage of total time: 66.71%
2024-09-11 11:41:31,771 Total time: 24.0510 seconds
2024-09-11 11:41:31,771 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,771 Inference time as percentage of total time: 66.42%
2024-09-11 11:41:31,874 Total time: 24.1538 seconds
2024-09-11 11:41:31,874 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,874 Inference time as percentage of total time: 66.14%
2024-09-11 11:41:31,979 Total time: 24.2597 seconds
2024-09-11 11:41:31,980 Total inference time: 15.9748 seconds
2024-09-11 11:41:31,980 Inference time as percentage of total time: 65.85%
2024-09-11 11:41:32,081 Total time: 24.3608 seconds
2024-09-11 11:41:32,081 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,081 Inference time as percentage of total time: 65.58%
2024-09-11 11:41:32,185 Total time: 24.4654 seconds
2024-09-11 11:41:32,186 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,186 Inference time as percentage of total time: 65.30%
2024-09-11 11:41:32,291 Total time: 24.5716 seconds
2024-09-11 11:41:32,292 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,292 Inference time as percentage of total time: 65.01%
2024-09-11 11:41:32,392 Total time: 24.6725 seconds
2024-09-11 11:41:32,393 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,393 Inference time as percentage of total time: 64.75%
2024-09-11 11:41:32,498 Total time: 24.7784 seconds
2024-09-11 11:41:32,499 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,499 Inference time as percentage of total time: 64.47%
2024-09-11 11:41:32,604 Total time: 24.8843 seconds
2024-09-11 11:41:32,604 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,605 Inference time as percentage of total time: 64.20%
2024-09-11 11:41:32,706 Total time: 24.9862 seconds
2024-09-11 11:41:32,706 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,706 Inference time as percentage of total time: 63.93%
2024-09-11 11:41:32,812 Total time: 25.0920 seconds
2024-09-11 11:41:32,812 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,812 Inference time as percentage of total time: 63.66%
2024-09-11 11:41:32,914 Total time: 25.1941 seconds
2024-09-11 11:41:32,914 Total inference time: 15.9748 seconds
2024-09-11 11:41:32,914 Inference time as percentage of total time: 63.41%
2024-09-11 11:41:33,015 Total time: 25.2956 seconds
2024-09-11 11:41:33,016 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,016 Inference time as percentage of total time: 63.15%
2024-09-11 11:41:33,121 Total time: 25.4014 seconds
2024-09-11 11:41:33,121 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,122 Inference time as percentage of total time: 62.89%
2024-09-11 11:41:33,227 Total time: 25.5073 seconds
2024-09-11 11:41:33,227 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,228 Inference time as percentage of total time: 62.63%
2024-09-11 11:41:33,332 Total time: 25.6124 seconds
2024-09-11 11:41:33,333 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,333 Inference time as percentage of total time: 62.37%
2024-09-11 11:41:33,438 Total time: 25.7185 seconds
2024-09-11 11:41:33,439 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,439 Inference time as percentage of total time: 62.11%
2024-09-11 11:41:33,539 Total time: 25.8195 seconds
2024-09-11 11:41:33,540 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,540 Inference time as percentage of total time: 61.87%
2024-09-11 11:41:33,641 Total time: 25.9210 seconds
2024-09-11 11:41:33,641 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,641 Inference time as percentage of total time: 61.63%
2024-09-11 11:41:33,747 Total time: 26.0270 seconds
2024-09-11 11:41:33,747 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,747 Inference time as percentage of total time: 61.38%
2024-09-11 11:41:33,848 Total time: 26.1284 seconds
2024-09-11 11:41:33,848 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,849 Inference time as percentage of total time: 61.14%
2024-09-11 11:41:33,953 Total time: 26.2335 seconds
2024-09-11 11:41:33,953 Total inference time: 15.9748 seconds
2024-09-11 11:41:33,953 Inference time as percentage of total time: 60.89%
2024-09-11 11:41:34,056 Total time: 26.3362 seconds
2024-09-11 11:41:34,056 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,056 Inference time as percentage of total time: 60.66%
2024-09-11 11:41:34,162 Total time: 26.4419 seconds
2024-09-11 11:41:34,162 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,162 Inference time as percentage of total time: 60.41%
2024-09-11 11:41:34,267 Total time: 26.5477 seconds
2024-09-11 11:41:34,268 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,268 Inference time as percentage of total time: 60.17%
2024-09-11 11:41:34,373 Total time: 26.6537 seconds
2024-09-11 11:41:34,374 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,374 Inference time as percentage of total time: 59.93%
2024-09-11 11:41:34,476 Total time: 26.7566 seconds
2024-09-11 11:41:34,477 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,477 Inference time as percentage of total time: 59.70%
2024-09-11 11:41:34,582 Total time: 26.8626 seconds
2024-09-11 11:41:34,583 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,583 Inference time as percentage of total time: 59.47%
2024-09-11 11:41:34,686 Total time: 26.9661 seconds
2024-09-11 11:41:34,686 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,686 Inference time as percentage of total time: 59.24%
2024-09-11 11:41:34,791 Total time: 27.0710 seconds
2024-09-11 11:41:34,791 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,792 Inference time as percentage of total time: 59.01%
2024-09-11 11:41:34,897 Total time: 27.1773 seconds
2024-09-11 11:41:34,897 Total inference time: 15.9748 seconds
2024-09-11 11:41:34,898 Inference time as percentage of total time: 58.78%
2024-09-11 11:41:35,000 Total time: 27.2801 seconds
2024-09-11 11:41:35,000 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,000 Inference time as percentage of total time: 58.56%
2024-09-11 11:41:35,106 Total time: 27.3860 seconds
2024-09-11 11:41:35,106 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,106 Inference time as percentage of total time: 58.33%
2024-09-11 11:41:35,212 Total time: 27.4920 seconds
2024-09-11 11:41:35,212 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,213 Inference time as percentage of total time: 58.11%
2024-09-11 11:41:35,316 Total time: 27.5964 seconds
2024-09-11 11:41:35,317 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,317 Inference time as percentage of total time: 57.89%
2024-09-11 11:41:35,418 Total time: 27.6980 seconds
2024-09-11 11:41:35,418 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,418 Inference time as percentage of total time: 57.68%
2024-09-11 11:41:35,524 Total time: 27.8038 seconds
2024-09-11 11:41:35,524 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,524 Inference time as percentage of total time: 57.46%
2024-09-11 11:41:35,626 Total time: 27.9065 seconds
2024-09-11 11:41:35,627 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,627 Inference time as percentage of total time: 57.24%
2024-09-11 11:41:35,730 Total time: 28.0108 seconds
2024-09-11 11:41:35,731 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,731 Inference time as percentage of total time: 57.03%
2024-09-11 11:41:35,834 Total time: 28.1148 seconds
2024-09-11 11:41:35,835 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,835 Inference time as percentage of total time: 56.82%
2024-09-11 11:41:35,936 Total time: 28.2161 seconds
2024-09-11 11:41:35,936 Total inference time: 15.9748 seconds
2024-09-11 11:41:35,936 Inference time as percentage of total time: 56.62%
2024-09-11 11:41:36,042 Total time: 28.3220 seconds
2024-09-11 11:41:36,042 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,042 Inference time as percentage of total time: 56.40%
2024-09-11 11:41:36,146 Total time: 28.4267 seconds
2024-09-11 11:41:36,147 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,147 Inference time as percentage of total time: 56.20%
2024-09-11 11:41:36,248 Total time: 28.5284 seconds
2024-09-11 11:41:36,248 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,249 Inference time as percentage of total time: 56.00%
2024-09-11 11:41:36,354 Total time: 28.6343 seconds
2024-09-11 11:41:36,354 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,355 Inference time as percentage of total time: 55.79%
2024-09-11 11:41:36,460 Total time: 28.7402 seconds
2024-09-11 11:41:36,460 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,460 Inference time as percentage of total time: 55.58%
2024-09-11 11:41:36,566 Total time: 28.8461 seconds
2024-09-11 11:41:36,566 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,566 Inference time as percentage of total time: 55.38%
2024-09-11 11:41:36,672 Total time: 28.9520 seconds
2024-09-11 11:41:36,672 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,672 Inference time as percentage of total time: 55.18%
2024-09-11 11:41:36,776 Total time: 29.0562 seconds
2024-09-11 11:41:36,776 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,776 Inference time as percentage of total time: 54.98%
2024-09-11 11:41:36,880 Total time: 29.1608 seconds
2024-09-11 11:41:36,881 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,881 Inference time as percentage of total time: 54.78%
2024-09-11 11:41:36,982 Total time: 29.2621 seconds
2024-09-11 11:41:36,982 Total inference time: 15.9748 seconds
2024-09-11 11:41:36,982 Inference time as percentage of total time: 54.59%
2024-09-11 11:41:37,085 Total time: 29.3658 seconds
2024-09-11 11:41:37,086 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,086 Inference time as percentage of total time: 54.40%
2024-09-11 11:41:37,187 Total time: 29.4672 seconds
2024-09-11 11:41:37,187 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,187 Inference time as percentage of total time: 54.21%
2024-09-11 11:41:37,289 Total time: 29.5695 seconds
2024-09-11 11:41:37,290 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,290 Inference time as percentage of total time: 54.02%
2024-09-11 11:41:37,394 Total time: 29.6744 seconds
2024-09-11 11:41:37,394 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,395 Inference time as percentage of total time: 53.83%
2024-09-11 11:41:37,499 Total time: 29.7790 seconds
2024-09-11 11:41:37,499 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,499 Inference time as percentage of total time: 53.64%
2024-09-11 11:41:37,601 Total time: 29.8814 seconds
2024-09-11 11:41:37,602 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,602 Inference time as percentage of total time: 53.46%
2024-09-11 11:41:37,707 Total time: 29.9874 seconds
2024-09-11 11:41:37,708 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,708 Inference time as percentage of total time: 53.27%
2024-09-11 11:41:37,813 Total time: 30.0935 seconds
2024-09-11 11:41:37,814 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,814 Inference time as percentage of total time: 53.08%
2024-09-11 11:41:37,915 Total time: 30.1950 seconds
2024-09-11 11:41:37,915 Total inference time: 15.9748 seconds
2024-09-11 11:41:37,915 Inference time as percentage of total time: 52.91%
2024-09-11 11:41:38,017 Total time: 30.2976 seconds
2024-09-11 11:41:38,018 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,018 Inference time as percentage of total time: 52.73%
2024-09-11 11:41:38,123 Total time: 30.4036 seconds
2024-09-11 11:41:38,124 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,124 Inference time as percentage of total time: 52.54%
2024-09-11 11:41:38,225 Total time: 30.5052 seconds
2024-09-11 11:41:38,226 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,226 Inference time as percentage of total time: 52.37%
2024-09-11 11:41:38,330 Total time: 30.6108 seconds
2024-09-11 11:41:38,331 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,331 Inference time as percentage of total time: 52.19%
2024-09-11 11:41:38,436 Total time: 30.7161 seconds
2024-09-11 11:41:38,436 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,436 Inference time as percentage of total time: 52.01%
2024-09-11 11:41:38,540 Total time: 30.8200 seconds
2024-09-11 11:41:38,541 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,541 Inference time as percentage of total time: 51.83%
2024-09-11 11:41:38,646 Total time: 30.9267 seconds
2024-09-11 11:41:38,647 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,647 Inference time as percentage of total time: 51.65%
2024-09-11 11:41:38,751 Total time: 31.0311 seconds
2024-09-11 11:41:38,751 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,751 Inference time as percentage of total time: 51.48%
2024-09-11 11:41:38,856 Total time: 31.1367 seconds
2024-09-11 11:41:38,857 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,857 Inference time as percentage of total time: 51.31%
2024-09-11 11:41:38,962 Total time: 31.2428 seconds
2024-09-11 11:41:38,963 Total inference time: 15.9748 seconds
2024-09-11 11:41:38,963 Inference time as percentage of total time: 51.13%
2024-09-11 11:41:39,064 Total time: 31.3442 seconds
2024-09-11 11:41:39,064 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,064 Inference time as percentage of total time: 50.97%
2024-09-11 11:41:39,170 Total time: 31.4502 seconds
2024-09-11 11:41:39,170 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,170 Inference time as percentage of total time: 50.79%
2024-09-11 11:41:39,276 Total time: 31.5561 seconds
2024-09-11 11:41:39,276 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,277 Inference time as percentage of total time: 50.62%
2024-09-11 11:41:39,381 Total time: 31.6617 seconds
2024-09-11 11:41:39,382 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,382 Inference time as percentage of total time: 50.45%
2024-09-11 11:41:39,486 Total time: 31.7665 seconds
2024-09-11 11:41:39,487 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,487 Inference time as percentage of total time: 50.29%
2024-09-11 11:41:39,591 Total time: 31.8714 seconds
2024-09-11 11:41:39,591 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,592 Inference time as percentage of total time: 50.12%
2024-09-11 11:41:39,697 Total time: 31.9776 seconds
2024-09-11 11:41:39,698 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,698 Inference time as percentage of total time: 49.96%
2024-09-11 11:41:39,802 Total time: 32.0820 seconds
2024-09-11 11:41:39,802 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,802 Inference time as percentage of total time: 49.79%
2024-09-11 11:41:39,905 Total time: 32.1850 seconds
2024-09-11 11:41:39,905 Total inference time: 15.9748 seconds
2024-09-11 11:41:39,905 Inference time as percentage of total time: 49.63%
2024-09-11 11:41:40,011 Total time: 32.2910 seconds
2024-09-11 11:41:40,011 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,011 Inference time as percentage of total time: 49.47%
2024-09-11 11:41:40,112 Total time: 32.3927 seconds
2024-09-11 11:41:40,113 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,113 Inference time as percentage of total time: 49.32%
2024-09-11 11:41:40,215 Total time: 32.4952 seconds
2024-09-11 11:41:40,215 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,215 Inference time as percentage of total time: 49.16%
2024-09-11 11:41:40,319 Total time: 32.5992 seconds
2024-09-11 11:41:40,319 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,319 Inference time as percentage of total time: 49.00%
2024-09-11 11:41:40,423 Total time: 32.7029 seconds
2024-09-11 11:41:40,423 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,423 Inference time as percentage of total time: 48.85%
2024-09-11 11:41:40,525 Total time: 32.8049 seconds
2024-09-11 11:41:40,525 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,525 Inference time as percentage of total time: 48.70%
2024-09-11 11:41:40,630 Total time: 32.9109 seconds
2024-09-11 11:41:40,631 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,631 Inference time as percentage of total time: 48.54%
2024-09-11 11:41:40,736 Total time: 33.0166 seconds
2024-09-11 11:41:40,737 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,737 Inference time as percentage of total time: 48.38%
2024-09-11 11:41:40,841 Total time: 33.1210 seconds
2024-09-11 11:41:40,841 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,841 Inference time as percentage of total time: 48.23%
2024-09-11 11:41:40,947 Total time: 33.2272 seconds
2024-09-11 11:41:40,947 Total inference time: 15.9748 seconds
2024-09-11 11:41:40,947 Inference time as percentage of total time: 48.08%
2024-09-11 11:41:41,049 Total time: 33.3298 seconds
2024-09-11 11:41:41,050 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,050 Inference time as percentage of total time: 47.93%
2024-09-11 11:41:41,155 Total time: 33.4357 seconds
2024-09-11 11:41:41,156 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,156 Inference time as percentage of total time: 47.78%
2024-09-11 11:41:41,257 Total time: 33.5375 seconds
2024-09-11 11:41:41,258 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,258 Inference time as percentage of total time: 47.63%
2024-09-11 11:41:41,363 Total time: 33.6434 seconds
2024-09-11 11:41:41,363 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,364 Inference time as percentage of total time: 47.48%
2024-09-11 11:41:41,467 Total time: 33.7473 seconds
2024-09-11 11:41:41,467 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,467 Inference time as percentage of total time: 47.34%
2024-09-11 11:41:41,568 Total time: 33.8486 seconds
2024-09-11 11:41:41,568 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,569 Inference time as percentage of total time: 47.20%
2024-09-11 11:41:41,672 Total time: 33.9528 seconds
2024-09-11 11:41:41,673 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,673 Inference time as percentage of total time: 47.05%
2024-09-11 11:41:41,775 Total time: 34.0550 seconds
2024-09-11 11:41:41,775 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,775 Inference time as percentage of total time: 46.91%
2024-09-11 11:41:41,876 Total time: 34.1560 seconds
2024-09-11 11:41:41,876 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,876 Inference time as percentage of total time: 46.77%
2024-09-11 11:41:41,980 Total time: 34.2607 seconds
2024-09-11 11:41:41,981 Total inference time: 15.9748 seconds
2024-09-11 11:41:41,981 Inference time as percentage of total time: 46.63%
2024-09-11 11:41:42,086 Total time: 34.3665 seconds
2024-09-11 11:41:42,086 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,087 Inference time as percentage of total time: 46.48%
2024-09-11 11:41:42,192 Total time: 34.4722 seconds
2024-09-11 11:41:42,192 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,192 Inference time as percentage of total time: 46.34%
2024-09-11 11:41:42,298 Total time: 34.5780 seconds
2024-09-11 11:41:42,298 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,299 Inference time as percentage of total time: 46.20%
2024-09-11 11:41:42,404 Total time: 34.6842 seconds
2024-09-11 11:41:42,404 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,404 Inference time as percentage of total time: 46.06%
2024-09-11 11:41:42,507 Total time: 34.7871 seconds
2024-09-11 11:41:42,507 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,507 Inference time as percentage of total time: 45.92%
2024-09-11 11:41:42,612 Total time: 34.8927 seconds
2024-09-11 11:41:42,613 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,613 Inference time as percentage of total time: 45.78%
2024-09-11 11:41:42,714 Total time: 34.9941 seconds
2024-09-11 11:41:42,714 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,714 Inference time as percentage of total time: 45.65%
2024-09-11 11:41:42,815 Total time: 35.0950 seconds
2024-09-11 11:41:42,815 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,815 Inference time as percentage of total time: 45.52%
2024-09-11 11:41:42,920 Total time: 35.2004 seconds
2024-09-11 11:41:42,920 Total inference time: 15.9748 seconds
2024-09-11 11:41:42,920 Inference time as percentage of total time: 45.38%
2024-09-11 11:41:43,026 Total time: 35.3061 seconds
2024-09-11 11:41:43,026 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,026 Inference time as percentage of total time: 45.25%
2024-09-11 11:41:43,132 Total time: 35.4121 seconds
2024-09-11 11:41:43,132 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,132 Inference time as percentage of total time: 45.11%
2024-09-11 11:41:43,235 Total time: 35.5158 seconds
2024-09-11 11:41:43,236 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,236 Inference time as percentage of total time: 44.98%
2024-09-11 11:41:43,341 Total time: 35.6217 seconds
2024-09-11 11:41:43,342 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,342 Inference time as percentage of total time: 44.85%
2024-09-11 11:41:43,447 Total time: 35.7272 seconds
2024-09-11 11:41:43,447 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,447 Inference time as percentage of total time: 44.71%
2024-09-11 11:41:43,548 Total time: 35.8284 seconds
2024-09-11 11:41:43,548 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,549 Inference time as percentage of total time: 44.59%
2024-09-11 11:41:43,654 Total time: 35.9345 seconds
2024-09-11 11:41:43,655 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,655 Inference time as percentage of total time: 44.46%
2024-09-11 11:41:43,756 Total time: 36.0362 seconds
2024-09-11 11:41:43,756 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,756 Inference time as percentage of total time: 44.33%
2024-09-11 11:41:43,862 Total time: 36.1422 seconds
2024-09-11 11:41:43,862 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,863 Inference time as percentage of total time: 44.20%
2024-09-11 11:41:43,965 Total time: 36.2457 seconds
2024-09-11 11:41:43,966 Total inference time: 15.9748 seconds
2024-09-11 11:41:43,966 Inference time as percentage of total time: 44.07%
