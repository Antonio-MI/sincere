2024-09-09 12:00:31,917 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-09 12:00:31,917 [33mPress CTRL+C to quit[0m
2024-09-09 12:00:35,058 127.0.0.1 - - [09/Sep/2024 12:00:35] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:37,535 127.0.0.1 - - [09/Sep/2024 12:00:37] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:38,529 127.0.0.1 - - [09/Sep/2024 12:00:38] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:38,716 127.0.0.1 - - [09/Sep/2024 12:00:38] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:39,822 Batch size condition met for model gpt2-124m
2024-09-09 12:00:39,822 4
2024-09-09 12:00:39,822 Loading model gpt2-124m
2024-09-09 12:00:39,906 Batch processing started at 356418.9211 for model gpt2-124m
2024-09-09 12:00:39,995 Time limit reached for model gpt2-124m at 1725897639.9958
2024-09-09 12:00:40,101 Time limit reached for model gpt2-124m at 1725897640.1010
2024-09-09 12:00:40,206 Time limit reached for model gpt2-124m at 1725897640.2062
2024-09-09 12:00:40,308 Time limit reached for model gpt2-124m at 1725897640.3084
2024-09-09 12:00:40,413 Time limit reached for model gpt2-124m at 1725897640.4136
2024-09-09 12:00:40,518 Time limit reached for model gpt2-124m at 1725897640.5188
2024-09-09 12:00:40,595 Processed request ID fc7614e1 with model gpt2-124m
2024-09-09 12:00:40,595 Processed request ID c9437b84 with model gpt2-124m
2024-09-09 12:00:40,596 Processed request ID 5ad8a99d with model gpt2-124m
2024-09-09 12:00:40,596 Processed request ID f3455175 with model gpt2-124m
2024-09-09 12:00:40,596 Processed batch: ['fc7614e1', 'c9437b84', '5ad8a99d', 'f3455175'] with model gpt2-124m in 0.6898 seconds
2024-09-09 12:00:40,596 Latency for request fc7614e1 with model gpt2-124m: 5.5383 seconds
2024-09-09 12:00:40,598 Latency for request c9437b84 with model gpt2-124m: 3.0614 seconds
2024-09-09 12:00:40,598 Latency for request 5ad8a99d with model gpt2-124m: 1.8806 seconds
2024-09-09 12:00:40,598 Latency for request f3455175 with model gpt2-124m: 0.7744 seconds
2024-09-09 12:00:40,599 127.0.0.1 - - [09/Sep/2024 12:00:40] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:41,809 127.0.0.1 - - [09/Sep/2024 12:00:41] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:43,520 Time limit reached for model gpt2medium-355m at 1725897643.5205
2024-09-09 12:00:43,521 Moving batch for gpt2medium-355m from incoming to running due to time limit with batch size 1
2024-09-09 12:00:43,521 Time limit condition met for model gpt2medium-355m
2024-09-09 12:00:43,521 1
2024-09-09 12:00:43,521 Padding requests generated in 0.0001 seconds
2024-09-09 12:00:43,521 Loading model gpt2medium-355m
2024-09-09 12:00:43,635 127.0.0.1 - - [09/Sep/2024 12:00:43] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:43,663 Batch processing started at 356422.6784 for model gpt2medium-355m
2024-09-09 12:00:45,331 127.0.0.1 - - [09/Sep/2024 12:00:45] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:46,505 127.0.0.1 - - [09/Sep/2024 12:00:46] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:47,188 Processed request ID 62c390ae with model gpt2medium-355m
2024-09-09 12:00:47,188 Processed request ID c932 with model gpt2medium-355m
2024-09-09 12:00:47,189 Processed request ID 4e7a with model gpt2medium-355m
2024-09-09 12:00:47,189 Processed request ID 689c with model gpt2medium-355m
2024-09-09 12:00:47,189 Processed batch: ['62c390ae', 'c932', '4e7a', '689c'] with model gpt2medium-355m in 3.5254 seconds
2024-09-09 12:00:47,189 Latency for request 62c390ae with model gpt2medium-355m: 8.6605 seconds
2024-09-09 12:00:47,189 Latency for request c932 with model gpt2medium-355m: 3.6677 seconds
2024-09-09 12:00:47,190 Latency for request 4e7a with model gpt2medium-355m: 3.6677 seconds
2024-09-09 12:00:47,190 Latency for request 689c with model gpt2medium-355m: 3.6676 seconds
2024-09-09 12:00:47,293 Time limit reached for model distilgpt2-124m at 1725897647.2931
2024-09-09 12:00:47,293 Moving batch for distilgpt2-124m from incoming to running due to time limit with batch size 3
2024-09-09 12:00:47,293 Time limit condition met for model distilgpt2-124m
2024-09-09 12:00:47,293 3
2024-09-09 12:00:47,293 Padding requests generated in 0.0000 seconds
2024-09-09 12:00:47,293 Loading model distilgpt2-124m
2024-09-09 12:00:47,350 Batch processing started at 356426.3653 for model distilgpt2-124m
2024-09-09 12:00:47,870 127.0.0.1 - - [09/Sep/2024 12:00:47] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:48,527 Processed request ID 46aea331 with model distilgpt2-124m
2024-09-09 12:00:48,527 Processed request ID c033873b with model distilgpt2-124m
2024-09-09 12:00:48,528 Processed request ID 41461291 with model distilgpt2-124m
2024-09-09 12:00:48,528 Processed request ID f5f3 with model distilgpt2-124m
2024-09-09 12:00:48,528 Processed batch: ['46aea331', 'c033873b', '41461291', 'f5f3'] with model distilgpt2-124m in 1.1780 seconds
2024-09-09 12:00:48,528 Latency for request 46aea331 with model distilgpt2-124m: 6.7191 seconds
2024-09-09 12:00:48,529 Latency for request c033873b with model distilgpt2-124m: 4.8938 seconds
2024-09-09 12:00:48,529 Latency for request 41461291 with model distilgpt2-124m: 2.0230 seconds
2024-09-09 12:00:48,530 Latency for request f5f3 with model distilgpt2-124m: 1.2355 seconds
2024-09-09 12:00:49,341 127.0.0.1 - - [09/Sep/2024 12:00:49] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:51,171 127.0.0.1 - - [09/Sep/2024 12:00:51] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:52,260 127.0.0.1 - - [09/Sep/2024 12:00:52] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:52,892 Time limit reached for model gpt2medium-355m at 1725897652.8920
2024-09-09 12:00:52,892 Moving batch for gpt2medium-355m from incoming to running due to time limit with batch size 3
2024-09-09 12:00:52,892 Time limit condition met for model gpt2medium-355m
2024-09-09 12:00:52,893 3
2024-09-09 12:00:52,893 Padding requests generated in 0.0001 seconds
2024-09-09 12:00:52,893 Loading model gpt2medium-355m
2024-09-09 12:00:53,088 Batch processing started at 356432.1032 for model gpt2medium-355m
2024-09-09 12:00:53,262 127.0.0.1 - - [09/Sep/2024 12:00:53] "POST /inference HTTP/1.1" 200 -
2024-09-09 12:00:55,652 Processed request ID ef79ad4a with model gpt2medium-355m
2024-09-09 12:00:55,652 Processed request ID 7289302d with model gpt2medium-355m
2024-09-09 12:00:55,652 Processed request ID 9a93a7d2 with model gpt2medium-355m
2024-09-09 12:00:55,653 Processed request ID dbc5 with model gpt2medium-355m
2024-09-09 12:00:55,653 Processed batch: ['ef79ad4a', '7289302d', '9a93a7d2', 'dbc5'] with model gpt2medium-355m in 2.5645 seconds
2024-09-09 12:00:55,653 Latency for request ef79ad4a with model gpt2medium-355m: 10.3216 seconds
2024-09-09 12:00:55,653 Latency for request 7289302d with model gpt2medium-355m: 7.7830 seconds
2024-09-09 12:00:55,654 Latency for request 9a93a7d2 with model gpt2medium-355m: 4.4818 seconds
2024-09-09 12:00:55,654 Latency for request dbc5 with model gpt2medium-355m: 2.7596 seconds
2024-09-09 12:00:55,759 Time limit reached for model gpt2-124m at 1725897655.7596
2024-09-09 12:00:55,759 Moving batch for gpt2-124m from incoming to running due to time limit with batch size 2
2024-09-09 12:00:55,759 Time limit condition met for model gpt2-124m
2024-09-09 12:00:55,759 2
2024-09-09 12:00:55,759 Padding requests generated in 0.0000 seconds
2024-09-09 12:00:55,759 Loading model gpt2-124m
2024-09-09 12:00:55,830 Batch processing started at 356434.8455 for model gpt2-124m
2024-09-09 12:00:56,684 Processed request ID 019c24d3 with model gpt2-124m
2024-09-09 12:00:56,684 Processed request ID 01d6d749 with model gpt2-124m
2024-09-09 12:00:56,684 Processed request ID 70ab with model gpt2-124m
2024-09-09 12:00:56,685 Processed request ID 94eb with model gpt2-124m
2024-09-09 12:00:56,685 Processed batch: ['019c24d3', '01d6d749', '70ab', '94eb'] with model gpt2-124m in 0.8542 seconds
2024-09-09 12:00:56,685 Latency for request 019c24d3 with model gpt2-124m: 7.3440 seconds
2024-09-09 12:00:56,685 Latency for request 01d6d749 with model gpt2-124m: 4.4259 seconds
2024-09-09 12:00:56,686 Latency for request 70ab with model gpt2-124m: 0.9254 seconds
2024-09-09 12:00:56,686 Latency for request 94eb with model gpt2-124m: 0.9254 seconds
2024-09-09 12:00:58,249 Time limit reached for model distilgpt2-124m at 1725897658.2490
2024-09-09 12:00:58,249 Moving batch for distilgpt2-124m from incoming to running due to time limit with batch size 1
2024-09-09 12:00:58,249 Time limit condition met for model distilgpt2-124m
2024-09-09 12:00:58,249 1
2024-09-09 12:00:58,249 Padding requests generated in 0.0001 seconds
2024-09-09 12:00:58,249 Loading model distilgpt2-124m
2024-09-09 12:00:58,321 Batch processing started at 356437.3364 for model distilgpt2-124m
2024-09-09 12:00:58,878 Processed request ID c52cb1c4 with model distilgpt2-124m
2024-09-09 12:00:58,878 Processed request ID c50b with model distilgpt2-124m
2024-09-09 12:00:58,878 Processed request ID bcd0 with model distilgpt2-124m
2024-09-09 12:00:58,879 Processed request ID 2df7 with model distilgpt2-124m
2024-09-09 12:00:58,879 Processed batch: ['c52cb1c4', 'c50b', 'bcd0', '2df7'] with model distilgpt2-124m in 0.5573 seconds
2024-09-09 12:00:58,879 Latency for request c52cb1c4 with model distilgpt2-124m: 5.6165 seconds
2024-09-09 12:00:58,879 Latency for request c50b with model distilgpt2-124m: 0.6294 seconds
2024-09-09 12:00:58,880 Latency for request bcd0 with model distilgpt2-124m: 0.6293 seconds
2024-09-09 12:00:58,880 Latency for request 2df7 with model distilgpt2-124m: 0.6293 seconds
