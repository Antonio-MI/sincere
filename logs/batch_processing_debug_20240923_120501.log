2024-09-23 12:05:01,356 Using device: cuda
2024-09-23 12:05:01,356 Scheduling mode set as HigherBatch+PartialBatch
2024-09-23 12:05:01,356 Monitoring status set to True
2024-09-23 12:05:12,197 Loaded model llama3-8b
2024-09-23 12:05:27,216 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.122.143:5000
2024-09-23 12:05:27,216 [33mPress CTRL+C to quit[0m
2024-09-23 12:05:27,438 Request with ID c9419dff for model llama3-8b received
2024-09-23 12:05:27,439 Request with ID e45d07fd for model gemma-7b received
2024-09-23 12:05:27,439 Current model llama3-8b is already loaded
2024-09-23 12:05:27,440 Keeping current model llama3-8b loaded
2024-09-23 12:05:27,440 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:27,446 127.0.0.1 - - [23/Sep/2024 12:05:27] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:27,446 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:27,447 Next: call load_model for llama3-8b
2024-09-23 12:05:27,453 Batch processing started for model llama3-8b
2024-09-23 12:05:27,501 Request with ID 785b3663 for model granite-7b received
2024-09-23 12:05:27,501 Decided to switch to model granite-7b
2024-09-23 12:05:27,501 Processing batch for new model condition met for model granite-7b
2024-09-23 12:05:27,507 Request with ID 807ed4ee for model llama3-8b received
2024-09-23 12:05:27,508 Current model llama3-8b is already loaded
2024-09-23 12:05:27,508 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:27,508 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:27,538 Request with ID 4df39dfa for model gemma-7b received
2024-09-23 12:05:27,538 Decided to switch to model gemma-7b
2024-09-23 12:05:27,538 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:27,559 Request with ID 7358b00a for model gemma-7b received
2024-09-23 12:05:27,559 Decided to switch to model gemma-7b
2024-09-23 12:05:27,559 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:28,380 Request with ID c23ccfd0 for model granite-7b received
2024-09-23 12:05:28,380 Decided to switch to model granite-7b
2024-09-23 12:05:28,380 Processing batch for new model condition met for model granite-7b
2024-09-23 12:05:28,569 Request with ID 68a175b0 for model llama3-8b received
2024-09-23 12:05:28,569 Current model llama3-8b is already loaded
2024-09-23 12:05:28,569 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:28,569 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:28,571 Request with ID 8736db20 for model llama3-8b received
2024-09-23 12:05:28,571 Current model llama3-8b is already loaded
2024-09-23 12:05:28,571 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:28,571 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:28,906 Request with ID e9a54a24 for model gemma-7b received
2024-09-23 12:05:28,906 Decided to switch to model gemma-7b
2024-09-23 12:05:28,906 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:29,137 Request with ID 4eb2ba0c for model granite-7b received
2024-09-23 12:05:29,137 Decided to switch to model granite-7b
2024-09-23 12:05:29,137 Processing batch for new model condition met for model granite-7b
2024-09-23 12:05:29,619 Request with ID ecfc48e4 for model gemma-7b received
2024-09-23 12:05:29,620 Decided to switch to model gemma-7b
2024-09-23 12:05:29,620 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:29,903 Request with ID 0771bf26 for model llama3-8b received
2024-09-23 12:05:29,903 Current model llama3-8b is already loaded
2024-09-23 12:05:29,903 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:29,903 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:30,063 Request with ID 2f3a0ecd for model llama3-8b received
2024-09-23 12:05:30,063 Current model llama3-8b is already loaded
2024-09-23 12:05:30,063 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:30,063 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:30,072 Processed batch: ['c9419dff'] with model llama3-8b in 2.6184 seconds
2024-09-23 12:05:30,072 Saving sys info
2024-09-23 12:05:30,115 Latency for request c9419dff with model llama3-8b: 2.6330 seconds
2024-09-23 12:05:30,115 Saving results with gpu monitoring
2024-09-23 12:05:30,121 127.0.0.1 - - [23/Sep/2024 12:05:30] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:30,122 Next: call load_model for granite-7b
2024-09-23 12:05:30,122 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'granite-7b'
2024-09-23 12:05:30,123 Next: call load_model for llama3-8b
2024-09-23 12:05:30,125 127.0.0.1 - - [23/Sep/2024 12:05:30] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:30,133 Batch processing started for model llama3-8b
2024-09-23 12:05:30,362 Request with ID 5b19684b for model gemma-7b received
2024-09-23 12:05:30,362 Decided to switch to model gemma-7b
2024-09-23 12:05:30,362 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:30,888 Request with ID d98e006d for model gemma-7b received
2024-09-23 12:05:30,888 Decided to switch to model gemma-7b
2024-09-23 12:05:30,888 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:31,058 Request with ID 770e291b for model granite-7b received
2024-09-23 12:05:31,059 Decided to switch to model granite-7b
2024-09-23 12:05:31,059 Processing batch for new model condition met for model granite-7b
2024-09-23 12:05:31,139 Request with ID 8ab78947 for model llama3-8b received
2024-09-23 12:05:31,139 Current model llama3-8b is already loaded
2024-09-23 12:05:31,139 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:31,139 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:31,317 Request with ID 4feee417 for model llama3-8b received
2024-09-23 12:05:31,318 Current model llama3-8b is already loaded
2024-09-23 12:05:31,318 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:31,318 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:31,384 Request with ID 7977e703 for model llama3-8b received
2024-09-23 12:05:31,384 Current model llama3-8b is already loaded
2024-09-23 12:05:31,384 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:31,384 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:31,400 Request with ID 172eb572 for model gemma-7b received
2024-09-23 12:05:31,400 Decided to switch to model gemma-7b
2024-09-23 12:05:31,400 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:31,472 Request with ID 8844651b for model gemma-7b received
2024-09-23 12:05:31,472 Decided to switch to model gemma-7b
2024-09-23 12:05:31,472 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:31,723 Request with ID bc0e9cff for model llama3-8b received
2024-09-23 12:05:31,723 Current model llama3-8b is already loaded
2024-09-23 12:05:31,723 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:31,723 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:31,738 Request with ID cdf8783b for model llama3-8b received
2024-09-23 12:05:31,738 Current model llama3-8b is already loaded
2024-09-23 12:05:31,738 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:31,739 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,068 Processed batch: ['2f3a0ecd'] with model llama3-8b in 1.9352 seconds
2024-09-23 12:05:32,068 Saving sys info
2024-09-23 12:05:32,099 Latency for request 2f3a0ecd with model llama3-8b: 2.0050 seconds
2024-09-23 12:05:32,099 Saving results with gpu monitoring
2024-09-23 12:05:32,102 127.0.0.1 - - [23/Sep/2024 12:05:32] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:32,102 Next: call load_model for gemma-7b
2024-09-23 12:05:32,103 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'gemma-7b'
2024-09-23 12:05:32,103 No batch to process for model gemma-7b
2024-09-23 12:05:32,104 127.0.0.1 - - [23/Sep/2024 12:05:32] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:32,105 127.0.0.1 - - [23/Sep/2024 12:05:32] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:32,105 Next: call load_model for granite-7b
2024-09-23 12:05:32,106 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'granite-7b'
2024-09-23 12:05:32,106 Next: call load_model for llama3-8b
2024-09-23 12:05:32,107 127.0.0.1 - - [23/Sep/2024 12:05:32] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:32,114 Batch processing started for model llama3-8b
2024-09-23 12:05:32,118 Request with ID a0e53440 for model gemma-7b received
2024-09-23 12:05:32,119 Decided to switch to model gemma-7b
2024-09-23 12:05:32,119 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:32,132 Request with ID 87f15794 for model llama3-8b received
2024-09-23 12:05:32,132 Current model llama3-8b is already loaded
2024-09-23 12:05:32,132 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:32,132 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,315 Request with ID 20363c15 for model llama3-8b received
2024-09-23 12:05:32,315 Current model llama3-8b is already loaded
2024-09-23 12:05:32,315 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:32,315 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,341 Request with ID 5952e306 for model llama3-8b received
2024-09-23 12:05:32,341 Current model llama3-8b is already loaded
2024-09-23 12:05:32,341 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:32,341 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,599 Request with ID 0f7c3af1 for model llama3-8b received
2024-09-23 12:05:32,600 Current model llama3-8b is already loaded
2024-09-23 12:05:32,600 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:32,600 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,736 Request with ID 7ca7897a for model llama3-8b received
2024-09-23 12:05:32,737 Current model llama3-8b is already loaded
2024-09-23 12:05:32,737 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:32,737 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:32,761 Request with ID ce7e901d for model gemma-7b received
2024-09-23 12:05:32,762 Decided to switch to model gemma-7b
2024-09-23 12:05:32,762 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:33,217 Request with ID edb8880b for model llama3-8b received
2024-09-23 12:05:33,217 Current model llama3-8b is already loaded
2024-09-23 12:05:33,217 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:33,217 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:33,458 Request with ID 7d70f0e9 for model llama3-8b received
2024-09-23 12:05:33,458 Current model llama3-8b is already loaded
2024-09-23 12:05:33,458 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:33,458 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:33,511 Request with ID 71834d95 for model llama3-8b received
2024-09-23 12:05:33,511 Current model llama3-8b is already loaded
2024-09-23 12:05:33,511 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:33,511 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:33,751 Request with ID 49d4c3a5 for model llama3-8b received
2024-09-23 12:05:33,751 Current model llama3-8b is already loaded
2024-09-23 12:05:33,752 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:33,752 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,074 Processed batch: ['cdf8783b'] with model llama3-8b in 1.9604 seconds
2024-09-23 12:05:34,074 Saving sys info
2024-09-23 12:05:34,083 Request with ID 1333e38a for model llama3-8b received
2024-09-23 12:05:34,083 Current model llama3-8b is already loaded
2024-09-23 12:05:34,083 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:34,083 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,107 Latency for request cdf8783b with model llama3-8b: 2.3360 seconds
2024-09-23 12:05:34,107 Saving results with gpu monitoring
2024-09-23 12:05:34,110 127.0.0.1 - - [23/Sep/2024 12:05:34] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:34,111 Next: call load_model for llama3-8b
2024-09-23 12:05:34,116 Batch processing started for model llama3-8b
2024-09-23 12:05:34,125 Request with ID 4314a965 for model llama3-8b received
2024-09-23 12:05:34,125 Current model llama3-8b is already loaded
2024-09-23 12:05:34,125 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:34,125 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,219 Processed batch: ['1333e38a'] with model llama3-8b in 0.1027 seconds
2024-09-23 12:05:34,219 Saving sys info
2024-09-23 12:05:34,254 Latency for request 1333e38a with model llama3-8b: 0.1360 seconds
2024-09-23 12:05:34,254 Saving results with gpu monitoring
2024-09-23 12:05:34,257 127.0.0.1 - - [23/Sep/2024 12:05:34] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:34,257 Next: call load_model for gemma-7b
2024-09-23 12:05:34,258 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'gemma-7b'
2024-09-23 12:05:34,259 127.0.0.1 - - [23/Sep/2024 12:05:34] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:34,259 No batch to process for model granite-7b
2024-09-23 12:05:34,260 127.0.0.1 - - [23/Sep/2024 12:05:34] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:34,260 No batch to process for model gemma-7b
2024-09-23 12:05:34,261 127.0.0.1 - - [23/Sep/2024 12:05:34] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:34,261 Next: call load_model for llama3-8b
2024-09-23 12:05:34,269 Batch processing started for model llama3-8b
2024-09-23 12:05:34,457 Request with ID 9cb88e1c for model gemma-7b received
2024-09-23 12:05:34,457 Decided to switch to model gemma-7b
2024-09-23 12:05:34,457 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:34,482 Request with ID f0d953d7 for model llama3-8b received
2024-09-23 12:05:34,483 Request with ID 28ef0728 for model llama3-8b received
2024-09-23 12:05:34,483 Current model llama3-8b is already loaded
2024-09-23 12:05:34,483 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:34,483 Current model llama3-8b is already loaded
2024-09-23 12:05:34,484 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,485 Request with ID 20db89d3 for model llama3-8b received
2024-09-23 12:05:34,486 Current model llama3-8b is already loaded
2024-09-23 12:05:34,485 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:34,486 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:34,486 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,486 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:34,619 Request with ID b762b899 for model gemma-7b received
2024-09-23 12:05:34,620 Decided to switch to model gemma-7b
2024-09-23 12:05:34,620 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:35,195 Request with ID aa3d7b2f for model llama3-8b received
2024-09-23 12:05:35,195 Current model llama3-8b is already loaded
2024-09-23 12:05:35,195 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:35,195 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:35,300 Request with ID 8323169e for model llama3-8b received
2024-09-23 12:05:35,300 Current model llama3-8b is already loaded
2024-09-23 12:05:35,300 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:35,301 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:35,959 Request with ID fe91cc44 for model gemma-7b received
2024-09-23 12:05:35,959 Decided to switch to model gemma-7b
2024-09-23 12:05:35,959 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:35,966 Request with ID 10b31c0f for model llama3-8b received
2024-09-23 12:05:35,966 Current model llama3-8b is already loaded
2024-09-23 12:05:35,967 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:35,967 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:36,075 Request with ID c31e1bd9 for model gemma-7b received
2024-09-23 12:05:36,075 Decided to switch to model gemma-7b
2024-09-23 12:05:36,075 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:36,316 Processed batch: ['4314a965'] with model llama3-8b in 2.0475 seconds
2024-09-23 12:05:36,317 Saving sys info
2024-09-23 12:05:36,352 Latency for request 4314a965 with model llama3-8b: 2.1910 seconds
2024-09-23 12:05:36,352 Saving results with gpu monitoring
2024-09-23 12:05:36,355 127.0.0.1 - - [23/Sep/2024 12:05:36] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:36,355 Next: call load_model for llama3-8b
2024-09-23 12:05:36,362 Batch processing started for model llama3-8b
2024-09-23 12:05:36,554 Request with ID 24c159e9 for model gemma-7b received
2024-09-23 12:05:36,555 Decided to switch to model gemma-7b
2024-09-23 12:05:36,555 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:36,691 Request with ID 54adf413 for model llama3-8b received
2024-09-23 12:05:36,691 Current model llama3-8b is already loaded
2024-09-23 12:05:36,691 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:36,691 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:36,872 Request with ID 3c27cbe8 for model gemma-7b received
2024-09-23 12:05:36,873 Decided to switch to model gemma-7b
2024-09-23 12:05:36,873 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:36,929 Request with ID 3cedefe5 for model gemma-7b received
2024-09-23 12:05:36,929 Decided to switch to model gemma-7b
2024-09-23 12:05:36,929 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:37,131 Request with ID 82ac09ec for model gemma-7b received
2024-09-23 12:05:37,131 Decided to switch to model gemma-7b
2024-09-23 12:05:37,131 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:37,471 Request with ID 9b5c70d5 for model gemma-7b received
2024-09-23 12:05:37,472 Decided to switch to model gemma-7b
2024-09-23 12:05:37,472 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:37,615 Request with ID e6051c27 for model llama3-8b received
2024-09-23 12:05:37,615 Current model llama3-8b is already loaded
2024-09-23 12:05:37,615 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:37,615 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:37,843 Request with ID 37e63b0e for model llama3-8b received
2024-09-23 12:05:37,843 Current model llama3-8b is already loaded
2024-09-23 12:05:37,843 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:37,844 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:37,858 Processed batch: ['10b31c0f'] with model llama3-8b in 1.4967 seconds
2024-09-23 12:05:37,858 Saving sys info
2024-09-23 12:05:37,894 Latency for request 10b31c0f with model llama3-8b: 1.8920 seconds
2024-09-23 12:05:37,894 Saving results with gpu monitoring
2024-09-23 12:05:37,897 Request with ID 55dafe98 for model gemma-7b received
2024-09-23 12:05:37,898 Decided to switch to model gemma-7b
2024-09-23 12:05:37,898 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:37,899 127.0.0.1 - - [23/Sep/2024 12:05:37] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:37,899 Next: call load_model for gemma-7b
2024-09-23 12:05:37,900 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'gemma-7b'
2024-09-23 12:05:37,900 No batch to process for model gemma-7b
2024-09-23 12:05:37,901 127.0.0.1 - - [23/Sep/2024 12:05:37] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:37,902 127.0.0.1 - - [23/Sep/2024 12:05:37] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:37,902 No batch to process for model granite-7b
2024-09-23 12:05:37,903 127.0.0.1 - - [23/Sep/2024 12:05:37] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:37,904 Next: call load_model for llama3-8b
2024-09-23 12:05:37,910 Batch processing started for model llama3-8b
2024-09-23 12:05:38,132 Request with ID 61d345a3 for model llama3-8b received
2024-09-23 12:05:38,132 Current model llama3-8b is already loaded
2024-09-23 12:05:38,132 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:38,132 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:38,498 Request with ID 2ef257af for model gemma-7b received
2024-09-23 12:05:38,498 Decided to switch to model gemma-7b
2024-09-23 12:05:38,498 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:38,523 Request with ID a35d20a7 for model llama3-8b received
2024-09-23 12:05:38,523 Current model llama3-8b is already loaded
2024-09-23 12:05:38,523 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:38,523 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:38,585 Request with ID dbc6021f for model gemma-7b received
2024-09-23 12:05:38,585 Decided to switch to model gemma-7b
2024-09-23 12:05:38,585 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:38,995 Processed batch: ['37e63b0e'] with model llama3-8b in 1.0847 seconds
2024-09-23 12:05:38,995 Saving sys info
2024-09-23 12:05:39,028 Latency for request 37e63b0e with model llama3-8b: 1.1520 seconds
2024-09-23 12:05:39,029 Saving results with gpu monitoring
2024-09-23 12:05:39,032 127.0.0.1 - - [23/Sep/2024 12:05:39] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:39,032 Next: call load_model for llama3-8b
2024-09-23 12:05:39,038 Batch processing started for model llama3-8b
2024-09-23 12:05:39,393 Request with ID 782f13aa for model gemma-7b received
2024-09-23 12:05:39,393 Decided to switch to model gemma-7b
2024-09-23 12:05:39,393 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:39,413 Request with ID 823cf544 for model llama3-8b received
2024-09-23 12:05:39,413 Current model llama3-8b is already loaded
2024-09-23 12:05:39,413 Processing partial batch for model llama3-8b after stay time
2024-09-23 12:05:39,414 Partial batch after stay time condition met for model llama3-8b
2024-09-23 12:05:39,617 Request with ID 75ceb23c for model gemma-7b received
2024-09-23 12:05:39,617 Decided to switch to model gemma-7b
2024-09-23 12:05:39,617 Processing batch for new model condition met for model gemma-7b
2024-09-23 12:05:41,001 Processed batch: ['a35d20a7'] with model llama3-8b in 1.9631 seconds
2024-09-23 12:05:41,001 Saving sys info
2024-09-23 12:05:41,035 Latency for request a35d20a7 with model llama3-8b: 2.4780 seconds
2024-09-23 12:05:41,035 Saving results with gpu monitoring
2024-09-23 12:05:41,038 127.0.0.1 - - [23/Sep/2024 12:05:41] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:41,038 Next: call load_model for llama3-8b
2024-09-23 12:05:41,044 Batch processing started for model llama3-8b
2024-09-23 12:05:42,968 Processed batch: ['823cf544'] with model llama3-8b in 1.9235 seconds
2024-09-23 12:05:42,968 Saving sys info
2024-09-23 12:05:42,999 Latency for request 823cf544 with model llama3-8b: 3.5540 seconds
2024-09-23 12:05:42,999 Saving results with gpu monitoring
2024-09-23 12:05:43,002 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,002 Next: call load_model for gemma-7b
2024-09-23 12:05:43,002 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 518, in inference
    completed_inference_ids = process_batch(model_alias, "Processing batch for new model", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'gemma-7b'
2024-09-23 12:05:43,003 No batch to process for model gemma-7b
2024-09-23 12:05:43,004 127.0.0.1 - - [23/Sep/2024 12:05:43] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 12:05:43,005 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,005 No batch to process for model llama3-8b
2024-09-23 12:05:43,006 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,006 No batch to process for model llama3-8b
2024-09-23 12:05:43,007 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,007 No batch to process for model gemma-7b
2024-09-23 12:05:43,008 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,008 No batch to process for model llama3-8b
2024-09-23 12:05:43,009 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,009 No batch to process for model llama3-8b
2024-09-23 12:05:43,010 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,011 No batch to process for model llama3-8b
2024-09-23 12:05:43,012 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,012 No batch to process for model llama3-8b
2024-09-23 12:05:43,012 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,013 No batch to process for model llama3-8b
2024-09-23 12:05:43,013 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,014 No batch to process for model gemma-7b
2024-09-23 12:05:43,015 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,015 No batch to process for model llama3-8b
2024-09-23 12:05:43,016 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,016 No batch to process for model llama3-8b
2024-09-23 12:05:43,017 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,017 No batch to process for model llama3-8b
2024-09-23 12:05:43,018 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,018 No batch to process for model llama3-8b
2024-09-23 12:05:43,018 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,019 No batch to process for model llama3-8b
2024-09-23 12:05:43,019 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,020 No batch to process for model llama3-8b
2024-09-23 12:05:43,021 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,021 No batch to process for model gemma-7b
2024-09-23 12:05:43,022 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,022 No batch to process for model llama3-8b
2024-09-23 12:05:43,023 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,024 No batch to process for model llama3-8b
2024-09-23 12:05:43,024 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,025 No batch to process for model llama3-8b
2024-09-23 12:05:43,025 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,026 No batch to process for model gemma-7b
2024-09-23 12:05:43,026 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,027 No batch to process for model llama3-8b
2024-09-23 12:05:43,027 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,027 No batch to process for model llama3-8b
2024-09-23 12:05:43,028 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,028 No batch to process for model gemma-7b
2024-09-23 12:05:43,029 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,029 No batch to process for model llama3-8b
2024-09-23 12:05:43,030 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,030 No batch to process for model gemma-7b
2024-09-23 12:05:43,031 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,031 No batch to process for model gemma-7b
2024-09-23 12:05:43,031 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,032 No batch to process for model llama3-8b
2024-09-23 12:05:43,032 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,032 No batch to process for model gemma-7b
2024-09-23 12:05:43,033 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,033 No batch to process for model gemma-7b
2024-09-23 12:05:43,034 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,035 No batch to process for model gemma-7b
2024-09-23 12:05:43,035 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,036 No batch to process for model gemma-7b
2024-09-23 12:05:43,036 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,036 No batch to process for model llama3-8b
2024-09-23 12:05:43,037 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,037 No batch to process for model llama3-8b
2024-09-23 12:05:43,038 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,039 No batch to process for model gemma-7b
2024-09-23 12:05:43,039 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,039 No batch to process for model llama3-8b
2024-09-23 12:05:43,039 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,041 No batch to process for model gemma-7b
2024-09-23 12:05:43,041 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,041 No batch to process for model llama3-8b
2024-09-23 12:05:43,042 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,042 No batch to process for model gemma-7b
2024-09-23 12:05:43,042 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,043 No batch to process for model gemma-7b
2024-09-23 12:05:43,043 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,043 No batch to process for model llama3-8b
2024-09-23 12:05:43,044 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
2024-09-23 12:05:43,044 No batch to process for model gemma-7b
2024-09-23 12:05:43,045 127.0.0.1 - - [23/Sep/2024 12:05:43] "POST /inference HTTP/1.1" 200 -
