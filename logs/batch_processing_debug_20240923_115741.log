2024-09-23 11:57:41,476 Using device: cuda
2024-09-23 11:57:41,476 Scheduling mode set as HigherBatch+PartialBatch
2024-09-23 11:57:41,476 Monitoring status set to True
2024-09-23 11:57:58,363 Loaded model llama3-8b
2024-09-23 11:58:13,383 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.122.143:5000
2024-09-23 11:58:13,383 [33mPress CTRL+C to quit[0m
2024-09-23 11:58:18,210 Request with ID 663efc40 for model gemma-7b received
2024-09-23 11:58:18,211 Decided to switch to model gemma-7b
2024-09-23 11:58:18,285 Unloaded previous model
2024-09-23 11:58:18,817 Request with ID 0b51c0bf for model llama3-8b received
2024-09-23 11:58:18,819 Current model llama3-8b is already loaded
2024-09-23 11:58:18,819 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:18,819 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:18,819 Next: call load_model for llama3-8b
2024-09-23 11:58:18,820 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:18,822 Request with ID 8b2ac1e9 for model llama3-8b received
2024-09-23 11:58:18,826 Request with ID 6796a990 for model granite-7b received
2024-09-23 11:58:18,833 127.0.0.1 - - [23/Sep/2024 11:58:18] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:18,833 Current model llama3-8b is already loaded
2024-09-23 11:58:18,834 Keeping current model llama3-8b loaded
2024-09-23 11:58:18,834 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:18,835 127.0.0.1 - - [23/Sep/2024 11:58:18] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:18,837 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:18,838 Next: call load_model for llama3-8b
2024-09-23 11:58:18,839 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:18,840 127.0.0.1 - - [23/Sep/2024 11:58:18] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:19,614 Request with ID b2be3eef for model llama3-8b received
2024-09-23 11:58:19,618 Current model llama3-8b is already loaded
2024-09-23 11:58:19,618 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:19,618 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:19,618 Next: call load_model for llama3-8b
2024-09-23 11:58:19,618 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:19,636 127.0.0.1 - - [23/Sep/2024 11:58:19] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:19,639 Request with ID 36e9e793 for model gemma-7b received
2024-09-23 11:58:19,767 Decided to switch to model gemma-7b
2024-09-23 11:58:19,775 Request with ID c7f72411 for model granite-7b received
2024-09-23 11:58:19,779 Decided to switch to model granite-7b
2024-09-23 11:58:19,789 Request with ID afbc33e2 for model llama3-8b received
2024-09-23 11:58:19,794 Current model llama3-8b is already loaded
2024-09-23 11:58:19,794 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:19,794 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:19,794 Next: call load_model for llama3-8b
2024-09-23 11:58:19,794 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:19,796 Request with ID c08726d6 for model gemma-7b received
2024-09-23 11:58:19,798 127.0.0.1 - - [23/Sep/2024 11:58:19] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:19,801 Decided to switch to model gemma-7b
2024-09-23 11:58:19,821 Request with ID f28da278 for model gemma-7b received
2024-09-23 11:58:20,406 Decided to switch to model gemma-7b
2024-09-23 11:58:20,424 Request with ID 1cd4bfc6 for model granite-7b received
2024-09-23 11:58:20,452 Decided to switch to model granite-7b
2024-09-23 11:58:21,381 Request with ID 6403a802 for model llama3-8b received
2024-09-23 11:58:21,381 Current model llama3-8b is already loaded
2024-09-23 11:58:21,381 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:21,381 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:21,381 Next: call load_model for llama3-8b
2024-09-23 11:58:21,381 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:21,383 Request with ID 9d76c688 for model llama3-8b received
2024-09-23 11:58:21,384 127.0.0.1 - - [23/Sep/2024 11:58:21] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:21,397 Current model llama3-8b is already loaded
2024-09-23 11:58:21,401 Request with ID ee109b2b for model gemma-7b received
2024-09-23 11:58:21,404 Request with ID 7c5b75d8 for model granite-7b received
2024-09-23 11:58:21,407 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:21,408 Decided to switch to model gemma-7b
2024-09-23 11:58:21,410 Decided to switch to model granite-7b
2024-09-23 11:58:21,411 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:21,415 Next: call load_model for llama3-8b
2024-09-23 11:58:21,415 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:21,417 127.0.0.1 - - [23/Sep/2024 11:58:21] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:22,169 Request with ID 34889af0 for model gemma-7b received
2024-09-23 11:58:22,170 Decided to switch to model gemma-7b
2024-09-23 11:58:22,177 Request with ID 22aab77a for model llama3-8b received
2024-09-23 11:58:22,181 Current model llama3-8b is already loaded
2024-09-23 11:58:22,181 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:22,181 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:22,181 Next: call load_model for llama3-8b
2024-09-23 11:58:22,181 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:22,205 Request with ID 2b3c9d90 for model llama3-8b received
2024-09-23 11:58:22,796 127.0.0.1 - - [23/Sep/2024 11:58:22] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:22,805 Current model llama3-8b is already loaded
2024-09-23 11:58:22,817 Request with ID 45fdcc89 for model gemma-7b received
2024-09-23 11:58:22,821 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:22,824 Decided to switch to model gemma-7b
2024-09-23 11:58:22,828 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:22,843 Next: call load_model for llama3-8b
2024-09-23 11:58:22,851 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:22,859 127.0.0.1 - - [23/Sep/2024 11:58:22] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:23,298 Request with ID a9402409 for model gemma-7b received
2024-09-23 11:58:23,305 Decided to switch to model gemma-7b
2024-09-23 11:58:23,317 Request with ID 2c3d794f for model granite-7b received
2024-09-23 11:58:23,317 Decided to switch to model granite-7b
2024-09-23 11:58:23,333 Request with ID 4da068cf for model llama3-8b received
2024-09-23 11:58:23,411 Current model llama3-8b is already loaded
2024-09-23 11:58:23,417 Request with ID 07c174f4 for model llama3-8b received
2024-09-23 11:58:23,418 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:23,475 Request with ID 6db0f8fd for model llama3-8b received
2024-09-23 11:58:23,551 Current model llama3-8b is already loaded
2024-09-23 11:58:23,558 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:23,637 Current model llama3-8b is already loaded
2024-09-23 11:58:23,638 Request with ID 2368951a for model gemma-7b received
2024-09-23 11:58:24,142 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,144 Next: call load_model for llama3-8b
2024-09-23 11:58:24,146 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,147 Request with ID 4df859db for model gemma-7b received
2024-09-23 11:58:24,161 Decided to switch to model gemma-7b
2024-09-23 11:58:24,171 Request with ID 8a1f1815 for model llama3-8b received
2024-09-23 11:58:24,177 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,182 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:24,191 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,193 Request with ID 5e9b95ed for model llama3-8b received
2024-09-23 11:58:24,194 Request with ID 5b450c17 for model gemma-7b received
2024-09-23 11:58:24,199 Decided to switch to model gemma-7b
2024-09-23 11:58:24,206 Request with ID 9d8626b1 for model llama3-8b received
2024-09-23 11:58:24,207 Current model llama3-8b is already loaded
2024-09-23 11:58:24,208 No batch to process for model llama3-8b
2024-09-23 11:58:24,663 Request with ID 810eb576 for model llama3-8b received
2024-09-23 11:58:24,669 127.0.0.1 - - [23/Sep/2024 11:58:24] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:24,674 Request with ID 8355b1d4 for model llama3-8b received
2024-09-23 11:58:24,677 Current model llama3-8b is already loaded
2024-09-23 11:58:24,679 Request with ID 6df8cecb for model llama3-8b received
2024-09-23 11:58:24,680 Decided to switch to model gemma-7b
2024-09-23 11:58:24,686 Processing pending requests for current model llama3-8b before switching
2024-09-23 11:58:24,689 Current model llama3-8b is already loaded
2024-09-23 11:58:24,690 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,694 127.0.0.1 - - [23/Sep/2024 11:58:24] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:24,697 Current model llama3-8b is already loaded
2024-09-23 11:58:24,701 No batch to process for model llama3-8b
2024-09-23 11:58:24,702 Current model llama3-8b is already loaded
2024-09-23 11:58:24,705 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,707 Current model llama3-8b is already loaded
2024-09-23 11:58:24,708 Processing pending requests for current model llama3-8b before switching
2024-09-23 11:58:24,710 Processing pending requests before switch condition met for model llama3-8b
2024-09-23 11:58:24,717 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,722 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,842 Request with ID 83702e5d for model llama3-8b received
2024-09-23 11:58:24,916 Current model llama3-8b is already loaded
2024-09-23 11:58:24,852 Request with ID 4911d257 for model gemma-7b received
2024-09-23 11:58:24,859 127.0.0.1 - - [23/Sep/2024 11:58:24] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:24,872 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,878 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,881 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,894 Processing pending requests before switch condition met for model llama3-8b
2024-09-23 11:58:24,903 No batch to process for model llama3-8b
2024-09-23 11:58:24,906 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,849 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,919 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:24,926 Decided to switch to model gemma-7b
2024-09-23 11:58:24,958 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,965 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:24,981 No batch to process for model llama3-8b
2024-09-23 11:58:24,991 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:25,433 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:25,435 Request with ID daf98bc6 for model llama3-8b received
2024-09-23 11:58:25,450 127.0.0.1 - - [23/Sep/2024 11:58:25] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:25,451 No batch to process for model llama3-8b
2024-09-23 11:58:25,476 Current model llama3-8b is already loaded
2024-09-23 11:58:25,478 Request with ID 21f64ffd for model llama3-8b received
2024-09-23 11:58:25,481 127.0.0.1 - - [23/Sep/2024 11:58:25] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:25,482 No batch to process for model llama3-8b
2024-09-23 11:58:25,484 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:25,490 Current model llama3-8b is already loaded
2024-09-23 11:58:25,947 Request with ID 0d83ac1c for model llama3-8b received
2024-09-23 11:58:25,950 No batch to process for model llama3-8b
2024-09-23 11:58:25,952 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:25,955 Request with ID 5561d2d4 for model llama3-8b received
2024-09-23 11:58:25,958 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:25,967 Current model llama3-8b is already loaded
2024-09-23 11:58:25,970 127.0.0.1 - - [23/Sep/2024 11:58:25] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:25,972 Next: call load_model for llama3-8b
2024-09-23 11:58:25,981 Current model llama3-8b is already loaded
2024-09-23 11:58:26,557 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:26,574 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:26,676 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:26,678 Next: call load_model for llama3-8b
2024-09-23 11:58:26,679 Processing partial batch for model llama3-8b after stay time
2024-09-23 11:58:26,684 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:26,688 127.0.0.1 - - [23/Sep/2024 11:58:26] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:26,691 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/update.py", line 487, in inference
    completed_inference_ids = process_batch(model_alias, "Partial batch after stay time", running_request_batches[model_alias].qsize())
  File "/home/amartinezi/sincere/update.py", line 326, in process_batch
    model=loaded_models[model_alias]["model"],
KeyError: 'llama3-8b'
2024-09-23 11:58:26,692 No batch to process for model llama3-8b
2024-09-23 11:58:26,693 Partial batch after stay time condition met for model llama3-8b
2024-09-23 11:58:26,712 127.0.0.1 - - [23/Sep/2024 11:58:26] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-23 11:58:26,720 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:26,721 No batch to process for model llama3-8b
2024-09-23 11:58:26,750 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:26,752 No batch to process for model llama3-8b
2024-09-23 11:58:26,755 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:26,756 No batch to process for model llama3-8b
2024-09-23 11:58:26,756 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:26,758 No batch to process for model llama3-8b
2024-09-23 11:58:26,761 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
2024-09-23 11:58:26,762 No batch to process for model llama3-8b
2024-09-23 11:58:26,763 127.0.0.1 - - [23/Sep/2024 11:58:26] "POST /inference HTTP/1.1" 200 -
