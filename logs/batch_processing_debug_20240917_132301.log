2024-09-17 13:23:01,193 Using device: cuda
2024-09-17 13:23:01,194 Scheduling mode set as FCFS
2024-09-17 13:23:16,212 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.122.143:5000
2024-09-17 13:23:16,212 [33mPress CTRL+C to quit[0m
2024-09-17 13:23:52,923 Request with ID 7ed5d3d6 for model llama3-8b received
2024-09-17 13:23:52,923 Batch size condition met for model llama3-8b
2024-09-17 13:23:52,923 Next: call load_model for llama3-8b
2024-09-17 13:23:54,259 Request with ID 132dc00a for model llama3-8b received
2024-09-17 13:23:54,260 Batch size condition met for model llama3-8b
2024-09-17 13:23:56,511 Request with ID 464146ca for model granite-7b received
2024-09-17 13:23:56,511 Batch size condition met for model granite-7b
2024-09-17 13:23:58,840 Request with ID b71e1883 for model gemma-7b received
2024-09-17 13:23:58,840 Batch size condition met for model gemma-7b
2024-09-17 13:24:00,276 Request with ID 19deb3d8 for model granite-7b received
2024-09-17 13:24:00,276 Batch size condition met for model granite-7b
2024-09-17 13:24:02,458 Request with ID eb845a80 for model llama3-8b received
2024-09-17 13:24:02,459 Batch size condition met for model llama3-8b
2024-09-17 13:24:04,736 Request with ID 1383b76d for model llama3-8b received
2024-09-17 13:24:04,737 Batch size condition met for model llama3-8b
2024-09-17 13:24:05,822 Loaded model llama3-8b
2024-09-17 13:24:05,825 Batch processing started for model llama3-8b
2024-09-17 13:24:06,921 Request with ID 5e90bbf4 for model granite-7b received
2024-09-17 13:24:06,922 Batch size condition met for model granite-7b
2024-09-17 13:24:07,621 Request with ID d66eb833 for model granite-7b received
2024-09-17 13:24:07,622 Batch size condition met for model granite-7b
2024-09-17 13:24:08,282 Request with ID 99e4c348 for model gemma-7b received
2024-09-17 13:24:08,282 Batch size condition met for model gemma-7b
2024-09-17 13:24:08,323 Processed batch: ['7ed5d3d6'] with model llama3-8b in 2.4980 seconds
2024-09-17 13:24:08,324 Saving sys info
2024-09-17 13:24:08,324 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 500, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 264, in process_batch
    sys_info = monitor.get_sys_info()
NameError: name 'monitor' is not defined
2024-09-17 13:24:08,324 Next: call load_model for llama3-8b
2024-09-17 13:24:08,325 127.0.0.1 - - [17/Sep/2024 13:24:08] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 13:24:08,326 Model llama3-8b already loaded
2024-09-17 13:24:08,331 Batch processing started for model llama3-8b
2024-09-17 13:24:09,098 Request with ID 1fbc344b for model llama3-8b received
2024-09-17 13:24:09,099 Batch size condition met for model llama3-8b
2024-09-17 13:24:10,230 Processed batch: ['1383b76d'] with model llama3-8b in 1.8992 seconds
2024-09-17 13:24:10,230 Saving sys info
2024-09-17 13:24:10,230 Exception on /inference [POST]
Traceback (most recent call last):
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/amartinezi/sincere/venv/lib64/python3.9/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 500, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
  File "/home/amartinezi/sincere/api_scheduler_experiments.py", line 264, in process_batch
    sys_info = monitor.get_sys_info()
NameError: name 'monitor' is not defined
2024-09-17 13:24:10,231 Next: call load_model for granite-7b
2024-09-17 13:24:10,231 127.0.0.1 - - [17/Sep/2024 13:24:10] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-17 13:24:10,231 Unloaded previous model
2024-09-17 13:24:10,766 Request with ID a5e659ca for model gemma-7b received
2024-09-17 13:24:10,766 Batch size condition met for model gemma-7b
2024-09-17 13:24:12,928 Waiting for running processes to finish
2024-09-17 13:24:13,029 Waiting for running processes to finish
2024-09-17 13:24:13,130 Waiting for running processes to finish
2024-09-17 13:24:13,230 Waiting for running processes to finish
2024-09-17 13:24:13,331 Waiting for running processes to finish
2024-09-17 13:24:13,432 Waiting for running processes to finish
2024-09-17 13:24:13,532 Waiting for running processes to finish
2024-09-17 13:24:13,633 Waiting for running processes to finish
2024-09-17 13:24:13,734 Waiting for running processes to finish
2024-09-17 13:24:13,834 Waiting for running processes to finish
2024-09-17 13:24:13,935 Waiting for running processes to finish
2024-09-17 13:24:14,036 Waiting for running processes to finish
2024-09-17 13:24:14,136 Waiting for running processes to finish
2024-09-17 13:24:14,237 Waiting for running processes to finish
2024-09-17 13:24:14,338 Waiting for running processes to finish
2024-09-17 13:24:14,438 Waiting for running processes to finish
2024-09-17 13:24:14,602 Waiting for running processes to finish
2024-09-17 13:24:14,703 Waiting for running processes to finish
2024-09-17 13:24:14,804 Waiting for running processes to finish
2024-09-17 13:24:14,904 Waiting for running processes to finish
2024-09-17 13:24:15,005 Waiting for running processes to finish
2024-09-17 13:24:15,106 Waiting for running processes to finish
2024-09-17 13:24:15,206 Waiting for running processes to finish
2024-09-17 13:24:15,307 Waiting for running processes to finish
2024-09-17 13:24:15,408 Waiting for running processes to finish
2024-09-17 13:24:15,509 Waiting for running processes to finish
2024-09-17 13:24:15,609 Waiting for running processes to finish
2024-09-17 13:24:15,710 Waiting for running processes to finish
2024-09-17 13:24:15,811 Waiting for running processes to finish
2024-09-17 13:24:15,912 Waiting for running processes to finish
2024-09-17 13:24:16,012 Waiting for running processes to finish
2024-09-17 13:24:16,113 Waiting for running processes to finish
2024-09-17 13:24:16,214 Waiting for running processes to finish
2024-09-17 13:24:16,314 Waiting for running processes to finish
2024-09-17 13:24:16,415 Waiting for running processes to finish
