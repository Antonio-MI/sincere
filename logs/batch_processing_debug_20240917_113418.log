2024-09-17 11:34:18,522 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-17 11:34:18,522 [33mPress CTRL+C to quit[0m
2024-09-17 11:34:22,212 Request with ID 18c79e5c for model gpt2medium-355m received
2024-09-17 11:34:22,212 Batch size condition met for model gpt2medium-355m
2024-09-17 11:34:22,212 Next: call load_model for gpt2medium-355m
2024-09-17 11:34:22,338 Loaded model gpt2medium-355m
2024-09-17 11:34:22,338 Batch processing started for model gpt2medium-355m
2024-09-17 11:34:23,539 Request with ID d0b6892b for model gpt2medium-355m received
2024-09-17 11:34:23,539 Batch size condition met for model gpt2medium-355m
2024-09-17 11:34:24,597 Processed batch: ['18c79e5c'] with model gpt2medium-355m in 2.2585 seconds
2024-09-17 11:34:24,597 Latency for request 18c79e5c with model gpt2medium-355m: 2.3847 seconds
2024-09-17 11:34:24,599 127.0.0.1 - - [17/Sep/2024 11:34:24] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:34:24,599 Next: call load_model for gpt2medium-355m
2024-09-17 11:34:24,599 Model gpt2medium-355m already loaded
2024-09-17 11:34:24,599 Batch processing started for model gpt2medium-355m
2024-09-17 11:34:25,794 Request with ID 7dd6f447 for model gpt2-124m received
2024-09-17 11:34:25,794 Batch size condition met for model gpt2-124m
2024-09-17 11:34:25,869 Processed batch: ['d0b6892b'] with model gpt2medium-355m in 1.2696 seconds
2024-09-17 11:34:25,869 Latency for request d0b6892b with model gpt2medium-355m: 2.3300 seconds
2024-09-17 11:34:25,870 127.0.0.1 - - [17/Sep/2024 11:34:25] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:34:25,870 Next: call load_model for gpt2-124m
2024-09-17 11:34:25,877 Unloaded previous model
2024-09-17 11:34:25,942 Loaded model gpt2-124m
2024-09-17 11:34:25,942 Batch processing started for model gpt2-124m
2024-09-17 11:34:27,298 Processed batch: ['7dd6f447'] with model gpt2-124m in 1.3559 seconds
2024-09-17 11:34:27,298 Latency for request 7dd6f447 with model gpt2-124m: 1.5042 seconds
2024-09-17 11:34:27,299 127.0.0.1 - - [17/Sep/2024 11:34:27] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:34:28,122 Request with ID 3dcb323d for model distilgpt2-124m received
2024-09-17 11:34:28,122 Batch size condition met for model distilgpt2-124m
2024-09-17 11:34:28,122 Next: call load_model for distilgpt2-124m
2024-09-17 11:34:28,135 Unloaded previous model
2024-09-17 11:34:28,189 Loaded model distilgpt2-124m
2024-09-17 11:34:28,189 Batch processing started for model distilgpt2-124m
2024-09-17 11:34:29,371 Processed batch: ['3dcb323d'] with model distilgpt2-124m in 1.1815 seconds
2024-09-17 11:34:29,371 Latency for request 3dcb323d with model distilgpt2-124m: 1.2489 seconds
2024-09-17 11:34:29,372 127.0.0.1 - - [17/Sep/2024 11:34:29] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:34:29,555 Request with ID 29382978 for model gpt2-124m received
2024-09-17 11:34:29,555 Batch size condition met for model gpt2-124m
2024-09-17 11:34:29,555 Next: call load_model for gpt2-124m
2024-09-17 11:34:29,560 Unloaded previous model
2024-09-17 11:34:29,619 Loaded model gpt2-124m
2024-09-17 11:34:29,619 Batch processing started for model gpt2-124m
2024-09-17 11:34:30,441 Processed batch: ['29382978'] with model gpt2-124m in 0.8218 seconds
2024-09-17 11:34:30,441 Latency for request 29382978 with model gpt2-124m: 0.8861 seconds
2024-09-17 11:34:30,441 127.0.0.1 - - [17/Sep/2024 11:34:30] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:34:31,739 Request with ID aae74ef6 for model gpt2medium-355m received
2024-09-17 11:34:31,739 Batch size condition met for model gpt2medium-355m
2024-09-17 11:34:31,740 Next: call load_model for gpt2medium-355m
2024-09-17 11:34:31,762 Unloaded previous model
2024-09-17 11:34:31,887 Loaded model gpt2medium-355m
2024-09-17 11:34:31,887 Batch processing started for model gpt2medium-355m
2024-09-17 11:34:32,508 Processed batch: ['aae74ef6'] with model gpt2medium-355m in 0.6209 seconds
2024-09-17 11:34:32,508 Latency for request aae74ef6 with model gpt2medium-355m: 0.7693 seconds
2024-09-17 11:34:32,509 127.0.0.1 - - [17/Sep/2024 11:34:32] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:35:31,521 Request with ID 42bf435a for model gpt2medium-355m received
2024-09-17 11:35:31,521 Batch size condition met for model gpt2medium-355m
2024-09-17 11:35:31,521 Next: call load_model for gpt2medium-355m
2024-09-17 11:35:31,521 Model gpt2medium-355m already loaded
2024-09-17 11:35:31,521 Batch processing started for model gpt2medium-355m
2024-09-17 11:35:32,847 Request with ID b969d64c for model gpt2medium-355m received
2024-09-17 11:35:32,848 Batch size condition met for model gpt2medium-355m
2024-09-17 11:35:33,696 Processed batch: ['42bf435a'] with model gpt2medium-355m in 2.1751 seconds
2024-09-17 11:35:33,696 Latency for request 42bf435a with model gpt2medium-355m: 2.1755 seconds
2024-09-17 11:35:33,697 127.0.0.1 - - [17/Sep/2024 11:35:33] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:35:33,697 Next: call load_model for gpt2medium-355m
2024-09-17 11:35:33,697 Model gpt2medium-355m already loaded
2024-09-17 11:35:33,697 Batch processing started for model gpt2medium-355m
2024-09-17 11:35:35,103 Request with ID 30ffda7d for model gpt2-124m received
2024-09-17 11:35:35,103 Batch size condition met for model gpt2-124m
2024-09-17 11:35:35,908 Processed batch: ['b969d64c'] with model gpt2medium-355m in 2.2107 seconds
2024-09-17 11:35:35,908 Latency for request b969d64c with model gpt2medium-355m: 3.0606 seconds
2024-09-17 11:35:35,909 127.0.0.1 - - [17/Sep/2024 11:35:35] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:35:35,909 Next: call load_model for gpt2-124m
2024-09-17 11:35:35,917 Unloaded previous model
2024-09-17 11:35:35,974 Loaded model gpt2-124m
2024-09-17 11:35:35,974 Batch processing started for model gpt2-124m
2024-09-17 11:35:36,895 Processed batch: ['30ffda7d'] with model gpt2-124m in 0.9210 seconds
2024-09-17 11:35:36,895 Latency for request 30ffda7d with model gpt2-124m: 1.7920 seconds
2024-09-17 11:35:36,896 127.0.0.1 - - [17/Sep/2024 11:35:36] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:35:37,435 Request with ID f172de9e for model distilgpt2-124m received
2024-09-17 11:35:37,435 Batch size condition met for model distilgpt2-124m
2024-09-17 11:35:37,436 Next: call load_model for distilgpt2-124m
2024-09-17 11:35:37,457 Unloaded previous model
2024-09-17 11:35:37,518 Loaded model distilgpt2-124m
2024-09-17 11:35:37,518 Batch processing started for model distilgpt2-124m
2024-09-17 11:35:38,616 Processed batch: ['f172de9e'] with model distilgpt2-124m in 1.0987 seconds
2024-09-17 11:35:38,617 Latency for request f172de9e with model distilgpt2-124m: 1.1815 seconds
2024-09-17 11:35:38,617 127.0.0.1 - - [17/Sep/2024 11:35:38] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:35:38,864 Request with ID 0945baeb for model gpt2-124m received
2024-09-17 11:35:38,864 Batch size condition met for model gpt2-124m
2024-09-17 11:35:38,864 Next: call load_model for gpt2-124m
2024-09-17 11:35:38,870 Unloaded previous model
2024-09-17 11:35:38,929 Loaded model gpt2-124m
2024-09-17 11:35:38,929 Batch processing started for model gpt2-124m
2024-09-17 11:35:39,771 Processed batch: ['0945baeb'] with model gpt2-124m in 0.8423 seconds
2024-09-17 11:35:39,771 Latency for request 0945baeb with model gpt2-124m: 0.9069 seconds
2024-09-17 11:35:39,772 127.0.0.1 - - [17/Sep/2024 11:35:39] "POST /inference HTTP/1.1" 200 -
