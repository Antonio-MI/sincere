2024-09-18 12:17:03,771 Using device: cpu
2024-09-18 12:17:03,771 Scheduling mode set as batchedFCFS
2024-09-18 12:17:03,795 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 12:17:03,795 [33mPress CTRL+C to quit[0m
2024-09-18 12:17:08,839 Request with ID 023e403d for model gpt2medium-355m received
2024-09-18 12:17:08,839 127.0.0.1 - - [18/Sep/2024 12:17:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:08,973 Request with ID 2db69c5b for model distilgpt2-124m received
2024-09-18 12:17:08,973 127.0.0.1 - - [18/Sep/2024 12:17:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:08,991 Request with ID bf83b727 for model gpt2medium-355m received
2024-09-18 12:17:08,991 127.0.0.1 - - [18/Sep/2024 12:17:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,196 Request with ID 9485af41 for model distilgpt2-124m received
2024-09-18 12:17:09,197 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,203 Request with ID f8bc09f9 for model distilgpt2-124m received
2024-09-18 12:17:09,204 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,228 Request with ID 4c1307bf for model gpt2-124m received
2024-09-18 12:17:09,228 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,328 Request with ID a23cc7bd for model gpt2-124m received
2024-09-18 12:17:09,329 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,387 Request with ID 620247dd for model gpt2medium-355m received
2024-09-18 12:17:09,387 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,440 Request with ID d1a2d60e for model gpt2medium-355m received
2024-09-18 12:17:09,441 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,505 Request with ID 429c661b for model gpt2medium-355m received
2024-09-18 12:17:09,506 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:09,914 Request with ID 7d1a2b95 for model gpt2medium-355m received
2024-09-18 12:17:09,914 127.0.0.1 - - [18/Sep/2024 12:17:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,113 Request with ID 1b5ebf88 for model gpt2-124m received
2024-09-18 12:17:10,114 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,175 Request with ID 3835a2c2 for model gpt2-124m received
2024-09-18 12:17:10,176 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,514 Request with ID f97a456a for model distilgpt2-124m received
2024-09-18 12:17:10,515 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,553 Request with ID 1096b0ac for model gpt2medium-355m received
2024-09-18 12:17:10,555 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,671 Request with ID 98d4369a for model gpt2medium-355m received
2024-09-18 12:17:10,671 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:10,671 Next: call load_model for gpt2medium-355m
2024-09-18 12:17:10,693 Request with ID 047a6d55 for model distilgpt2-124m received
2024-09-18 12:17:10,693 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,800 Request with ID 01d673bb for model distilgpt2-124m received
2024-09-18 12:17:10,800 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,830 Request with ID b243013c for model gpt2medium-355m received
2024-09-18 12:17:10,830 127.0.0.1 - - [18/Sep/2024 12:17:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:10,832 Loaded model gpt2medium-355m
2024-09-18 12:17:10,832 Batch processing started for model gpt2medium-355m
2024-09-18 12:17:11,010 Request with ID f81f810f for model distilgpt2-124m received
2024-09-18 12:17:11,010 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,061 Request with ID fc64dd84 for model distilgpt2-124m received
2024-09-18 12:17:11,061 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:11,193 Request with ID 1c5eaf16 for model distilgpt2-124m received
2024-09-18 12:17:11,193 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,217 Request with ID 3dd34d72 for model distilgpt2-124m received
2024-09-18 12:17:11,219 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,257 Request with ID 4c02d1ba for model gpt2medium-355m received
2024-09-18 12:17:11,257 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,435 Request with ID 4db7989c for model gpt2-124m received
2024-09-18 12:17:11,435 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,451 Request with ID d9fb0362 for model gpt2-124m received
2024-09-18 12:17:11,451 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,460 Request with ID dd8582e7 for model gpt2-124m received
2024-09-18 12:17:11,460 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,484 Request with ID f671d6a8 for model gpt2medium-355m received
2024-09-18 12:17:11,484 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,654 Request with ID 0b492b98 for model gpt2medium-355m received
2024-09-18 12:17:11,654 127.0.0.1 - - [18/Sep/2024 12:17:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:11,812 Request with ID 0fe2156b for model gpt2-124m received
2024-09-18 12:17:11,812 Batch size condition met for model gpt2-124m
2024-09-18 12:17:12,074 Request with ID f9d81cb8 for model gpt2medium-355m received
2024-09-18 12:17:12,074 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,265 Request with ID bd715ff2 for model distilgpt2-124m received
2024-09-18 12:17:12,265 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,276 Request with ID 3a3dd2a1 for model gpt2medium-355m received
2024-09-18 12:17:12,276 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,388 Request with ID 95874bf2 for model gpt2medium-355m received
2024-09-18 12:17:12,388 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,478 Request with ID ebd8e85f for model gpt2-124m received
2024-09-18 12:17:12,478 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,528 Request with ID 473f977e for model gpt2medium-355m received
2024-09-18 12:17:12,528 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:12,672 Request with ID b4d0156d for model gpt2medium-355m received
2024-09-18 12:17:12,672 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,703 Request with ID 2c17b8a5 for model gpt2medium-355m received
2024-09-18 12:17:12,703 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,760 Request with ID b458a66f for model gpt2medium-355m received
2024-09-18 12:17:12,760 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,763 Request with ID a0978a18 for model gpt2-124m received
2024-09-18 12:17:12,763 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,836 Request with ID 6b7f60a9 for model gpt2-124m received
2024-09-18 12:17:12,836 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,866 Request with ID 5185e7a3 for model gpt2medium-355m received
2024-09-18 12:17:12,866 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,889 Request with ID a628450f for model gpt2medium-355m received
2024-09-18 12:17:12,889 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:12,925 Request with ID d7e11418 for model distilgpt2-124m received
2024-09-18 12:17:12,925 127.0.0.1 - - [18/Sep/2024 12:17:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,024 Request with ID e0840c13 for model gpt2-124m received
2024-09-18 12:17:13,024 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,036 Request with ID 87daa6dc for model distilgpt2-124m received
2024-09-18 12:17:13,037 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,253 Request with ID b55a0eaf for model distilgpt2-124m received
2024-09-18 12:17:13,253 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,267 Request with ID ae29f6da for model gpt2medium-355m received
2024-09-18 12:17:13,267 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,507 Request with ID fa724bc7 for model gpt2-124m received
2024-09-18 12:17:13,507 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,612 Request with ID 273e1b5f for model gpt2-124m received
2024-09-18 12:17:13,612 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,649 Request with ID 410f3edc for model gpt2medium-355m received
2024-09-18 12:17:13,650 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,705 Request with ID 684b0056 for model gpt2medium-355m received
2024-09-18 12:17:13,705 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:13,725 Request with ID ce5c581a for model gpt2-124m received
2024-09-18 12:17:13,725 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,779 Request with ID 83a63f74 for model gpt2-124m received
2024-09-18 12:17:13,779 Batch size condition met for model gpt2-124m
2024-09-18 12:17:13,809 Request with ID f302af1a for model distilgpt2-124m received
2024-09-18 12:17:13,809 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:13,930 Request with ID c5f7edd3 for model gpt2-124m received
2024-09-18 12:17:13,930 127.0.0.1 - - [18/Sep/2024 12:17:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:14,540 Request with ID 02934228 for model gpt2-124m received
2024-09-18 12:17:14,541 127.0.0.1 - - [18/Sep/2024 12:17:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:14,917 Request with ID 1a890db9 for model gpt2medium-355m received
2024-09-18 12:17:14,917 127.0.0.1 - - [18/Sep/2024 12:17:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:14,951 Request with ID 97288f6e for model distilgpt2-124m received
2024-09-18 12:17:14,951 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:15,028 Request with ID 82957c3b for model distilgpt2-124m received
2024-09-18 12:17:15,028 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,057 Request with ID b1dbc655 for model gpt2medium-355m received
2024-09-18 12:17:15,057 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,201 Request with ID c5a34122 for model gpt2-124m received
2024-09-18 12:17:15,202 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,333 Request with ID 4f10d22c for model distilgpt2-124m received
2024-09-18 12:17:15,333 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,412 Request with ID fd101aae for model gpt2-124m received
2024-09-18 12:17:15,412 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,451 Request with ID 820a414c for model gpt2medium-355m received
2024-09-18 12:17:15,451 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,480 Request with ID 8310313c for model gpt2medium-355m received
2024-09-18 12:17:15,480 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,646 Request with ID 4ddc0348 for model gpt2-124m received
2024-09-18 12:17:15,646 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:15,765 Request with ID 32b536da for model gpt2medium-355m received
2024-09-18 12:17:15,765 127.0.0.1 - - [18/Sep/2024 12:17:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,043 Request with ID 1b6df8d4 for model gpt2medium-355m received
2024-09-18 12:17:16,043 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,086 Request with ID 72eaadad for model gpt2medium-355m received
2024-09-18 12:17:16,087 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,166 Request with ID 161cdcab for model gpt2medium-355m received
2024-09-18 12:17:16,166 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:16,274 Request with ID b5a4faf9 for model gpt2-124m received
2024-09-18 12:17:16,275 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,349 Request with ID d6a416dc for model distilgpt2-124m received
2024-09-18 12:17:16,349 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,833 Processed batch: ['023e403d', 'bf83b727', '620247dd', 'd1a2d60e', '429c661b', '7d1a2b95', '1096b0ac', '98d4369a'] with model gpt2medium-355m in 6.0004 seconds
2024-09-18 12:17:16,833 Latency for request 023e403d with model gpt2medium-355m: 7.9934 seconds
2024-09-18 12:17:16,833 Saving results without gpu monitoring
2024-09-18 12:17:16,836 Latency for request bf83b727 with model gpt2medium-355m: 7.8418 seconds
2024-09-18 12:17:16,836 Saving results without gpu monitoring
2024-09-18 12:17:16,837 Latency for request 620247dd with model gpt2medium-355m: 7.4460 seconds
2024-09-18 12:17:16,837 Saving results without gpu monitoring
2024-09-18 12:17:16,838 Latency for request d1a2d60e with model gpt2medium-355m: 7.3921 seconds
2024-09-18 12:17:16,838 Saving results without gpu monitoring
2024-09-18 12:17:16,838 Latency for request 429c661b with model gpt2medium-355m: 7.3278 seconds
2024-09-18 12:17:16,838 Saving results without gpu monitoring
2024-09-18 12:17:16,838 Latency for request 7d1a2b95 with model gpt2medium-355m: 6.9187 seconds
2024-09-18 12:17:16,838 Saving results without gpu monitoring
2024-09-18 12:17:16,839 Latency for request 1096b0ac with model gpt2medium-355m: 6.2792 seconds
2024-09-18 12:17:16,839 Saving results without gpu monitoring
2024-09-18 12:17:16,839 Latency for request 98d4369a with model gpt2medium-355m: 6.1619 seconds
2024-09-18 12:17:16,839 Saving results without gpu monitoring
2024-09-18 12:17:16,839 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,839 Next: call load_model for distilgpt2-124m
2024-09-18 12:17:16,853 Unloaded previous model
2024-09-18 12:17:16,909 Request with ID c4dc82cc for model gpt2-124m received
2024-09-18 12:17:16,909 127.0.0.1 - - [18/Sep/2024 12:17:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:16,911 Loaded model distilgpt2-124m
2024-09-18 12:17:16,911 Batch processing started for model distilgpt2-124m
2024-09-18 12:17:17,124 Request with ID b2b531da for model gpt2-124m received
2024-09-18 12:17:17,124 Batch size condition met for model gpt2-124m
2024-09-18 12:17:17,168 Request with ID 4ab3eafa for model distilgpt2-124m received
2024-09-18 12:17:17,168 127.0.0.1 - - [18/Sep/2024 12:17:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:17,321 Request with ID 636a3458 for model gpt2medium-355m received
2024-09-18 12:17:17,321 127.0.0.1 - - [18/Sep/2024 12:17:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:17,597 Request with ID 5e695a05 for model distilgpt2-124m received
2024-09-18 12:17:17,597 127.0.0.1 - - [18/Sep/2024 12:17:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:17,709 Request with ID 06e80326 for model gpt2medium-355m received
2024-09-18 12:17:17,709 127.0.0.1 - - [18/Sep/2024 12:17:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:17,757 Request with ID 7169bffb for model gpt2medium-355m received
2024-09-18 12:17:17,757 127.0.0.1 - - [18/Sep/2024 12:17:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,122 Request with ID 7faba4cf for model distilgpt2-124m received
2024-09-18 12:17:18,122 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,202 Request with ID afa17d77 for model gpt2-124m received
2024-09-18 12:17:18,202 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,223 Request with ID 412278e7 for model gpt2-124m received
2024-09-18 12:17:18,223 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,292 Request with ID d506d03e for model gpt2medium-355m received
2024-09-18 12:17:18,293 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,421 Request with ID 7d283c3e for model gpt2-124m received
2024-09-18 12:17:18,421 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,501 Request with ID b2987efd for model gpt2medium-355m received
2024-09-18 12:17:18,501 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,528 Processed batch: ['1c5eaf16', '3dd34d72', 'bd715ff2', 'd7e11418', '87daa6dc', 'b55a0eaf', 'f302af1a', '97288f6e'] with model distilgpt2-124m in 1.6161 seconds
2024-09-18 12:17:18,528 Latency for request 1c5eaf16 with model distilgpt2-124m: 7.3349 seconds
2024-09-18 12:17:18,528 Saving results without gpu monitoring
2024-09-18 12:17:18,529 Latency for request 3dd34d72 with model distilgpt2-124m: 7.3107 seconds
2024-09-18 12:17:18,529 Saving results without gpu monitoring
2024-09-18 12:17:18,529 Latency for request bd715ff2 with model distilgpt2-124m: 6.2625 seconds
2024-09-18 12:17:18,529 Saving results without gpu monitoring
2024-09-18 12:17:18,529 Latency for request d7e11418 with model distilgpt2-124m: 5.6026 seconds
2024-09-18 12:17:18,529 Saving results without gpu monitoring
2024-09-18 12:17:18,530 Latency for request 87daa6dc with model distilgpt2-124m: 5.4911 seconds
2024-09-18 12:17:18,530 Saving results without gpu monitoring
2024-09-18 12:17:18,530 Latency for request b55a0eaf with model distilgpt2-124m: 5.2749 seconds
2024-09-18 12:17:18,530 Saving results without gpu monitoring
2024-09-18 12:17:18,530 Latency for request f302af1a with model distilgpt2-124m: 4.7188 seconds
2024-09-18 12:17:18,530 Saving results without gpu monitoring
2024-09-18 12:17:18,530 Latency for request 97288f6e with model distilgpt2-124m: 3.5767 seconds
2024-09-18 12:17:18,530 Saving results without gpu monitoring
2024-09-18 12:17:18,531 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,531 Next: call load_model for gpt2-124m
2024-09-18 12:17:18,538 Unloaded previous model
2024-09-18 12:17:18,602 Loaded model gpt2-124m
2024-09-18 12:17:18,602 Batch processing started for model gpt2-124m
2024-09-18 12:17:18,603 Request with ID 4910feb3 for model distilgpt2-124m received
2024-09-18 12:17:18,603 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,694 Request with ID 2268db17 for model distilgpt2-124m received
2024-09-18 12:17:18,694 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:18,931 Request with ID e3288b70 for model distilgpt2-124m received
2024-09-18 12:17:18,931 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,977 Request with ID b1443af4 for model gpt2-124m received
2024-09-18 12:17:18,977 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:18,999 Request with ID cd3531b2 for model gpt2medium-355m received
2024-09-18 12:17:18,999 127.0.0.1 - - [18/Sep/2024 12:17:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,085 Request with ID 1d476120 for model gpt2-124m received
2024-09-18 12:17:19,085 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,090 Request with ID a47d0c69 for model distilgpt2-124m received
2024-09-18 12:17:19,090 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,399 Request with ID 93d98488 for model gpt2-124m received
2024-09-18 12:17:19,399 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,708 Request with ID 237fb393 for model gpt2-124m received
2024-09-18 12:17:19,708 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,793 Request with ID edc57ece for model gpt2-124m received
2024-09-18 12:17:19,794 Batch size condition met for model gpt2-124m
2024-09-18 12:17:19,802 Request with ID 774a0023 for model gpt2medium-355m received
2024-09-18 12:17:19,802 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,835 Request with ID 881af55d for model distilgpt2-124m received
2024-09-18 12:17:19,835 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:19,918 Request with ID 0d2c5d96 for model distilgpt2-124m received
2024-09-18 12:17:19,918 127.0.0.1 - - [18/Sep/2024 12:17:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,039 Request with ID 665e8e15 for model distilgpt2-124m received
2024-09-18 12:17:20,039 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,049 Request with ID 9250d3bd for model distilgpt2-124m received
2024-09-18 12:17:20,049 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,080 Request with ID 54b0427f for model distilgpt2-124m received
2024-09-18 12:17:20,080 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,216 Request with ID 72bbabd6 for model gpt2medium-355m received
2024-09-18 12:17:20,216 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:20,253 Request with ID 86b34663 for model gpt2medium-355m received
2024-09-18 12:17:20,253 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,283 Request with ID 6ad7d492 for model distilgpt2-124m received
2024-09-18 12:17:20,284 Request with ID 78bbfdef for model gpt2-124m received
2024-09-18 12:17:20,284 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:20,284 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,307 Request with ID 108f0168 for model distilgpt2-124m received
2024-09-18 12:17:20,307 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,528 Request with ID 237aa563 for model distilgpt2-124m received
2024-09-18 12:17:20,528 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,539 Request with ID ed35679d for model distilgpt2-124m received
2024-09-18 12:17:20,539 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,580 Request with ID f4f8cefe for model distilgpt2-124m received
2024-09-18 12:17:20,580 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,615 Request with ID b3953ba3 for model distilgpt2-124m received
2024-09-18 12:17:20,615 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,790 Request with ID a884917d for model gpt2medium-355m received
2024-09-18 12:17:20,790 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,805 Request with ID 4d695f97 for model gpt2-124m received
2024-09-18 12:17:20,805 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,816 Processed batch: ['c5f7edd3', '02934228', 'c5a34122', 'fd101aae', '4ddc0348', 'b5a4faf9', 'c4dc82cc', 'b2b531da'] with model gpt2-124m in 2.2137 seconds
2024-09-18 12:17:20,816 Latency for request c5f7edd3 with model gpt2-124m: 6.8861 seconds
2024-09-18 12:17:20,816 Saving results without gpu monitoring
2024-09-18 12:17:20,817 Latency for request 02934228 with model gpt2-124m: 6.2755 seconds
2024-09-18 12:17:20,817 Saving results without gpu monitoring
2024-09-18 12:17:20,817 Latency for request c5a34122 with model gpt2-124m: 5.6144 seconds
2024-09-18 12:17:20,817 Saving results without gpu monitoring
2024-09-18 12:17:20,818 Latency for request fd101aae with model gpt2-124m: 5.4043 seconds
2024-09-18 12:17:20,818 Saving results without gpu monitoring
2024-09-18 12:17:20,818 Latency for request 4ddc0348 with model gpt2-124m: 5.1696 seconds
2024-09-18 12:17:20,818 Saving results without gpu monitoring
2024-09-18 12:17:20,818 Latency for request b5a4faf9 with model gpt2-124m: 4.5415 seconds
2024-09-18 12:17:20,818 Saving results without gpu monitoring
2024-09-18 12:17:20,818 Latency for request c4dc82cc with model gpt2-124m: 3.9067 seconds
2024-09-18 12:17:20,818 Saving results without gpu monitoring
2024-09-18 12:17:20,819 Latency for request b2b531da with model gpt2-124m: 3.6919 seconds
2024-09-18 12:17:20,819 Saving results without gpu monitoring
2024-09-18 12:17:20,819 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,819 Next: call load_model for gpt2medium-355m
2024-09-18 12:17:20,826 Unloaded previous model
2024-09-18 12:17:20,909 Request with ID 4fd6b8d5 for model gpt2-124m received
2024-09-18 12:17:20,910 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,911 Request with ID fd5a8b59 for model gpt2medium-355m received
2024-09-18 12:17:20,911 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:20,947 Loaded model gpt2medium-355m
2024-09-18 12:17:20,947 Batch processing started for model gpt2medium-355m
2024-09-18 12:17:20,993 Request with ID fdb7d265 for model distilgpt2-124m received
2024-09-18 12:17:20,994 127.0.0.1 - - [18/Sep/2024 12:17:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,066 Request with ID ba534dfa for model gpt2medium-355m received
2024-09-18 12:17:21,067 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,228 Request with ID 78ba80d5 for model gpt2medium-355m received
2024-09-18 12:17:21,228 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,286 Request with ID 8377f3a3 for model distilgpt2-124m received
2024-09-18 12:17:21,286 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,288 Request with ID 77dabe04 for model distilgpt2-124m received
2024-09-18 12:17:21,289 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:21,472 Request with ID b55e7668 for model gpt2-124m received
2024-09-18 12:17:21,472 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,479 Request with ID ca77c041 for model gpt2-124m received
2024-09-18 12:17:21,479 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,537 Request with ID 66967df3 for model distilgpt2-124m received
2024-09-18 12:17:21,538 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,549 Request with ID d3ddf3bc for model gpt2-124m received
2024-09-18 12:17:21,549 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,585 Request with ID db9711cb for model gpt2medium-355m received
2024-09-18 12:17:21,585 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,692 Request with ID 006c591b for model distilgpt2-124m received
2024-09-18 12:17:21,692 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,824 Request with ID 82fc3fba for model gpt2medium-355m received
2024-09-18 12:17:21,824 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,968 Request with ID 9e4cee73 for model gpt2-124m received
2024-09-18 12:17:21,969 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:21,979 Request with ID dc1fbe23 for model distilgpt2-124m received
2024-09-18 12:17:21,979 127.0.0.1 - - [18/Sep/2024 12:17:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:22,368 Request with ID c4eba3cf for model distilgpt2-124m received
2024-09-18 12:17:22,368 127.0.0.1 - - [18/Sep/2024 12:17:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:22,514 Request with ID ad0c379f for model gpt2-124m received
2024-09-18 12:17:22,514 Batch size condition met for model gpt2-124m
2024-09-18 12:17:22,578 Request with ID 712bd04b for model gpt2medium-355m received
2024-09-18 12:17:22,578 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:22,583 Request with ID 80465bd9 for model gpt2-124m received
2024-09-18 12:17:22,584 127.0.0.1 - - [18/Sep/2024 12:17:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:22,804 Request with ID f668235d for model gpt2-124m received
2024-09-18 12:17:22,804 127.0.0.1 - - [18/Sep/2024 12:17:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:22,883 Request with ID 0c36de24 for model gpt2medium-355m received
2024-09-18 12:17:22,883 127.0.0.1 - - [18/Sep/2024 12:17:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,172 Request with ID 4eb66fef for model gpt2medium-355m received
2024-09-18 12:17:23,172 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,335 Request with ID 715ed1d7 for model distilgpt2-124m received
2024-09-18 12:17:23,336 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,422 Request with ID aed0b7b7 for model gpt2-124m received
2024-09-18 12:17:23,422 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,589 Request with ID a56f8bf6 for model gpt2medium-355m received
2024-09-18 12:17:23,589 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,763 Request with ID a3527995 for model distilgpt2-124m received
2024-09-18 12:17:23,763 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,787 Request with ID 1338096e for model gpt2-124m received
2024-09-18 12:17:23,787 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,813 Request with ID 2eac3ec3 for model gpt2medium-355m received
2024-09-18 12:17:23,813 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:23,854 Request with ID ec7269c1 for model gpt2-124m received
2024-09-18 12:17:23,854 127.0.0.1 - - [18/Sep/2024 12:17:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,041 Request with ID 6940005b for model distilgpt2-124m received
2024-09-18 12:17:24,041 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,211 Request with ID e40ed247 for model gpt2medium-355m received
2024-09-18 12:17:24,211 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,225 Request with ID b738af38 for model gpt2-124m received
2024-09-18 12:17:24,225 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,503 Request with ID b0a971ee for model gpt2-124m received
2024-09-18 12:17:24,503 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,847 Request with ID eda3d9e6 for model distilgpt2-124m received
2024-09-18 12:17:24,847 Batch size condition met for model distilgpt2-124m
2024-09-18 12:17:24,904 Request with ID 6f248e4a for model distilgpt2-124m received
2024-09-18 12:17:24,904 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:24,934 Request with ID c8a7b120 for model gpt2medium-355m received
2024-09-18 12:17:24,934 127.0.0.1 - - [18/Sep/2024 12:17:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,203 Request with ID 7f463c1c for model gpt2-124m received
2024-09-18 12:17:25,203 Batch size condition met for model gpt2-124m
2024-09-18 12:17:25,318 Request with ID a844bf29 for model distilgpt2-124m received
2024-09-18 12:17:25,318 127.0.0.1 - - [18/Sep/2024 12:17:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,552 Request with ID a306d654 for model gpt2-124m received
2024-09-18 12:17:25,552 127.0.0.1 - - [18/Sep/2024 12:17:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,592 Request with ID fa5e5cbb for model gpt2medium-355m received
2024-09-18 12:17:25,592 127.0.0.1 - - [18/Sep/2024 12:17:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,643 Request with ID cb0d27a0 for model gpt2-124m received
2024-09-18 12:17:25,643 127.0.0.1 - - [18/Sep/2024 12:17:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,880 Request with ID c5498d54 for model gpt2-124m received
2024-09-18 12:17:25,881 127.0.0.1 - - [18/Sep/2024 12:17:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:25,921 Request with ID 001fbaf2 for model gpt2medium-355m received
2024-09-18 12:17:25,921 Batch size condition met for model gpt2medium-355m
2024-09-18 12:17:26,222 Request with ID 7faac5eb for model distilgpt2-124m received
2024-09-18 12:17:26,222 127.0.0.1 - - [18/Sep/2024 12:17:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:26,325 Request with ID 0d48586a for model gpt2-124m received
2024-09-18 12:17:26,325 127.0.0.1 - - [18/Sep/2024 12:17:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:26,359 Request with ID dde156dd for model gpt2-124m received
2024-09-18 12:17:26,360 127.0.0.1 - - [18/Sep/2024 12:17:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:26,518 Request with ID 0545d8a3 for model gpt2-124m received
2024-09-18 12:17:26,518 127.0.0.1 - - [18/Sep/2024 12:17:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:26,786 Request with ID 0882341c for model gpt2medium-355m received
2024-09-18 12:17:26,787 127.0.0.1 - - [18/Sep/2024 12:17:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:27,338 Request with ID e92a5dcb for model gpt2-124m received
2024-09-18 12:17:27,338 127.0.0.1 - - [18/Sep/2024 12:17:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:27,474 Request with ID d0c0cbb9 for model gpt2medium-355m received
2024-09-18 12:17:27,474 127.0.0.1 - - [18/Sep/2024 12:17:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:27,597 Request with ID 1d7e2b1d for model gpt2-124m received
2024-09-18 12:17:27,597 Batch size condition met for model gpt2-124m
2024-09-18 12:17:27,742 Request with ID cc587468 for model distilgpt2-124m received
2024-09-18 12:17:27,742 127.0.0.1 - - [18/Sep/2024 12:17:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:27,864 Request with ID 20b7f908 for model distilgpt2-124m received
2024-09-18 12:17:27,864 127.0.0.1 - - [18/Sep/2024 12:17:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,036 Processed batch: ['636a3458', '06e80326', '7169bffb', 'd506d03e', 'b2987efd', 'cd3531b2', '774a0023', '72bbabd6'] with model gpt2medium-355m in 7.0890 seconds
2024-09-18 12:17:28,036 Latency for request 636a3458 with model gpt2medium-355m: 10.7151 seconds
2024-09-18 12:17:28,036 Saving results without gpu monitoring
2024-09-18 12:17:28,037 Latency for request 06e80326 with model gpt2medium-355m: 10.3272 seconds
2024-09-18 12:17:28,037 Saving results without gpu monitoring
2024-09-18 12:17:28,038 Latency for request 7169bffb with model gpt2medium-355m: 10.2793 seconds
2024-09-18 12:17:28,038 Saving results without gpu monitoring
2024-09-18 12:17:28,038 Latency for request d506d03e with model gpt2medium-355m: 9.7438 seconds
2024-09-18 12:17:28,038 Saving results without gpu monitoring
2024-09-18 12:17:28,038 Latency for request b2987efd with model gpt2medium-355m: 9.5355 seconds
2024-09-18 12:17:28,038 Saving results without gpu monitoring
2024-09-18 12:17:28,039 Latency for request cd3531b2 with model gpt2medium-355m: 9.0372 seconds
2024-09-18 12:17:28,039 Saving results without gpu monitoring
2024-09-18 12:17:28,039 Latency for request 774a0023 with model gpt2medium-355m: 8.2346 seconds
2024-09-18 12:17:28,039 Saving results without gpu monitoring
2024-09-18 12:17:28,039 Latency for request 72bbabd6 with model gpt2medium-355m: 7.8199 seconds
2024-09-18 12:17:28,039 Saving results without gpu monitoring
2024-09-18 12:17:28,039 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,039 Next: call load_model for gpt2medium-355m
2024-09-18 12:17:28,039 Model gpt2medium-355m already loaded
2024-09-18 12:17:28,040 Batch processing started for model gpt2medium-355m
2024-09-18 12:17:28,088 Request with ID 577eadba for model gpt2-124m received
2024-09-18 12:17:28,088 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,161 Request with ID b0b82d2f for model gpt2medium-355m received
2024-09-18 12:17:28,162 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,250 Request with ID de8a116a for model gpt2-124m received
2024-09-18 12:17:28,250 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,463 Request with ID 15786397 for model gpt2medium-355m received
2024-09-18 12:17:28,463 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,703 Request with ID 5862347d for model distilgpt2-124m received
2024-09-18 12:17:28,703 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,827 Request with ID c1316dd5 for model gpt2-124m received
2024-09-18 12:17:28,828 127.0.0.1 - - [18/Sep/2024 12:17:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:28,839 Waiting for running processes to finish
2024-09-18 12:17:29,843 Waiting for running processes to finish
2024-09-18 12:17:30,846 Waiting for running processes to finish
2024-09-18 12:17:31,848 Waiting for running processes to finish
2024-09-18 12:17:32,853 Waiting for running processes to finish
2024-09-18 12:17:33,804 Processed batch: ['0c36de24', '4eb66fef', 'a56f8bf6', '2eac3ec3', 'e40ed247', 'c8a7b120', 'fa5e5cbb', '001fbaf2'] with model gpt2medium-355m in 5.7648 seconds
2024-09-18 12:17:33,804 Latency for request 0c36de24 with model gpt2medium-355m: 10.9216 seconds
2024-09-18 12:17:33,805 Saving results without gpu monitoring
2024-09-18 12:17:33,806 Latency for request 4eb66fef with model gpt2medium-355m: 10.6322 seconds
2024-09-18 12:17:33,806 Saving results without gpu monitoring
2024-09-18 12:17:33,806 Latency for request a56f8bf6 with model gpt2medium-355m: 10.2152 seconds
2024-09-18 12:17:33,806 Saving results without gpu monitoring
2024-09-18 12:17:33,806 Latency for request 2eac3ec3 with model gpt2medium-355m: 9.9916 seconds
2024-09-18 12:17:33,806 Saving results without gpu monitoring
2024-09-18 12:17:33,807 Latency for request e40ed247 with model gpt2medium-355m: 9.5931 seconds
2024-09-18 12:17:33,807 Saving results without gpu monitoring
2024-09-18 12:17:33,807 Latency for request c8a7b120 with model gpt2medium-355m: 8.8706 seconds
2024-09-18 12:17:33,807 Saving results without gpu monitoring
2024-09-18 12:17:33,807 Latency for request fa5e5cbb with model gpt2medium-355m: 8.2124 seconds
2024-09-18 12:17:33,807 Saving results without gpu monitoring
2024-09-18 12:17:33,807 Latency for request 001fbaf2 with model gpt2medium-355m: 7.8836 seconds
2024-09-18 12:17:33,807 Saving results without gpu monitoring
2024-09-18 12:17:33,808 127.0.0.1 - - [18/Sep/2024 12:17:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:33,808 Next: call load_model for gpt2-124m
2024-09-18 12:17:33,823 Unloaded previous model
2024-09-18 12:17:33,866 Waiting for running processes to finish
2024-09-18 12:17:33,892 Loaded model gpt2-124m
2024-09-18 12:17:33,892 Batch processing started for model gpt2-124m
2024-09-18 12:17:34,869 Waiting for running processes to finish
2024-09-18 12:17:35,874 Waiting for running processes to finish
2024-09-18 12:17:36,167 Processed batch: ['a306d654', 'cb0d27a0', 'c5498d54', '0d48586a', 'dde156dd', '0545d8a3', 'e92a5dcb', '1d7e2b1d'] with model gpt2-124m in 2.2745 seconds
2024-09-18 12:17:36,167 Latency for request a306d654 with model gpt2-124m: 10.6149 seconds
2024-09-18 12:17:36,167 Saving results without gpu monitoring
2024-09-18 12:17:36,168 Latency for request cb0d27a0 with model gpt2-124m: 10.5241 seconds
2024-09-18 12:17:36,168 Saving results without gpu monitoring
2024-09-18 12:17:36,169 Latency for request c5498d54 with model gpt2-124m: 10.2865 seconds
2024-09-18 12:17:36,169 Saving results without gpu monitoring
2024-09-18 12:17:36,169 Latency for request 0d48586a with model gpt2-124m: 9.8424 seconds
2024-09-18 12:17:36,169 Saving results without gpu monitoring
2024-09-18 12:17:36,169 Latency for request dde156dd with model gpt2-124m: 9.8077 seconds
2024-09-18 12:17:36,169 Saving results without gpu monitoring
2024-09-18 12:17:36,169 Latency for request 0545d8a3 with model gpt2-124m: 9.6491 seconds
2024-09-18 12:17:36,169 Saving results without gpu monitoring
2024-09-18 12:17:36,170 Latency for request e92a5dcb with model gpt2-124m: 8.8289 seconds
2024-09-18 12:17:36,170 Saving results without gpu monitoring
2024-09-18 12:17:36,170 Latency for request 1d7e2b1d with model gpt2-124m: 8.5702 seconds
2024-09-18 12:17:36,170 Saving results without gpu monitoring
2024-09-18 12:17:36,170 127.0.0.1 - - [18/Sep/2024 12:17:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:36,170 Next: call load_model for distilgpt2-124m
2024-09-18 12:17:36,180 Unloaded previous model
2024-09-18 12:17:36,279 Loaded model distilgpt2-124m
2024-09-18 12:17:36,279 Batch processing started for model distilgpt2-124m
2024-09-18 12:17:36,877 Waiting for running processes to finish
2024-09-18 12:17:37,882 Waiting for running processes to finish
2024-09-18 12:17:37,983 Processed batch: ['66967df3', '006c591b', 'dc1fbe23', 'c4eba3cf', '715ed1d7', 'a3527995', '6940005b', 'eda3d9e6'] with model distilgpt2-124m in 1.7041 seconds
2024-09-18 12:17:37,983 Latency for request 66967df3 with model distilgpt2-124m: 16.4454 seconds
2024-09-18 12:17:37,983 Saving results without gpu monitoring
2024-09-18 12:17:37,984 Latency for request 006c591b with model distilgpt2-124m: 16.2911 seconds
2024-09-18 12:17:37,984 Saving results without gpu monitoring
2024-09-18 12:17:37,984 Latency for request dc1fbe23 with model distilgpt2-124m: 16.0043 seconds
2024-09-18 12:17:37,984 Saving results without gpu monitoring
2024-09-18 12:17:37,984 Latency for request c4eba3cf with model distilgpt2-124m: 15.6145 seconds
2024-09-18 12:17:37,984 Saving results without gpu monitoring
2024-09-18 12:17:37,985 Latency for request 715ed1d7 with model distilgpt2-124m: 14.6476 seconds
2024-09-18 12:17:37,985 Saving results without gpu monitoring
2024-09-18 12:17:37,985 Latency for request a3527995 with model distilgpt2-124m: 14.2200 seconds
2024-09-18 12:17:37,985 Saving results without gpu monitoring
2024-09-18 12:17:37,985 Latency for request 6940005b with model distilgpt2-124m: 13.9417 seconds
2024-09-18 12:17:37,985 Saving results without gpu monitoring
2024-09-18 12:17:37,985 Latency for request eda3d9e6 with model distilgpt2-124m: 13.1357 seconds
2024-09-18 12:17:37,985 Saving results without gpu monitoring
2024-09-18 12:17:37,986 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,986 No batch to process for model gpt2medium-355m
2024-09-18 12:17:37,986 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,986 No batch to process for model gpt2-124m
2024-09-18 12:17:37,986 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,986 No batch to process for model distilgpt2-124m
2024-09-18 12:17:37,986 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,986 No batch to process for model gpt2-124m
2024-09-18 12:17:37,987 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,987 No batch to process for model gpt2medium-355m
2024-09-18 12:17:37,987 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,987 No batch to process for model distilgpt2-124m
2024-09-18 12:17:37,987 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,987 No batch to process for model distilgpt2-124m
2024-09-18 12:17:37,988 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,988 No batch to process for model gpt2-124m
2024-09-18 12:17:37,988 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,988 No batch to process for model gpt2medium-355m
2024-09-18 12:17:37,988 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,988 No batch to process for model distilgpt2-124m
2024-09-18 12:17:37,988 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,988 No batch to process for model gpt2-124m
2024-09-18 12:17:37,988 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,989 No batch to process for model gpt2medium-355m
2024-09-18 12:17:37,989 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:37,989 No batch to process for model gpt2-124m
2024-09-18 12:17:37,989 127.0.0.1 - - [18/Sep/2024 12:17:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:17:38,887 Waiting for running processes to finish
2024-09-18 12:17:39,893 Waiting for running processes to finish
2024-09-18 12:17:40,898 Waiting for running processes to finish
2024-09-18 12:17:41,901 Waiting for running processes to finish
2024-09-18 12:17:42,907 Waiting for running processes to finish
2024-09-18 12:17:43,909 Waiting for running processes to finish
2024-09-18 12:17:44,914 Waiting for running processes to finish
2024-09-18 12:17:45,920 Waiting for running processes to finish
2024-09-18 12:17:46,924 Waiting for running processes to finish
2024-09-18 12:17:47,927 Waiting for running processes to finish
2024-09-18 12:17:48,932 Waiting for running processes to finish
2024-09-18 12:17:49,935 Waiting for running processes to finish
2024-09-18 12:17:50,939 Waiting for running processes to finish
2024-09-18 12:17:51,940 Waiting for running processes to finish
2024-09-18 12:17:52,941 Waiting for running processes to finish
2024-09-18 12:17:53,946 Waiting for running processes to finish
2024-09-18 12:17:54,952 Waiting for running processes to finish
2024-09-18 12:17:55,954 Waiting for running processes to finish
2024-09-18 12:17:56,958 Waiting for running processes to finish
2024-09-18 12:17:57,963 Waiting for running processes to finish
2024-09-18 12:17:58,968 Waiting for running processes to finish
2024-09-18 12:17:59,974 Waiting for running processes to finish
2024-09-18 12:18:00,976 Waiting for running processes to finish
2024-09-18 12:18:01,982 Waiting for running processes to finish
2024-09-18 12:18:02,984 Waiting for running processes to finish
2024-09-18 12:18:03,989 Waiting for running processes to finish
2024-09-18 12:18:04,994 Waiting for running processes to finish
2024-09-18 12:18:05,999 Waiting for running processes to finish
2024-09-18 12:18:07,005 Waiting for running processes to finish
2024-09-18 12:18:08,009 Waiting for running processes to finish
2024-09-18 12:18:09,015 Waiting for running processes to finish
2024-09-18 12:18:10,017 Waiting for running processes to finish
2024-09-18 12:18:11,022 Waiting for running processes to finish
2024-09-18 12:18:12,027 Waiting for running processes to finish
2024-09-18 12:18:13,033 Waiting for running processes to finish
2024-09-18 12:18:14,038 Waiting for running processes to finish
2024-09-18 12:18:15,043 Waiting for running processes to finish
2024-09-18 12:18:16,046 Waiting for running processes to finish
2024-09-18 12:18:17,051 Waiting for running processes to finish
2024-09-18 12:18:18,054 Waiting for running processes to finish
2024-09-18 12:18:19,058 Waiting for running processes to finish
2024-09-18 12:18:20,063 Waiting for running processes to finish
2024-09-18 12:18:21,067 Waiting for running processes to finish
2024-09-18 12:18:22,070 Waiting for running processes to finish
2024-09-18 12:18:23,076 Waiting for running processes to finish
2024-09-18 12:18:24,082 Waiting for running processes to finish
2024-09-18 12:18:25,086 Waiting for running processes to finish
2024-09-18 12:18:26,091 Waiting for running processes to finish
2024-09-18 12:18:27,092 Waiting for running processes to finish
2024-09-18 12:18:28,098 Waiting for running processes to finish
2024-09-18 12:18:29,100 Waiting for running processes to finish
2024-09-18 12:18:30,106 Waiting for running processes to finish
2024-09-18 12:18:31,109 Waiting for running processes to finish
2024-09-18 12:18:32,115 Waiting for running processes to finish
2024-09-18 12:18:33,120 Waiting for running processes to finish
2024-09-18 12:18:34,126 Waiting for running processes to finish
2024-09-18 12:18:35,132 Waiting for running processes to finish
2024-09-18 12:18:36,137 Waiting for running processes to finish
2024-09-18 12:18:37,142 Waiting for running processes to finish
2024-09-18 12:18:38,147 Waiting for running processes to finish
2024-09-18 12:18:39,153 Waiting for running processes to finish
2024-09-18 12:18:40,158 Waiting for running processes to finish
2024-09-18 12:18:41,164 Waiting for running processes to finish
2024-09-18 12:18:42,169 Waiting for running processes to finish
2024-09-18 12:18:43,175 Waiting for running processes to finish
2024-09-18 12:18:44,180 Waiting for running processes to finish
2024-09-18 12:18:45,186 Waiting for running processes to finish
2024-09-18 12:18:46,192 Waiting for running processes to finish
2024-09-18 12:18:47,197 Waiting for running processes to finish
2024-09-18 12:18:48,200 Waiting for running processes to finish
2024-09-18 12:18:49,206 Waiting for running processes to finish
2024-09-18 12:18:50,211 Waiting for running processes to finish
2024-09-18 12:18:51,217 Waiting for running processes to finish
2024-09-18 12:18:52,222 Waiting for running processes to finish
2024-09-18 12:18:53,228 Waiting for running processes to finish
2024-09-18 12:18:54,233 Waiting for running processes to finish
2024-09-18 12:18:55,239 Waiting for running processes to finish
2024-09-18 12:18:56,242 Waiting for running processes to finish
2024-09-18 12:18:57,243 Waiting for running processes to finish
2024-09-18 12:18:58,244 Waiting for running processes to finish
2024-09-18 12:18:59,249 Waiting for running processes to finish
2024-09-18 12:19:00,255 Waiting for running processes to finish
2024-09-18 12:19:01,256 Waiting for running processes to finish
2024-09-18 12:19:02,259 Waiting for running processes to finish
2024-09-18 12:19:03,261 Waiting for running processes to finish
2024-09-18 12:19:04,266 Waiting for running processes to finish
2024-09-18 12:19:05,272 Waiting for running processes to finish
2024-09-18 12:19:06,273 Waiting for running processes to finish
2024-09-18 12:19:07,275 Waiting for running processes to finish
2024-09-18 12:19:08,279 Waiting for running processes to finish
2024-09-18 12:19:09,285 Waiting for running processes to finish
2024-09-18 12:19:10,290 Waiting for running processes to finish
2024-09-18 12:19:11,295 Waiting for running processes to finish
2024-09-18 12:19:12,300 Waiting for running processes to finish
2024-09-18 12:19:13,305 Waiting for running processes to finish
2024-09-18 12:19:14,308 Waiting for running processes to finish
2024-09-18 12:19:15,311 Waiting for running processes to finish
2024-09-18 12:19:16,317 Waiting for running processes to finish
2024-09-18 12:19:17,322 Waiting for running processes to finish
2024-09-18 12:19:18,328 Waiting for running processes to finish
2024-09-18 12:19:19,333 Waiting for running processes to finish
2024-09-18 12:19:20,340 Waiting for running processes to finish
2024-09-18 12:19:21,345 Waiting for running processes to finish
2024-09-18 12:19:22,350 Waiting for running processes to finish
2024-09-18 12:19:23,355 Waiting for running processes to finish
2024-09-18 12:19:24,359 Waiting for running processes to finish
2024-09-18 12:19:25,364 Waiting for running processes to finish
2024-09-18 12:19:26,370 Waiting for running processes to finish
2024-09-18 12:19:27,370 Waiting for running processes to finish
2024-09-18 12:19:28,372 Waiting for running processes to finish
2024-09-18 12:19:29,376 Waiting for running processes to finish
2024-09-18 12:19:30,382 Waiting for running processes to finish
2024-09-18 12:19:31,383 Waiting for running processes to finish
2024-09-18 12:19:32,386 Waiting for running processes to finish
2024-09-18 12:19:33,391 Waiting for running processes to finish
2024-09-18 12:19:34,392 Waiting for running processes to finish
2024-09-18 12:19:35,397 Waiting for running processes to finish
2024-09-18 12:19:36,403 Waiting for running processes to finish
2024-09-18 12:19:37,408 Waiting for running processes to finish
2024-09-18 12:19:38,410 Waiting for running processes to finish
2024-09-18 12:19:39,415 Waiting for running processes to finish
2024-09-18 12:19:40,420 Waiting for running processes to finish
2024-09-18 12:19:41,425 Waiting for running processes to finish
2024-09-18 12:19:42,427 Waiting for running processes to finish
2024-09-18 12:19:43,428 Waiting for running processes to finish
2024-09-18 12:19:44,434 Waiting for running processes to finish
2024-09-18 12:19:45,437 Waiting for running processes to finish
2024-09-18 12:19:46,443 Waiting for running processes to finish
2024-09-18 12:19:47,446 Waiting for running processes to finish
2024-09-18 12:19:48,452 Waiting for running processes to finish
2024-09-18 12:19:49,458 Waiting for running processes to finish
2024-09-18 12:19:50,458 Waiting for running processes to finish
2024-09-18 12:19:51,460 Waiting for running processes to finish
2024-09-18 12:19:52,464 Waiting for running processes to finish
2024-09-18 12:19:53,470 Waiting for running processes to finish
2024-09-18 12:19:54,476 Waiting for running processes to finish
2024-09-18 12:19:55,477 Waiting for running processes to finish
2024-09-18 12:19:56,483 Waiting for running processes to finish
2024-09-18 12:19:57,488 Waiting for running processes to finish
2024-09-18 12:19:58,492 Waiting for running processes to finish
2024-09-18 12:19:59,494 Waiting for running processes to finish
2024-09-18 12:20:00,499 Waiting for running processes to finish
2024-09-18 12:20:01,505 Waiting for running processes to finish
2024-09-18 12:20:02,508 Waiting for running processes to finish
2024-09-18 12:20:03,514 Waiting for running processes to finish
2024-09-18 12:20:04,516 Waiting for running processes to finish
2024-09-18 12:20:05,522 Waiting for running processes to finish
2024-09-18 12:20:06,525 Waiting for running processes to finish
2024-09-18 12:20:07,531 Waiting for running processes to finish
2024-09-18 12:20:08,533 Waiting for running processes to finish
2024-09-18 12:20:09,538 Waiting for running processes to finish
2024-09-18 12:20:10,542 Waiting for running processes to finish
2024-09-18 12:20:11,547 Waiting for running processes to finish
2024-09-18 12:20:12,548 Waiting for running processes to finish
2024-09-18 12:20:13,550 Waiting for running processes to finish
2024-09-18 12:20:14,552 Waiting for running processes to finish
2024-09-18 12:20:15,558 Waiting for running processes to finish
2024-09-18 12:20:16,558 Waiting for running processes to finish
2024-09-18 12:20:17,564 Waiting for running processes to finish
2024-09-18 12:20:18,565 Waiting for running processes to finish
2024-09-18 12:20:19,570 Waiting for running processes to finish
2024-09-18 12:20:20,576 Waiting for running processes to finish
2024-09-18 12:20:21,581 Waiting for running processes to finish
2024-09-18 12:20:22,587 Waiting for running processes to finish
2024-09-18 12:20:23,591 Waiting for running processes to finish
2024-09-18 12:20:24,597 Waiting for running processes to finish
2024-09-18 12:20:25,600 Waiting for running processes to finish
2024-09-18 12:20:26,606 Waiting for running processes to finish
2024-09-18 12:20:27,608 Waiting for running processes to finish
2024-09-18 12:20:28,614 Waiting for running processes to finish
2024-09-18 12:20:29,619 Waiting for running processes to finish
2024-09-18 12:20:30,622 Waiting for running processes to finish
2024-09-18 12:20:31,625 Waiting for running processes to finish
2024-09-18 12:20:32,630 Waiting for running processes to finish
2024-09-18 12:20:33,634 Waiting for running processes to finish
2024-09-18 12:20:34,639 Waiting for running processes to finish
