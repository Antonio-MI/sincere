2024-09-18 12:36:00,877 Using device: cuda
2024-09-18 12:36:00,877 Scheduling mode set as FCFS
2024-09-18 12:36:00,877 Monitoring status set to True
2024-09-18 12:36:15,959 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.122.143:5000
2024-09-18 12:36:15,959 [33mPress CTRL+C to quit[0m
2024-09-18 12:37:13,219 Request with ID a05ae04d for model llama3-8b received
2024-09-18 12:37:13,220 Batch size condition met for model llama3-8b
2024-09-18 12:37:13,220 Next: call load_model for llama3-8b
2024-09-18 12:37:14,548 Request with ID bb4dd211 for model llama3-8b received
2024-09-18 12:37:14,549 Batch size condition met for model llama3-8b
2024-09-18 12:37:16,807 Request with ID f4acc31c for model granite-7b received
2024-09-18 12:37:16,807 Batch size condition met for model granite-7b
2024-09-18 12:37:19,136 Request with ID 0435f734 for model gemma-7b received
2024-09-18 12:37:19,136 Batch size condition met for model gemma-7b
2024-09-18 12:37:20,572 Request with ID cecb4658 for model granite-7b received
2024-09-18 12:37:20,572 Batch size condition met for model granite-7b
2024-09-18 12:37:22,755 Request with ID 055c2985 for model llama3-8b received
2024-09-18 12:37:22,755 Batch size condition met for model llama3-8b
2024-09-18 12:37:25,090 Request with ID 2a3927e8 for model llama3-8b received
2024-09-18 12:37:25,090 Batch size condition met for model llama3-8b
2024-09-18 12:37:25,945 Loaded model llama3-8b
2024-09-18 12:37:25,948 Batch processing started for model llama3-8b
2024-09-18 12:37:27,219 Request with ID b813d10c for model granite-7b received
2024-09-18 12:37:27,219 Batch size condition met for model granite-7b
2024-09-18 12:37:27,919 Request with ID e824251a for model granite-7b received
2024-09-18 12:37:27,919 Batch size condition met for model granite-7b
2024-09-18 12:37:28,579 Request with ID eaeb1b8d for model gemma-7b received
2024-09-18 12:37:28,579 Batch size condition met for model gemma-7b
2024-09-18 12:37:28,610 Processed batch: ['a05ae04d'] with model llama3-8b in 2.6622 seconds
2024-09-18 12:37:28,611 Saving sys info
2024-09-18 12:37:28,650 Latency for request a05ae04d with model llama3-8b: 15.3912 seconds
2024-09-18 12:37:28,650 Saving results with gpu monitoring
2024-09-18 12:37:28,656 127.0.0.1 - - [18/Sep/2024 12:37:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:37:28,656 Next: call load_model for llama3-8b
2024-09-18 12:37:28,656 Model llama3-8b already loaded
2024-09-18 12:37:28,662 Batch processing started for model llama3-8b
2024-09-18 12:37:29,395 Request with ID d98decd9 for model llama3-8b received
2024-09-18 12:37:29,396 Batch size condition met for model llama3-8b
2024-09-18 12:37:30,572 Processed batch: ['2a3927e8'] with model llama3-8b in 1.9100 seconds
2024-09-18 12:37:30,572 Saving sys info
2024-09-18 12:37:30,603 Latency for request 2a3927e8 with model llama3-8b: 5.4821 seconds
2024-09-18 12:37:30,603 Saving results with gpu monitoring
2024-09-18 12:37:30,608 127.0.0.1 - - [18/Sep/2024 12:37:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:37:30,609 Next: call load_model for granite-7b
2024-09-18 12:37:30,696 Unloaded previous model
2024-09-18 12:37:31,058 Request with ID 4ee731a3 for model gemma-7b received
2024-09-18 12:37:31,060 Batch size condition met for model gemma-7b
2024-09-18 12:37:33,336 Request with ID 57812105 for model gemma-7b received
2024-09-18 12:37:33,336 Batch size condition met for model gemma-7b
2024-09-18 12:37:33,562 Request with ID fbf9673b for model llama3-8b received
2024-09-18 12:37:33,562 Batch size condition met for model llama3-8b
2024-09-18 12:37:34,189 Request with ID f4e63daa for model gemma-7b received
2024-09-18 12:37:34,189 Batch size condition met for model gemma-7b
2024-09-18 12:37:35,562 Request with ID f7a2c44c for model gemma-7b received
2024-09-18 12:37:35,562 Batch size condition met for model gemma-7b
2024-09-18 12:37:36,291 Request with ID dfc71253 for model gemma-7b received
2024-09-18 12:37:36,291 Batch size condition met for model gemma-7b
2024-09-18 12:37:36,630 Request with ID f1264146 for model gemma-7b received
2024-09-18 12:37:36,630 Batch size condition met for model gemma-7b
2024-09-18 12:37:36,821 Request with ID 4c24f763 for model granite-7b received
2024-09-18 12:37:36,821 Batch size condition met for model granite-7b
2024-09-18 12:37:38,249 Request with ID ae58328f for model granite-7b received
2024-09-18 12:37:38,249 Batch size condition met for model granite-7b
2024-09-18 12:37:39,488 Request with ID 64e831ee for model granite-7b received
2024-09-18 12:37:39,488 Batch size condition met for model granite-7b
2024-09-18 12:37:41,038 Request with ID 33d9c6fa for model llama3-8b received
2024-09-18 12:37:41,039 Batch size condition met for model llama3-8b
2024-09-18 12:37:41,170 Loaded model granite-7b
2024-09-18 12:37:41,173 Batch processing started for model granite-7b
2024-09-18 12:37:41,770 Request with ID f06dfa28 for model llama3-8b received
2024-09-18 12:37:41,771 Batch size condition met for model llama3-8b
2024-09-18 12:37:42,776 Processed batch: ['e824251a'] with model granite-7b in 1.6031 seconds
2024-09-18 12:37:42,776 Saving sys info
2024-09-18 12:37:42,807 Latency for request e824251a with model granite-7b: 14.8571 seconds
2024-09-18 12:37:42,807 Saving results with gpu monitoring
2024-09-18 12:37:42,810 127.0.0.1 - - [18/Sep/2024 12:37:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 12:37:42,810 Next: call load_model for gemma-7b
2024-09-18 12:37:42,885 Unloaded previous model
