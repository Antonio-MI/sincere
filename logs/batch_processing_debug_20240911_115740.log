2024-09-11 11:57:40,701 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-11 11:57:40,701 [33mPress CTRL+C to quit[0m
2024-09-11 11:57:47,182 Request with ID 9db63a73 for model gpt2-124m received
2024-09-11 11:57:47,183 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-11 11:57:47,183 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-11 11:57:47,183 127.0.0.1 - - [11/Sep/2024 11:57:47] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:48,010 Request with ID 2abbfec2 for model gpt2-124m received
2024-09-11 11:57:48,010 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-11 11:57:48,011 127.0.0.1 - - [11/Sep/2024 11:57:48] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:48,343 Request with ID f1347bde for model gpt2medium-355m received
2024-09-11 11:57:48,343 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-11 11:57:48,343 Adjusted time limit for model gpt2medium-355m: 11.8866 seconds
2024-09-11 11:57:48,344 127.0.0.1 - - [11/Sep/2024 11:57:48] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:48,406 Request with ID 314dc457 for model gpt2-124m received
2024-09-11 11:57:48,407 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-11 11:57:48,407 127.0.0.1 - - [11/Sep/2024 11:57:48] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:48,778 Request with ID d11e96ba for model gpt2-124m received
2024-09-11 11:57:48,779 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-11 11:57:48,779 127.0.0.1 - - [11/Sep/2024 11:57:48] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:49,440 Request with ID 5d6516e2 for model distilgpt2-124m received
2024-09-11 11:57:49,441 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:57:49,441 Adjusted time limit for model distilgpt2-124m: 14.1882 seconds
2024-09-11 11:57:49,441 127.0.0.1 - - [11/Sep/2024 11:57:49] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:50,052 Request with ID 6f7079f3 for model distilgpt2-124m received
2024-09-11 11:57:50,052 Adjusted time limit based on total queue size 7: 9.0000 seconds
2024-09-11 11:57:50,053 127.0.0.1 - - [11/Sep/2024 11:57:50] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:50,620 Request with ID 4c9ae8aa for model gpt2medium-355m received
2024-09-11 11:57:50,620 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-11 11:57:50,620 127.0.0.1 - - [11/Sep/2024 11:57:50] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:51,012 Request with ID f85d47cd for model distilgpt2-124m received
2024-09-11 11:57:51,012 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-11 11:57:51,012 127.0.0.1 - - [11/Sep/2024 11:57:51] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:51,469 Request with ID cfa908f3 for model gpt2medium-355m received
2024-09-11 11:57:51,470 Adjusted time limit based on total queue size 10: 7.5000 seconds
2024-09-11 11:57:51,470 127.0.0.1 - - [11/Sep/2024 11:57:51] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:51,508 Time limit condition met for model gpt2medium-355m
2024-09-11 11:57:51,508 Updated batch size:4
2024-09-11 11:57:51,508 Loading model gpt2medium-355m
2024-09-11 11:57:51,957 Request with ID e195c108 for model gpt2-124m received
2024-09-11 11:57:51,957 Adjusted time limit based on total queue size 8: 7.5000 seconds
2024-09-11 11:57:51,957 127.0.0.1 - - [11/Sep/2024 11:57:51] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:52,568 Request with ID f5aa7449 for model gpt2medium-355m received
2024-09-11 11:57:52,568 Adjusted time limit based on total queue size 9: 7.5000 seconds
2024-09-11 11:57:52,568 127.0.0.1 - - [11/Sep/2024 11:57:52] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:52,931 Request with ID 631f1f29 for model gpt2-124m received
2024-09-11 11:57:52,931 Adjusted time limit based on total queue size 10: 7.5000 seconds
2024-09-11 11:57:52,931 127.0.0.1 - - [11/Sep/2024 11:57:52] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:53,268 Request with ID 4f1ea66a for model distilgpt2-124m received
2024-09-11 11:57:53,268 Adjusted time limit based on total queue size 11: 7.5000 seconds
2024-09-11 11:57:53,268 127.0.0.1 - - [11/Sep/2024 11:57:53] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:54,222 Processed batch: ['f1347bde', '4c9ae8aa', 'cfa908f3', 'b579'] with model gpt2medium-355m in 2.5755 seconds
2024-09-11 11:57:54,222 Latency for request f1347bde with model gpt2medium-355m: 5.8795 seconds
2024-09-11 11:57:54,224 Latency for request 4c9ae8aa with model gpt2medium-355m: 3.6027 seconds
2024-09-11 11:57:54,224 Latency for request cfa908f3 with model gpt2medium-355m: 2.7530 seconds
2024-09-11 11:57:54,224 Latency for request b579 with model gpt2medium-355m: 2.7142 seconds
2024-09-11 11:57:54,340 Request with ID fea4ae57 for model gpt2-124m received
2024-09-11 11:57:54,340 Adjusted time limit based on total queue size 12: 7.5000 seconds
2024-09-11 11:57:54,340 127.0.0.1 - - [11/Sep/2024 11:57:54] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:55,540 Request with ID 6fb246f9 for model gpt2-124m received
2024-09-11 11:57:55,540 Adjusted time limit based on total queue size 13: 7.5000 seconds
2024-09-11 11:57:55,540 Batch size condition met for model gpt2-124m
2024-09-11 11:57:55,540 Updated batch size:8
2024-09-11 11:57:55,540 Loading model gpt2-124m
2024-09-11 11:57:56,114 Time limit condition met for model gpt2-124m
2024-09-11 11:57:56,273 Request with ID 0b7c6122 for model distilgpt2-124m received
2024-09-11 11:57:56,273 Adjusted time limit based on total queue size 6: 9.0000 seconds
2024-09-11 11:57:56,273 127.0.0.1 - - [11/Sep/2024 11:57:56] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:56,793 Processed batch: ['9db63a73', '2abbfec2', '314dc457', 'd11e96ba', 'e195c108', '631f1f29', 'fea4ae57', '6fb246f9'] with model gpt2-124m in 1.1734 seconds
2024-09-11 11:57:56,793 Latency for request 9db63a73 with model gpt2-124m: 9.6101 seconds
2024-09-11 11:57:56,794 Latency for request 2abbfec2 with model gpt2-124m: 8.7825 seconds
2024-09-11 11:57:56,794 Latency for request 314dc457 with model gpt2-124m: 8.3864 seconds
2024-09-11 11:57:56,794 Latency for request d11e96ba with model gpt2-124m: 8.0144 seconds
2024-09-11 11:57:56,795 Latency for request e195c108 with model gpt2-124m: 4.8360 seconds
2024-09-11 11:57:56,795 Latency for request 631f1f29 with model gpt2-124m: 3.8615 seconds
2024-09-11 11:57:56,795 Latency for request fea4ae57 with model gpt2-124m: 2.4526 seconds
2024-09-11 11:57:56,795 Latency for request 6fb246f9 with model gpt2-124m: 1.2529 seconds
2024-09-11 11:57:56,796 127.0.0.1 - - [11/Sep/2024 11:57:56] "POST /inference HTTP/1.1" 200 -
2024-09-11 11:57:56,796 No batch to process for model gpt2-124m
2024-09-11 11:58:02,211 Time limit condition met for model distilgpt2-124m
2024-09-11 11:58:02,212 Updated batch size:8
2024-09-11 11:58:02,212 Loading model distilgpt2-124m
2024-09-11 11:58:03,176 Processed batch: ['5d6516e2', '6f7079f3', 'f85d47cd', '4f1ea66a', '0b7c6122', '3d3c', '27ef', '3f22'] with model distilgpt2-124m in 0.8912 seconds
2024-09-11 11:58:03,176 Latency for request 5d6516e2 with model distilgpt2-124m: 13.7350 seconds
2024-09-11 11:58:03,177 Latency for request 6f7079f3 with model distilgpt2-124m: 13.1237 seconds
2024-09-11 11:58:03,177 Latency for request f85d47cd with model distilgpt2-124m: 12.1637 seconds
2024-09-11 11:58:03,177 Latency for request 4f1ea66a with model distilgpt2-124m: 9.9076 seconds
2024-09-11 11:58:03,178 Latency for request 0b7c6122 with model distilgpt2-124m: 6.9025 seconds
2024-09-11 11:58:03,178 Latency for request 3d3c with model distilgpt2-124m: 0.9639 seconds
2024-09-11 11:58:03,178 Latency for request 27ef with model distilgpt2-124m: 0.9639 seconds
2024-09-11 11:58:03,178 Latency for request 3f22 with model distilgpt2-124m: 0.9639 seconds
