2024-09-18 13:03:00,768 Using device: cpu
2024-09-18 13:03:00,768 Scheduling mode set as batchedFCFS
2024-09-18 13:03:00,793 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 13:03:00,793 [33mPress CTRL+C to quit[0m
2024-09-18 13:03:06,001 Request with ID 18235513 for model gpt2medium-355m received
2024-09-18 13:03:06,001 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,133 Request with ID 0a511610 for model distilgpt2-124m received
2024-09-18 13:03:06,134 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,152 Request with ID 458eaeb9 for model gpt2medium-355m received
2024-09-18 13:03:06,152 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,357 Request with ID 0cbd03fd for model distilgpt2-124m received
2024-09-18 13:03:06,357 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,363 Request with ID bad288b2 for model distilgpt2-124m received
2024-09-18 13:03:06,363 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,391 Request with ID 4c9846c1 for model gpt2-124m received
2024-09-18 13:03:06,392 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,489 Request with ID 91aa3d73 for model gpt2-124m received
2024-09-18 13:03:06,490 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,547 Request with ID 204d2c29 for model gpt2medium-355m received
2024-09-18 13:03:06,548 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,601 Request with ID 48cd7b0f for model gpt2medium-355m received
2024-09-18 13:03:06,601 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:06,664 Request with ID 9ab5ea37 for model gpt2medium-355m received
2024-09-18 13:03:06,665 127.0.0.1 - - [18/Sep/2024 13:03:06] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,076 Request with ID 273792c2 for model gpt2medium-355m received
2024-09-18 13:03:07,077 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,275 Request with ID ca7e17a8 for model gpt2-124m received
2024-09-18 13:03:07,275 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,337 Request with ID 5f7374b8 for model gpt2-124m received
2024-09-18 13:03:07,338 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,677 Request with ID 9f2a4396 for model distilgpt2-124m received
2024-09-18 13:03:07,677 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,714 Request with ID fc56799b for model gpt2medium-355m received
2024-09-18 13:03:07,715 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,832 Request with ID 568d7323 for model gpt2medium-355m received
2024-09-18 13:03:07,833 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,853 Request with ID af4ab8b2 for model distilgpt2-124m received
2024-09-18 13:03:07,853 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,964 Request with ID 7b86458b for model distilgpt2-124m received
2024-09-18 13:03:07,965 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:07,989 Request with ID cb0359fb for model gpt2medium-355m received
2024-09-18 13:03:07,989 127.0.0.1 - - [18/Sep/2024 13:03:07] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,176 Request with ID 7ae583bb for model distilgpt2-124m received
2024-09-18 13:03:08,177 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,229 Request with ID 6e0a96cf for model distilgpt2-124m received
2024-09-18 13:03:08,230 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,361 Request with ID 4c3418a8 for model distilgpt2-124m received
2024-09-18 13:03:08,362 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,385 Request with ID 78168a8f for model distilgpt2-124m received
2024-09-18 13:03:08,385 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,427 Request with ID 3eefc35a for model gpt2medium-355m received
2024-09-18 13:03:08,427 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,605 Request with ID 05f15fb8 for model gpt2-124m received
2024-09-18 13:03:08,606 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,622 Request with ID 368c14c3 for model gpt2-124m received
2024-09-18 13:03:08,623 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,630 Request with ID 5fc4a4de for model gpt2-124m received
2024-09-18 13:03:08,631 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,654 Request with ID c3c9b80a for model gpt2medium-355m received
2024-09-18 13:03:08,655 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,827 Request with ID 72be569f for model gpt2medium-355m received
2024-09-18 13:03:08,827 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:08,985 Request with ID ea1c9b29 for model gpt2-124m received
2024-09-18 13:03:08,986 127.0.0.1 - - [18/Sep/2024 13:03:08] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,248 Request with ID 4dfebf6f for model gpt2medium-355m received
2024-09-18 13:03:09,248 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,440 Request with ID 67d5d884 for model distilgpt2-124m received
2024-09-18 13:03:09,440 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,451 Request with ID 98ee826b for model gpt2medium-355m received
2024-09-18 13:03:09,452 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,564 Request with ID 02feb185 for model gpt2medium-355m received
2024-09-18 13:03:09,565 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,652 Request with ID 186ae977 for model gpt2-124m received
2024-09-18 13:03:09,652 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,701 Request with ID 2285d89b for model gpt2medium-355m received
2024-09-18 13:03:09,702 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:09,702 Next: call load_model for gpt2medium-355m
2024-09-18 13:03:09,847 Request with ID 9c2e408d for model gpt2medium-355m received
2024-09-18 13:03:09,847 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,850 Loaded model gpt2medium-355m
2024-09-18 13:03:09,850 Batch processing started for model gpt2medium-355m
2024-09-18 13:03:09,876 Request with ID d1d4a14e for model gpt2medium-355m received
2024-09-18 13:03:09,877 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,934 Request with ID 788e76a5 for model gpt2medium-355m received
2024-09-18 13:03:09,934 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:09,936 Request with ID d49acd18 for model gpt2-124m received
2024-09-18 13:03:09,936 127.0.0.1 - - [18/Sep/2024 13:03:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,009 Request with ID b8768225 for model gpt2-124m received
2024-09-18 13:03:10,010 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,040 Request with ID 711c3efc for model gpt2medium-355m received
2024-09-18 13:03:10,040 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,065 Request with ID dd5aef53 for model gpt2medium-355m received
2024-09-18 13:03:10,066 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,101 Request with ID a7a3adc2 for model distilgpt2-124m received
2024-09-18 13:03:10,102 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,200 Request with ID df2dc342 for model gpt2-124m received
2024-09-18 13:03:10,200 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,213 Request with ID e80c9a23 for model distilgpt2-124m received
2024-09-18 13:03:10,213 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,428 Request with ID 8ec85870 for model distilgpt2-124m received
2024-09-18 13:03:10,428 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,442 Request with ID df308d9d for model gpt2medium-355m received
2024-09-18 13:03:10,443 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,681 Request with ID 4a2cf1e4 for model gpt2-124m received
2024-09-18 13:03:10,681 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,788 Request with ID e9e15376 for model gpt2-124m received
2024-09-18 13:03:10,788 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,825 Request with ID e430ff8a for model gpt2medium-355m received
2024-09-18 13:03:10,825 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,880 Request with ID 67c3ebe4 for model gpt2medium-355m received
2024-09-18 13:03:10,880 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,900 Request with ID 5966ac91 for model gpt2-124m received
2024-09-18 13:03:10,901 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:10,953 Request with ID fdeca43b for model gpt2-124m received
2024-09-18 13:03:10,953 Batch size condition met for model gpt2-124m
2024-09-18 13:03:10,986 Request with ID bc062ad3 for model distilgpt2-124m received
2024-09-18 13:03:10,986 127.0.0.1 - - [18/Sep/2024 13:03:10] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:11,105 Request with ID bc179ffb for model gpt2-124m received
2024-09-18 13:03:11,105 127.0.0.1 - - [18/Sep/2024 13:03:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:11,716 Request with ID 3dd03f99 for model gpt2-124m received
2024-09-18 13:03:11,717 127.0.0.1 - - [18/Sep/2024 13:03:11] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,092 Request with ID 7506a3bc for model gpt2medium-355m received
2024-09-18 13:03:12,092 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,125 Request with ID ac4eafe5 for model distilgpt2-124m received
2024-09-18 13:03:12,125 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:12,202 Request with ID f991c00f for model distilgpt2-124m received
2024-09-18 13:03:12,202 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,231 Request with ID 7a022f8b for model gpt2medium-355m received
2024-09-18 13:03:12,231 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,376 Request with ID 9a1e198e for model gpt2-124m received
2024-09-18 13:03:12,376 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,508 Request with ID e5d0e9bb for model distilgpt2-124m received
2024-09-18 13:03:12,508 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,587 Request with ID 0bf1f743 for model gpt2-124m received
2024-09-18 13:03:12,588 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,626 Request with ID 45d1e980 for model gpt2medium-355m received
2024-09-18 13:03:12,626 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,653 Request with ID e56e20a4 for model gpt2medium-355m received
2024-09-18 13:03:12,653 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,819 Request with ID 362173a2 for model gpt2-124m received
2024-09-18 13:03:12,820 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:12,938 Request with ID b58a3203 for model gpt2medium-355m received
2024-09-18 13:03:12,938 127.0.0.1 - - [18/Sep/2024 13:03:12] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:13,215 Request with ID 1bb01320 for model gpt2medium-355m received
2024-09-18 13:03:13,216 127.0.0.1 - - [18/Sep/2024 13:03:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:13,258 Request with ID f265e75d for model gpt2medium-355m received
2024-09-18 13:03:13,259 127.0.0.1 - - [18/Sep/2024 13:03:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:13,339 Request with ID 59c008c6 for model gpt2medium-355m received
2024-09-18 13:03:13,340 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:13,448 Request with ID 5a577d36 for model gpt2-124m received
2024-09-18 13:03:13,448 127.0.0.1 - - [18/Sep/2024 13:03:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:13,521 Request with ID a00a2fb1 for model distilgpt2-124m received
2024-09-18 13:03:13,521 127.0.0.1 - - [18/Sep/2024 13:03:13] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,082 Request with ID d68b0af9 for model gpt2-124m received
2024-09-18 13:03:14,082 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,297 Request with ID ca6ad38b for model gpt2-124m received
2024-09-18 13:03:14,297 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,341 Request with ID 5941e476 for model distilgpt2-124m received
2024-09-18 13:03:14,341 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,494 Request with ID 1a3b45c3 for model gpt2medium-355m received
2024-09-18 13:03:14,494 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,770 Request with ID 73383b12 for model distilgpt2-124m received
2024-09-18 13:03:14,770 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,882 Request with ID ecda92f1 for model gpt2medium-355m received
2024-09-18 13:03:14,883 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:14,930 Request with ID 6dfec16e for model gpt2medium-355m received
2024-09-18 13:03:14,931 127.0.0.1 - - [18/Sep/2024 13:03:14] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,295 Request with ID 29692ce6 for model distilgpt2-124m received
2024-09-18 13:03:15,295 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,376 Request with ID 6a782a56 for model gpt2-124m received
2024-09-18 13:03:15,376 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,398 Request with ID 49e69297 for model gpt2-124m received
2024-09-18 13:03:15,398 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,466 Request with ID 3bcd8ae7 for model gpt2medium-355m received
2024-09-18 13:03:15,467 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,595 Request with ID ba66b33f for model gpt2-124m received
2024-09-18 13:03:15,596 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,675 Request with ID e722707c for model gpt2medium-355m received
2024-09-18 13:03:15,675 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,777 Request with ID afad25c5 for model distilgpt2-124m received
2024-09-18 13:03:15,777 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:15,868 Request with ID fa548f98 for model distilgpt2-124m received
2024-09-18 13:03:15,868 127.0.0.1 - - [18/Sep/2024 13:03:15] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,106 Request with ID bb4211a0 for model distilgpt2-124m received
2024-09-18 13:03:16,106 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,149 Request with ID df909083 for model gpt2-124m received
2024-09-18 13:03:16,150 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,172 Request with ID c1276e64 for model gpt2medium-355m received
2024-09-18 13:03:16,172 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,259 Request with ID b81921a0 for model gpt2-124m received
2024-09-18 13:03:16,259 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,264 Request with ID 4591d1ea for model distilgpt2-124m received
2024-09-18 13:03:16,264 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,574 Request with ID 676e25fc for model gpt2-124m received
2024-09-18 13:03:16,574 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,881 Request with ID 11780726 for model gpt2-124m received
2024-09-18 13:03:16,881 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:16,966 Request with ID a5088f5d for model gpt2-124m received
2024-09-18 13:03:16,966 Batch size condition met for model gpt2-124m
2024-09-18 13:03:16,975 Request with ID 70b912fd for model gpt2medium-355m received
2024-09-18 13:03:16,976 127.0.0.1 - - [18/Sep/2024 13:03:16] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,009 Request with ID e774eb35 for model distilgpt2-124m received
2024-09-18 13:03:17,009 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,091 Request with ID 805e9169 for model distilgpt2-124m received
2024-09-18 13:03:17,092 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,212 Request with ID 0d7c3fcd for model distilgpt2-124m received
2024-09-18 13:03:17,212 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,224 Request with ID 8d6deb68 for model distilgpt2-124m received
2024-09-18 13:03:17,224 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,254 Request with ID 462d55c7 for model distilgpt2-124m received
2024-09-18 13:03:17,255 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,391 Request with ID 24a54052 for model gpt2medium-355m received
2024-09-18 13:03:17,391 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,427 Request with ID f0580240 for model gpt2medium-355m received
2024-09-18 13:03:17,427 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,457 Request with ID 9eee7cde for model distilgpt2-124m received
2024-09-18 13:03:17,457 Request with ID f925dffd for model gpt2-124m received
2024-09-18 13:03:17,457 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:17,458 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,481 Request with ID 13ff70ff for model distilgpt2-124m received
2024-09-18 13:03:17,481 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,703 Request with ID d5dd09ea for model distilgpt2-124m received
2024-09-18 13:03:17,703 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,714 Request with ID b866b9ab for model distilgpt2-124m received
2024-09-18 13:03:17,714 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,756 Request with ID ac556e8a for model distilgpt2-124m received
2024-09-18 13:03:17,756 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,790 Request with ID 46e1596e for model distilgpt2-124m received
2024-09-18 13:03:17,791 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,964 Request with ID 156d2324 for model gpt2medium-355m received
2024-09-18 13:03:17,965 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:17,979 Request with ID 5b024c42 for model gpt2-124m received
2024-09-18 13:03:17,979 127.0.0.1 - - [18/Sep/2024 13:03:17] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,043 Request with ID 463d116d for model gpt2-124m received
2024-09-18 13:03:18,043 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,067 Request with ID 40322e64 for model gpt2medium-355m received
2024-09-18 13:03:18,067 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,168 Request with ID 6665bc80 for model distilgpt2-124m received
2024-09-18 13:03:18,168 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,243 Request with ID 4c9ad571 for model gpt2medium-355m received
2024-09-18 13:03:18,243 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,402 Request with ID 44770620 for model gpt2medium-355m received
2024-09-18 13:03:18,402 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,461 Request with ID 25b32c8c for model distilgpt2-124m received
2024-09-18 13:03:18,461 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,463 Request with ID 7e53a692 for model distilgpt2-124m received
2024-09-18 13:03:18,464 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,647 Request with ID c2c6d114 for model gpt2-124m received
2024-09-18 13:03:18,647 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,652 Request with ID afe06722 for model gpt2-124m received
2024-09-18 13:03:18,653 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,713 Request with ID a43700b1 for model distilgpt2-124m received
2024-09-18 13:03:18,713 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,724 Request with ID 29756b2e for model gpt2-124m received
2024-09-18 13:03:18,724 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,758 Request with ID 9d51865f for model gpt2medium-355m received
2024-09-18 13:03:18,758 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,867 Request with ID c7c48c70 for model distilgpt2-124m received
2024-09-18 13:03:18,867 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:18,999 Request with ID 4a2c83e9 for model gpt2medium-355m received
2024-09-18 13:03:18,999 127.0.0.1 - - [18/Sep/2024 13:03:18] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,142 Request with ID d117a9b4 for model gpt2-124m received
2024-09-18 13:03:19,142 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,152 Request with ID 6754f722 for model distilgpt2-124m received
2024-09-18 13:03:19,152 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,544 Request with ID d07ecbc4 for model distilgpt2-124m received
2024-09-18 13:03:19,544 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,689 Request with ID cdaa7514 for model gpt2-124m received
2024-09-18 13:03:19,689 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,754 Request with ID c02b81ca for model gpt2medium-355m received
2024-09-18 13:03:19,754 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:19,759 Request with ID e8abd8d2 for model gpt2-124m received
2024-09-18 13:03:19,759 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:19,978 Request with ID cab25b51 for model gpt2-124m received
2024-09-18 13:03:19,978 127.0.0.1 - - [18/Sep/2024 13:03:19] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,056 Request with ID 6406edf7 for model gpt2medium-355m received
2024-09-18 13:03:20,056 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,346 Request with ID 2ade7031 for model gpt2medium-355m received
2024-09-18 13:03:20,346 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,508 Request with ID f63976d9 for model distilgpt2-124m received
2024-09-18 13:03:20,509 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,595 Request with ID e9a88fe0 for model gpt2-124m received
2024-09-18 13:03:20,595 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,764 Request with ID fcb44987 for model gpt2medium-355m received
2024-09-18 13:03:20,764 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,936 Request with ID 7e9318ad for model distilgpt2-124m received
2024-09-18 13:03:20,936 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,962 Request with ID 0bee7869 for model gpt2-124m received
2024-09-18 13:03:20,962 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:20,987 Request with ID 389dca7d for model gpt2medium-355m received
2024-09-18 13:03:20,988 127.0.0.1 - - [18/Sep/2024 13:03:20] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:21,030 Request with ID 9b3f5a86 for model gpt2-124m received
2024-09-18 13:03:21,030 127.0.0.1 - - [18/Sep/2024 13:03:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:21,217 Request with ID 825fc372 for model distilgpt2-124m received
2024-09-18 13:03:21,217 127.0.0.1 - - [18/Sep/2024 13:03:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:21,387 Request with ID 6652e046 for model gpt2medium-355m received
2024-09-18 13:03:21,387 127.0.0.1 - - [18/Sep/2024 13:03:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:21,400 Request with ID 4698b6c8 for model gpt2-124m received
2024-09-18 13:03:21,400 127.0.0.1 - - [18/Sep/2024 13:03:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:21,678 Request with ID 1f6fb178 for model gpt2-124m received
2024-09-18 13:03:21,678 127.0.0.1 - - [18/Sep/2024 13:03:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,021 Request with ID 6e7a4825 for model distilgpt2-124m received
2024-09-18 13:03:22,022 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:22,079 Request with ID 794f67b3 for model distilgpt2-124m received
2024-09-18 13:03:22,079 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,108 Request with ID 6c893fba for model gpt2medium-355m received
2024-09-18 13:03:22,108 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,376 Request with ID 0ef88987 for model gpt2-124m received
2024-09-18 13:03:22,376 Batch size condition met for model gpt2-124m
2024-09-18 13:03:22,492 Request with ID 1f886846 for model distilgpt2-124m received
2024-09-18 13:03:22,492 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,726 Request with ID e00b9a10 for model gpt2-124m received
2024-09-18 13:03:22,726 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,767 Request with ID b00b99e9 for model gpt2medium-355m received
2024-09-18 13:03:22,767 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:22,817 Request with ID 6419f08f for model gpt2-124m received
2024-09-18 13:03:22,817 127.0.0.1 - - [18/Sep/2024 13:03:22] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,055 Request with ID 434eaf8b for model gpt2-124m received
2024-09-18 13:03:23,055 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,094 Request with ID bf556952 for model gpt2medium-355m received
2024-09-18 13:03:23,094 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,397 Request with ID 5c4419fa for model distilgpt2-124m received
2024-09-18 13:03:23,397 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,498 Request with ID a294ccc9 for model gpt2-124m received
2024-09-18 13:03:23,498 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,533 Request with ID f9ea3b30 for model gpt2-124m received
2024-09-18 13:03:23,534 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,692 Request with ID 2a4a3e5e for model gpt2-124m received
2024-09-18 13:03:23,692 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:23,961 Request with ID ed5d05fa for model gpt2medium-355m received
2024-09-18 13:03:23,961 127.0.0.1 - - [18/Sep/2024 13:03:23] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:24,512 Request with ID 6963920e for model gpt2-124m received
2024-09-18 13:03:24,512 127.0.0.1 - - [18/Sep/2024 13:03:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:24,648 Request with ID bdcd1e57 for model gpt2medium-355m received
2024-09-18 13:03:24,648 127.0.0.1 - - [18/Sep/2024 13:03:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:24,735 Processed batch: ['18235513', '458eaeb9', '204d2c29', '48cd7b0f', '9ab5ea37', '273792c2', 'fc56799b', '568d7323', 'cb0359fb', '3eefc35a', 'c3c9b80a', '72be569f', '4dfebf6f', '98ee826b', '02feb185', '2285d89b'] with model gpt2medium-355m in 14.8851 seconds
2024-09-18 13:03:24,735 Latency for request 18235513 with model gpt2medium-355m: 18.7340 seconds
2024-09-18 13:03:24,735 Saving results without gpu monitoring
2024-09-18 13:03:24,736 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:24,736 Next: call load_model for gpt2-124m
2024-09-18 13:03:24,738 127.0.0.1 - - [18/Sep/2024 13:03:24] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:24,738 Unloaded previous model
2024-09-18 13:03:24,772 Request with ID 6c1a8bcf for model gpt2-124m received
2024-09-18 13:03:24,772 127.0.0.1 - - [18/Sep/2024 13:03:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:24,808 Loaded model gpt2-124m
2024-09-18 13:03:24,808 Batch processing started for model gpt2-124m
2024-09-18 13:03:24,915 Request with ID c27bb116 for model distilgpt2-124m received
2024-09-18 13:03:24,915 127.0.0.1 - - [18/Sep/2024 13:03:24] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,038 Request with ID b3c3944e for model distilgpt2-124m received
2024-09-18 13:03:25,038 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,263 Request with ID f2007398 for model gpt2-124m received
2024-09-18 13:03:25,263 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,335 Request with ID 8d9017d5 for model gpt2medium-355m received
2024-09-18 13:03:25,335 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,423 Request with ID d52960ba for model gpt2-124m received
2024-09-18 13:03:25,423 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,636 Request with ID 235b2d38 for model gpt2medium-355m received
2024-09-18 13:03:25,636 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:25,875 Request with ID f2c33571 for model distilgpt2-124m received
2024-09-18 13:03:25,875 127.0.0.1 - - [18/Sep/2024 13:03:25] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,000 Request with ID d4dd1bc9 for model gpt2-124m received
2024-09-18 13:03:26,000 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,048 Request with ID 5b751678 for model distilgpt2-124m received
2024-09-18 13:03:26,048 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,286 Request with ID 575d7654 for model distilgpt2-124m received
2024-09-18 13:03:26,287 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,356 Request with ID da6b9c20 for model distilgpt2-124m received
2024-09-18 13:03:26,356 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,420 Request with ID 8d65a730 for model gpt2-124m received
2024-09-18 13:03:26,420 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,593 Request with ID dc87af80 for model gpt2-124m received
2024-09-18 13:03:26,593 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,709 Request with ID 7e8949c0 for model gpt2-124m received
2024-09-18 13:03:26,709 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,757 Request with ID cf8e376b for model gpt2-124m received
2024-09-18 13:03:26,757 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,785 Request with ID 44e8cd48 for model distilgpt2-124m received
2024-09-18 13:03:26,785 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:26,785 Request with ID 2cf15c67 for model gpt2medium-355m received
2024-09-18 13:03:26,785 127.0.0.1 - - [18/Sep/2024 13:03:26] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,009 Request with ID 526a5a87 for model gpt2-124m received
2024-09-18 13:03:27,009 Batch size condition met for model gpt2-124m
2024-09-18 13:03:27,040 Request with ID 975cb3c6 for model gpt2-124m received
2024-09-18 13:03:27,040 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,115 Request with ID 29135c9f for model gpt2-124m received
2024-09-18 13:03:27,115 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,235 Request with ID 77ca514b for model distilgpt2-124m received
2024-09-18 13:03:27,235 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,324 Request with ID ce87d5c9 for model gpt2-124m received
2024-09-18 13:03:27,325 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,573 Request with ID 7eee12a8 for model distilgpt2-124m received
2024-09-18 13:03:27,573 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,635 Request with ID 62bacbce for model gpt2-124m received
2024-09-18 13:03:27,635 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,669 Request with ID ff23a237 for model distilgpt2-124m received
2024-09-18 13:03:27,669 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:27,719 Request with ID ef006eff for model gpt2-124m received
2024-09-18 13:03:27,719 127.0.0.1 - - [18/Sep/2024 13:03:27] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,088 Request with ID a282fbce for model distilgpt2-124m received
2024-09-18 13:03:28,088 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,090 Request with ID de86096b for model gpt2-124m received
2024-09-18 13:03:28,091 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,145 Request with ID a2069770 for model gpt2-124m received
2024-09-18 13:03:28,145 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,389 Request with ID b31ba072 for model distilgpt2-124m received
2024-09-18 13:03:28,389 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,443 Request with ID aeb42048 for model gpt2medium-355m received
2024-09-18 13:03:28,443 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,697 Request with ID 01724312 for model distilgpt2-124m received
2024-09-18 13:03:28,697 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:28,899 Request with ID 57d991f2 for model gpt2-124m received
2024-09-18 13:03:28,899 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:28,902 Request with ID 910d4ce9 for model distilgpt2-124m received
2024-09-18 13:03:28,902 127.0.0.1 - - [18/Sep/2024 13:03:28] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,070 Request with ID fbf25fbb for model gpt2medium-355m received
2024-09-18 13:03:29,070 127.0.0.1 - - [18/Sep/2024 13:03:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,087 Request with ID b50dda13 for model distilgpt2-124m received
2024-09-18 13:03:29,087 127.0.0.1 - - [18/Sep/2024 13:03:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,137 Request with ID 4ac222f4 for model gpt2-124m received
2024-09-18 13:03:29,137 127.0.0.1 - - [18/Sep/2024 13:03:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,144 Request with ID fc0753e9 for model gpt2medium-355m received
2024-09-18 13:03:29,144 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:29,160 Request with ID c3d06faf for model gpt2medium-355m received
2024-09-18 13:03:29,161 127.0.0.1 - - [18/Sep/2024 13:03:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,362 Request with ID ea268e3d for model gpt2-124m received
2024-09-18 13:03:29,362 127.0.0.1 - - [18/Sep/2024 13:03:29] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:29,868 Processed batch: ['f925dffd', '5b024c42', '463d116d', 'c2c6d114', 'afe06722', '29756b2e', 'd117a9b4', 'cdaa7514', 'e8abd8d2', 'cab25b51', 'e9a88fe0', '0bee7869', '9b3f5a86', '4698b6c8', '1f6fb178', '0ef88987'] with model gpt2-124m in 5.0593 seconds
2024-09-18 13:03:29,868 Latency for request f925dffd with model gpt2-124m: 12.4100 seconds
2024-09-18 13:03:29,868 Saving results without gpu monitoring
2024-09-18 13:03:29,868 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:29,868 Next: call load_model for distilgpt2-124m
2024-09-18 13:03:29,868 127.0.0.1 - - [18/Sep/2024 13:03:29] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:29,869 Unloaded previous model
2024-09-18 13:03:29,927 Loaded model distilgpt2-124m
2024-09-18 13:03:29,928 Batch processing started for model distilgpt2-124m
2024-09-18 13:03:30,180 Request with ID c06e4001 for model gpt2medium-355m received
2024-09-18 13:03:30,180 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,262 Request with ID 6e0bb3db for model distilgpt2-124m received
2024-09-18 13:03:30,262 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,311 Request with ID 46b9d9cd for model gpt2-124m received
2024-09-18 13:03:30,311 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,620 Request with ID 58316665 for model distilgpt2-124m received
2024-09-18 13:03:30,620 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,833 Request with ID a8c107d4 for model distilgpt2-124m received
2024-09-18 13:03:30,833 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,878 Request with ID a70ced7c for model distilgpt2-124m received
2024-09-18 13:03:30,878 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:30,948 Request with ID 9a7b3a30 for model distilgpt2-124m received
2024-09-18 13:03:30,948 127.0.0.1 - - [18/Sep/2024 13:03:30] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,127 Request with ID 6c41f9d8 for model gpt2-124m received
2024-09-18 13:03:31,127 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,209 Request with ID 042c88e1 for model distilgpt2-124m received
2024-09-18 13:03:31,210 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,420 Request with ID d22a178b for model gpt2-124m received
2024-09-18 13:03:31,420 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,435 Request with ID 3e163dba for model gpt2-124m received
2024-09-18 13:03:31,435 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,538 Request with ID c4f10695 for model gpt2medium-355m received
2024-09-18 13:03:31,538 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,773 Request with ID 17623976 for model distilgpt2-124m received
2024-09-18 13:03:31,773 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,816 Request with ID 1235c14b for model distilgpt2-124m received
2024-09-18 13:03:31,816 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,868 Request with ID 660bb4ae for model distilgpt2-124m received
2024-09-18 13:03:31,868 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:31,912 Request with ID 442bda0f for model gpt2-124m received
2024-09-18 13:03:31,912 127.0.0.1 - - [18/Sep/2024 13:03:31] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,066 Request with ID e6ee05e2 for model distilgpt2-124m received
2024-09-18 13:03:32,066 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,165 Request with ID affb865f for model gpt2-124m received
2024-09-18 13:03:32,165 Batch size condition met for model gpt2-124m
2024-09-18 13:03:32,326 Request with ID 5b344b7d for model gpt2-124m received
2024-09-18 13:03:32,326 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,395 Request with ID 7bb743c7 for model gpt2-124m received
2024-09-18 13:03:32,395 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,787 Request with ID fd160649 for model distilgpt2-124m received
2024-09-18 13:03:32,787 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,829 Request with ID c9f63f55 for model gpt2medium-355m received
2024-09-18 13:03:32,830 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,837 Request with ID 6476db27 for model gpt2medium-355m received
2024-09-18 13:03:32,838 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,852 Request with ID 8956f525 for model gpt2medium-355m received
2024-09-18 13:03:32,852 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:32,868 Request with ID 05ac862b for model gpt2-124m received
2024-09-18 13:03:32,869 127.0.0.1 - - [18/Sep/2024 13:03:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,097 Request with ID 7fd3b064 for model gpt2-124m received
2024-09-18 13:03:33,097 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,302 Processed batch: ['794f67b3', '1f886846', '5c4419fa', 'c27bb116', 'b3c3944e', 'f2c33571', '5b751678', '575d7654', 'da6b9c20', '44e8cd48', '77ca514b', '7eee12a8', 'ff23a237', 'a282fbce', 'b31ba072', '01724312'] with model distilgpt2-124m in 3.3746 seconds
2024-09-18 13:03:33,302 Latency for request 794f67b3 with model distilgpt2-124m: 11.2230 seconds
2024-09-18 13:03:33,302 Saving results without gpu monitoring
2024-09-18 13:03:33,302 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:33,303 Next: call load_model for gpt2medium-355m
2024-09-18 13:03:33,303 127.0.0.1 - - [18/Sep/2024 13:03:33] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:33,303 Unloaded previous model
2024-09-18 13:03:33,422 Loaded model gpt2medium-355m
2024-09-18 13:03:33,422 Batch processing started for model gpt2medium-355m
2024-09-18 13:03:33,461 Request with ID 830bc046 for model distilgpt2-124m received
2024-09-18 13:03:33,461 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,487 Request with ID 0cbcaa0f for model distilgpt2-124m received
2024-09-18 13:03:33,488 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,521 Request with ID 2f6a2f29 for model distilgpt2-124m received
2024-09-18 13:03:33,522 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:33,535 Request with ID 183c50fd for model distilgpt2-124m received
2024-09-18 13:03:33,535 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,762 Request with ID d3c5bc83 for model distilgpt2-124m received
2024-09-18 13:03:33,762 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,803 Request with ID 1341fc01 for model gpt2-124m received
2024-09-18 13:03:33,803 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,921 Request with ID 148a0d92 for model distilgpt2-124m received
2024-09-18 13:03:33,921 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,939 Request with ID 98bc81e8 for model gpt2-124m received
2024-09-18 13:03:33,939 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:33,941 Request with ID b5139157 for model gpt2-124m received
2024-09-18 13:03:33,941 127.0.0.1 - - [18/Sep/2024 13:03:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,033 Request with ID 38eb0625 for model gpt2medium-355m received
2024-09-18 13:03:34,033 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,111 Request with ID 33a37e86 for model distilgpt2-124m received
2024-09-18 13:03:34,111 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,138 Request with ID f08277c5 for model gpt2medium-355m received
2024-09-18 13:03:34,139 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,216 Request with ID 2c94bd18 for model distilgpt2-124m received
2024-09-18 13:03:34,216 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,220 Request with ID b52a8529 for model gpt2medium-355m received
2024-09-18 13:03:34,220 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,348 Request with ID 274346f5 for model gpt2medium-355m received
2024-09-18 13:03:34,348 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,419 Request with ID 6391c045 for model gpt2-124m received
2024-09-18 13:03:34,419 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,625 Request with ID b0608e82 for model gpt2medium-355m received
2024-09-18 13:03:34,625 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,636 Request with ID ad010a14 for model gpt2-124m received
2024-09-18 13:03:34,636 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,657 Request with ID 2b48fc12 for model gpt2-124m received
2024-09-18 13:03:34,657 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,842 Request with ID df10f7a5 for model distilgpt2-124m received
2024-09-18 13:03:34,843 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,922 Request with ID 1b273a7a for model gpt2-124m received
2024-09-18 13:03:34,922 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:34,964 Request with ID b9d57590 for model distilgpt2-124m received
2024-09-18 13:03:34,964 127.0.0.1 - - [18/Sep/2024 13:03:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,121 Request with ID 2e147eef for model distilgpt2-124m received
2024-09-18 13:03:35,121 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,131 Request with ID 48a40ac3 for model distilgpt2-124m received
2024-09-18 13:03:35,132 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,222 Request with ID cc6c1fe7 for model distilgpt2-124m received
2024-09-18 13:03:35,222 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,316 Request with ID 2887e8cb for model gpt2medium-355m received
2024-09-18 13:03:35,316 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,748 Request with ID 99b8a0cd for model gpt2-124m received
2024-09-18 13:03:35,748 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,761 Request with ID 0c1565ec for model distilgpt2-124m received
2024-09-18 13:03:35,761 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,791 Request with ID 3189602b for model gpt2medium-355m received
2024-09-18 13:03:35,791 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:35,976 Request with ID a75a5c2d for model gpt2medium-355m received
2024-09-18 13:03:35,977 127.0.0.1 - - [18/Sep/2024 13:03:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,031 Request with ID 4528f804 for model gpt2medium-355m received
2024-09-18 13:03:36,031 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,260 Request with ID 6e92710e for model gpt2medium-355m received
2024-09-18 13:03:36,260 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:36,463 Request with ID 604249a9 for model distilgpt2-124m received
2024-09-18 13:03:36,464 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,495 Request with ID 30b6ea08 for model gpt2medium-355m received
2024-09-18 13:03:36,495 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,668 Request with ID 62602d4b for model gpt2medium-355m received
2024-09-18 13:03:36,668 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,745 Request with ID bd7991fc for model gpt2-124m received
2024-09-18 13:03:36,746 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:36,894 Request with ID 5eca7b9e for model distilgpt2-124m received
2024-09-18 13:03:36,894 127.0.0.1 - - [18/Sep/2024 13:03:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,019 Request with ID a39c8d86 for model gpt2-124m received
2024-09-18 13:03:37,019 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,154 Request with ID 72675e66 for model gpt2-124m received
2024-09-18 13:03:37,154 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,250 Request with ID 07631474 for model distilgpt2-124m received
2024-09-18 13:03:37,250 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,338 Request with ID cc1db84b for model gpt2medium-355m received
2024-09-18 13:03:37,338 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,405 Request with ID d7d2e9be for model gpt2medium-355m received
2024-09-18 13:03:37,405 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,411 Request with ID e2693856 for model distilgpt2-124m received
2024-09-18 13:03:37,411 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,493 Request with ID 1bebceb7 for model distilgpt2-124m received
2024-09-18 13:03:37,493 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:37,532 Request with ID 612400e5 for model gpt2-124m received
2024-09-18 13:03:37,533 Batch size condition met for model gpt2-124m
2024-09-18 13:03:37,548 Request with ID 73a8850e for model gpt2medium-355m received
2024-09-18 13:03:37,549 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,613 Request with ID e0bea008 for model gpt2medium-355m received
2024-09-18 13:03:37,614 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,658 Request with ID 955537b8 for model distilgpt2-124m received
2024-09-18 13:03:37,658 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:37,796 Request with ID 46687a9d for model distilgpt2-124m received
2024-09-18 13:03:37,797 127.0.0.1 - - [18/Sep/2024 13:03:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,068 Request with ID 2e3494f1 for model distilgpt2-124m received
2024-09-18 13:03:38,068 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,090 Request with ID 736f3110 for model gpt2-124m received
2024-09-18 13:03:38,090 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,226 Request with ID 825d5ab3 for model gpt2-124m received
2024-09-18 13:03:38,226 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,242 Request with ID b84e4809 for model gpt2-124m received
2024-09-18 13:03:38,242 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,266 Request with ID 1f35f79e for model gpt2-124m received
2024-09-18 13:03:38,266 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,288 Request with ID ac64eb1b for model distilgpt2-124m received
2024-09-18 13:03:38,288 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,528 Request with ID 4bb501e0 for model gpt2-124m received
2024-09-18 13:03:38,529 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,620 Request with ID 2e6add53 for model gpt2medium-355m received
2024-09-18 13:03:38,620 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,745 Request with ID 1244aa55 for model gpt2medium-355m received
2024-09-18 13:03:38,745 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,804 Request with ID b15cedf6 for model gpt2-124m received
2024-09-18 13:03:38,805 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,850 Request with ID 5de8c432 for model distilgpt2-124m received
2024-09-18 13:03:38,850 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:38,929 Request with ID b6186741 for model gpt2medium-355m received
2024-09-18 13:03:38,930 127.0.0.1 - - [18/Sep/2024 13:03:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,009 Request with ID 1f3e8015 for model distilgpt2-124m received
2024-09-18 13:03:39,009 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,050 Request with ID b4062379 for model distilgpt2-124m received
2024-09-18 13:03:39,051 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,066 Request with ID b13a68f0 for model gpt2-124m received
2024-09-18 13:03:39,067 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,137 Request with ID 03979328 for model gpt2medium-355m received
2024-09-18 13:03:39,137 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,222 Request with ID 450222f9 for model gpt2-124m received
2024-09-18 13:03:39,223 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,252 Request with ID 2d612e2f for model gpt2-124m received
2024-09-18 13:03:39,253 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,256 Request with ID 4766a9d6 for model distilgpt2-124m received
2024-09-18 13:03:39,256 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,326 Request with ID fb226dc3 for model gpt2-124m received
2024-09-18 13:03:39,327 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,338 Request with ID 3d306465 for model gpt2medium-355m received
2024-09-18 13:03:39,338 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,387 Request with ID 28ace907 for model gpt2medium-355m received
2024-09-18 13:03:39,387 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,548 Request with ID 3c0da8ed for model gpt2-124m received
2024-09-18 13:03:39,548 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,678 Request with ID 1026ea82 for model gpt2medium-355m received
2024-09-18 13:03:39,678 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,798 Request with ID d72c705c for model gpt2medium-355m received
2024-09-18 13:03:39,798 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:39,889 Request with ID 13a18852 for model gpt2medium-355m received
2024-09-18 13:03:39,890 127.0.0.1 - - [18/Sep/2024 13:03:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,210 Request with ID e9dfff72 for model gpt2medium-355m received
2024-09-18 13:03:40,211 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:40,274 Request with ID 651cdadc for model distilgpt2-124m received
2024-09-18 13:03:40,274 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,363 Request with ID e316bdfc for model gpt2-124m received
2024-09-18 13:03:40,363 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,365 Request with ID ccce47e3 for model gpt2medium-355m received
2024-09-18 13:03:40,365 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,636 Request with ID 5ebf2c26 for model gpt2-124m received
2024-09-18 13:03:40,636 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,893 Request with ID c37e4f10 for model gpt2medium-355m received
2024-09-18 13:03:40,893 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:40,911 Request with ID c7075010 for model gpt2medium-355m received
2024-09-18 13:03:40,911 127.0.0.1 - - [18/Sep/2024 13:03:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,006 Request with ID d8f32ccd for model gpt2medium-355m received
2024-09-18 13:03:41,006 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,141 Request with ID ed48b4db for model gpt2-124m received
2024-09-18 13:03:41,141 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,154 Request with ID 1832d6b8 for model distilgpt2-124m received
2024-09-18 13:03:41,154 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,337 Request with ID 9817991f for model distilgpt2-124m received
2024-09-18 13:03:41,337 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,411 Request with ID 1cd9ac92 for model gpt2medium-355m received
2024-09-18 13:03:41,411 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,464 Request with ID 37f19803 for model gpt2-124m received
2024-09-18 13:03:41,464 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,489 Request with ID 7dea6a58 for model distilgpt2-124m received
2024-09-18 13:03:41,489 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,531 Request with ID a41580d1 for model gpt2-124m received
2024-09-18 13:03:41,531 Batch size condition met for model gpt2-124m
2024-09-18 13:03:41,731 Request with ID d0397eea for model gpt2medium-355m received
2024-09-18 13:03:41,732 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,776 Request with ID e62269e9 for model gpt2medium-355m received
2024-09-18 13:03:41,776 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,837 Request with ID cc04e73b for model gpt2medium-355m received
2024-09-18 13:03:41,838 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,899 Request with ID 59832cf9 for model distilgpt2-124m received
2024-09-18 13:03:41,899 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,919 Request with ID fc81647a for model distilgpt2-124m received
2024-09-18 13:03:41,920 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:41,923 Request with ID b09b5422 for model gpt2-124m received
2024-09-18 13:03:41,923 127.0.0.1 - - [18/Sep/2024 13:03:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,076 Request with ID 1e41586e for model distilgpt2-124m received
2024-09-18 13:03:42,076 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,103 Request with ID 0b3c9ca0 for model gpt2-124m received
2024-09-18 13:03:42,103 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,123 Request with ID 2f3ab90f for model gpt2-124m received
2024-09-18 13:03:42,123 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,141 Request with ID 429620ce for model gpt2medium-355m received
2024-09-18 13:03:42,141 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,720 Request with ID 50930c02 for model gpt2-124m received
2024-09-18 13:03:42,721 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,755 Processed batch: ['6406edf7', '2ade7031', 'fcb44987', '389dca7d', '6652e046', '6c893fba', 'b00b99e9', 'bf556952', 'ed5d05fa', 'bdcd1e57', '8d9017d5', '235b2d38', '2cf15c67', 'aeb42048', 'fbf25fbb', 'fc0753e9'] with model gpt2medium-355m in 9.3331 seconds
2024-09-18 13:03:42,755 Latency for request 6406edf7 with model gpt2medium-355m: 22.6990 seconds
2024-09-18 13:03:42,755 Saving results without gpu monitoring
2024-09-18 13:03:42,755 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:42,756 Next: call load_model for gpt2-124m
2024-09-18 13:03:42,756 127.0.0.1 - - [18/Sep/2024 13:03:42] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:42,756 Unloaded previous model
2024-09-18 13:03:42,791 Request with ID a94c8a0f for model gpt2medium-355m received
2024-09-18 13:03:42,791 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,815 Request with ID 0de15264 for model gpt2medium-355m received
2024-09-18 13:03:42,815 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,840 Loaded model gpt2-124m
2024-09-18 13:03:42,840 Batch processing started for model gpt2-124m
2024-09-18 13:03:42,859 Request with ID 8bd33f0d for model gpt2-124m received
2024-09-18 13:03:42,859 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:42,924 Request with ID c6c5aae9 for model gpt2-124m received
2024-09-18 13:03:42,924 127.0.0.1 - - [18/Sep/2024 13:03:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,076 Request with ID 23ceb788 for model distilgpt2-124m received
2024-09-18 13:03:43,076 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:43,135 Request with ID cf8e1a8d for model distilgpt2-124m received
2024-09-18 13:03:43,136 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,218 Request with ID 90402fc4 for model gpt2-124m received
2024-09-18 13:03:43,218 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,260 Request with ID 2fd22646 for model gpt2medium-355m received
2024-09-18 13:03:43,261 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,401 Request with ID c61df217 for model distilgpt2-124m received
2024-09-18 13:03:43,401 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,630 Request with ID a5b77051 for model gpt2medium-355m received
2024-09-18 13:03:43,630 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,650 Request with ID 9b47616f for model gpt2medium-355m received
2024-09-18 13:03:43,650 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,758 Request with ID 7aa50fc5 for model gpt2-124m received
2024-09-18 13:03:43,758 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,785 Request with ID c4eb1517 for model gpt2medium-355m received
2024-09-18 13:03:43,785 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,899 Request with ID c7f8a8e8 for model distilgpt2-124m received
2024-09-18 13:03:43,899 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:43,940 Request with ID 5d8e8cf0 for model distilgpt2-124m received
2024-09-18 13:03:43,941 127.0.0.1 - - [18/Sep/2024 13:03:43] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,105 Request with ID 44095e05 for model distilgpt2-124m received
2024-09-18 13:03:44,105 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,199 Request with ID f789a0a4 for model gpt2-124m received
2024-09-18 13:03:44,200 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,216 Request with ID b7df0df6 for model gpt2medium-355m received
2024-09-18 13:03:44,217 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:44,257 Request with ID 0aa184af for model gpt2medium-355m received
2024-09-18 13:03:44,257 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,358 Request with ID 343869ae for model gpt2-124m received
2024-09-18 13:03:44,358 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,569 Request with ID d1078587 for model distilgpt2-124m received
2024-09-18 13:03:44,569 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,794 Request with ID 5c946f7f for model gpt2medium-355m received
2024-09-18 13:03:44,794 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,825 Request with ID f60e89d4 for model gpt2medium-355m received
2024-09-18 13:03:44,826 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,917 Request with ID a7c3fe0b for model gpt2-124m received
2024-09-18 13:03:44,917 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:44,959 Request with ID a96c130c for model distilgpt2-124m received
2024-09-18 13:03:44,959 127.0.0.1 - - [18/Sep/2024 13:03:44] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,005 Request with ID 29e4388e for model gpt2medium-355m received
2024-09-18 13:03:45,005 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,305 Request with ID 886cc81c for model gpt2-124m received
2024-09-18 13:03:45,305 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,508 Request with ID 3b06175a for model distilgpt2-124m received
2024-09-18 13:03:45,508 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,573 Request with ID 23308b5f for model gpt2-124m received
2024-09-18 13:03:45,573 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,641 Request with ID da505587 for model distilgpt2-124m received
2024-09-18 13:03:45,641 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,743 Request with ID 144c15e7 for model distilgpt2-124m received
2024-09-18 13:03:45,744 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:45,925 Request with ID ad9b1eb4 for model distilgpt2-124m received
2024-09-18 13:03:45,925 127.0.0.1 - - [18/Sep/2024 13:03:45] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,000 Request with ID 86af8064 for model gpt2medium-355m received
2024-09-18 13:03:46,000 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,174 Request with ID 40b98749 for model gpt2-124m received
2024-09-18 13:03:46,174 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,209 Request with ID 1a7cdb41 for model gpt2-124m received
2024-09-18 13:03:46,209 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,278 Request with ID d3800bec for model gpt2medium-355m received
2024-09-18 13:03:46,279 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,291 Request with ID ddbc8d96 for model gpt2-124m received
2024-09-18 13:03:46,291 Batch size condition met for model gpt2-124m
2024-09-18 13:03:46,323 Request with ID 93a90da7 for model gpt2medium-355m received
2024-09-18 13:03:46,324 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,416 Request with ID 44081865 for model gpt2-124m received
2024-09-18 13:03:46,416 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,485 Request with ID 00e0bfcd for model gpt2-124m received
2024-09-18 13:03:46,485 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,539 Processed batch: ['736f3110', '825d5ab3', 'b84e4809', '1f35f79e', '4bb501e0', 'b15cedf6', 'b13a68f0', '450222f9', '2d612e2f', 'fb226dc3', '3c0da8ed', 'e316bdfc', '5ebf2c26', 'ed48b4db', '37f19803', 'a41580d1'] with model gpt2-124m in 3.6996 seconds
2024-09-18 13:03:46,540 Latency for request 736f3110 with model gpt2-124m: 8.4500 seconds
2024-09-18 13:03:46,540 Saving results without gpu monitoring
2024-09-18 13:03:46,540 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:46,540 Next: call load_model for distilgpt2-124m
2024-09-18 13:03:46,540 127.0.0.1 - - [18/Sep/2024 13:03:46] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:46,540 Unloaded previous model
2024-09-18 13:03:46,541 Request with ID def8e495 for model distilgpt2-124m received
2024-09-18 13:03:46,542 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,597 Request with ID bca6c831 for model distilgpt2-124m received
2024-09-18 13:03:46,598 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,605 Loaded model distilgpt2-124m
2024-09-18 13:03:46,605 Batch processing started for model distilgpt2-124m
2024-09-18 13:03:46,733 Request with ID 3c9507bd for model gpt2medium-355m received
2024-09-18 13:03:46,733 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,762 Request with ID f355d5c5 for model gpt2medium-355m received
2024-09-18 13:03:46,762 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,797 Request with ID c2e4074b for model gpt2medium-355m received
2024-09-18 13:03:46,797 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,834 Request with ID 4caaa645 for model distilgpt2-124m received
2024-09-18 13:03:46,835 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:46,900 Request with ID 2a8111b6 for model distilgpt2-124m received
2024-09-18 13:03:46,900 127.0.0.1 - - [18/Sep/2024 13:03:46] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,033 Request with ID a077545d for model distilgpt2-124m received
2024-09-18 13:03:47,033 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:47,036 Request with ID 51211a0e for model gpt2medium-355m received
2024-09-18 13:03:47,036 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,225 Request with ID 1a3fe8a5 for model gpt2medium-355m received
2024-09-18 13:03:47,225 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,267 Request with ID 2713de58 for model distilgpt2-124m received
2024-09-18 13:03:47,267 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,279 Request with ID d6350ca8 for model gpt2-124m received
2024-09-18 13:03:47,279 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,349 Request with ID 20f5148c for model distilgpt2-124m received
2024-09-18 13:03:47,349 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,412 Request with ID c645c17a for model distilgpt2-124m received
2024-09-18 13:03:47,412 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,576 Request with ID 464f3423 for model gpt2medium-355m received
2024-09-18 13:03:47,576 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,608 Request with ID 4da8b51e for model gpt2medium-355m received
2024-09-18 13:03:47,608 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,858 Request with ID fb74ed0c for model distilgpt2-124m received
2024-09-18 13:03:47,858 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,977 Request with ID f2526dfa for model gpt2medium-355m received
2024-09-18 13:03:47,977 127.0.0.1 - - [18/Sep/2024 13:03:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:47,987 Request with ID a99919df for model gpt2medium-355m received
2024-09-18 13:03:47,987 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:48,121 Request with ID fa7a0496 for model gpt2-124m received
2024-09-18 13:03:48,121 127.0.0.1 - - [18/Sep/2024 13:03:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:48,239 Request with ID 0f25621a for model gpt2-124m received
2024-09-18 13:03:48,240 127.0.0.1 - - [18/Sep/2024 13:03:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:48,473 Request with ID 281a2f0e for model gpt2medium-355m received
2024-09-18 13:03:48,473 127.0.0.1 - - [18/Sep/2024 13:03:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:48,593 Request with ID b2069612 for model gpt2-124m received
2024-09-18 13:03:48,593 127.0.0.1 - - [18/Sep/2024 13:03:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:48,705 Request with ID a4d5d165 for model gpt2medium-355m received
2024-09-18 13:03:48,705 127.0.0.1 - - [18/Sep/2024 13:03:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:49,172 Processed batch: ['955537b8', '46687a9d', '2e3494f1', 'ac64eb1b', '5de8c432', '1f3e8015', 'b4062379', '4766a9d6', '651cdadc', '1832d6b8', '9817991f', '7dea6a58', '59832cf9', 'fc81647a', '1e41586e', '23ceb788'] with model distilgpt2-124m in 2.5662 seconds
2024-09-18 13:03:49,172 Latency for request 955537b8 with model distilgpt2-124m: 11.5140 seconds
2024-09-18 13:03:49,172 Saving results without gpu monitoring
2024-09-18 13:03:49,172 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:03:49,172 Next: call load_model for gpt2medium-355m
2024-09-18 13:03:49,172 127.0.0.1 - - [18/Sep/2024 13:03:49] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:03:49,172 Unloaded previous model
2024-09-18 13:03:49,337 Loaded model gpt2medium-355m
2024-09-18 13:03:49,337 Batch processing started for model gpt2medium-355m
2024-09-18 13:03:49,440 Request with ID f9191031 for model gpt2-124m received
2024-09-18 13:03:49,440 127.0.0.1 - - [18/Sep/2024 13:03:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:49,507 Request with ID 1e3dba2d for model distilgpt2-124m received
2024-09-18 13:03:49,507 127.0.0.1 - - [18/Sep/2024 13:03:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:49,524 Request with ID 65f698ae for model gpt2medium-355m received
2024-09-18 13:03:49,524 127.0.0.1 - - [18/Sep/2024 13:03:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:49,781 Request with ID 9ca1a6e1 for model gpt2medium-355m received
2024-09-18 13:03:49,781 127.0.0.1 - - [18/Sep/2024 13:03:49] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,070 Request with ID 518f2e25 for model gpt2medium-355m received
2024-09-18 13:03:50,070 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,133 Request with ID 745d962d for model distilgpt2-124m received
2024-09-18 13:03:50,133 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,237 Request with ID f590b962 for model gpt2medium-355m received
2024-09-18 13:03:50,237 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,321 Request with ID 231d528c for model gpt2medium-355m received
2024-09-18 13:03:50,321 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,441 Request with ID 4ea2375a for model gpt2medium-355m received
2024-09-18 13:03:50,442 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,448 Request with ID 852aa195 for model distilgpt2-124m received
2024-09-18 13:03:50,449 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,490 Request with ID 486bdf63 for model gpt2-124m received
2024-09-18 13:03:50,490 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,588 Request with ID fbcc6475 for model gpt2medium-355m received
2024-09-18 13:03:50,588 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,792 Request with ID 6ef448e8 for model gpt2medium-355m received
2024-09-18 13:03:50,792 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,814 Request with ID 3d3ad121 for model gpt2medium-355m received
2024-09-18 13:03:50,815 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,824 Request with ID 820155c2 for model gpt2-124m received
2024-09-18 13:03:50,824 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:50,970 Request with ID 4494509b for model distilgpt2-124m received
2024-09-18 13:03:50,970 127.0.0.1 - - [18/Sep/2024 13:03:50] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,036 Request with ID 57c75fba for model gpt2medium-355m received
2024-09-18 13:03:51,036 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,088 Request with ID e7799f89 for model gpt2medium-355m received
2024-09-18 13:03:51,088 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,144 Request with ID 60bab06b for model gpt2-124m received
2024-09-18 13:03:51,144 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,173 Request with ID 43f0dd54 for model gpt2-124m received
2024-09-18 13:03:51,173 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,219 Request with ID ea652176 for model distilgpt2-124m received
2024-09-18 13:03:51,219 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,246 Request with ID 45c6ebdf for model gpt2-124m received
2024-09-18 13:03:51,247 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,489 Request with ID 47210bce for model gpt2-124m received
2024-09-18 13:03:51,489 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,501 Request with ID 35d259d9 for model gpt2medium-355m received
2024-09-18 13:03:51,501 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,802 Request with ID 437b0c67 for model gpt2-124m received
2024-09-18 13:03:51,802 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:51,812 Request with ID 6d9fd534 for model gpt2medium-355m received
2024-09-18 13:03:51,812 127.0.0.1 - - [18/Sep/2024 13:03:51] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,025 Request with ID 8c213cb6 for model gpt2medium-355m received
2024-09-18 13:03:52,025 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:52,260 Request with ID 93077d34 for model gpt2medium-355m received
2024-09-18 13:03:52,260 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,313 Request with ID 31d8eac9 for model gpt2-124m received
2024-09-18 13:03:52,313 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,492 Request with ID 90676136 for model distilgpt2-124m received
2024-09-18 13:03:52,492 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,497 Request with ID 909e352e for model distilgpt2-124m received
2024-09-18 13:03:52,497 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,501 Request with ID 23296c1f for model distilgpt2-124m received
2024-09-18 13:03:52,501 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,665 Request with ID 3b2a541b for model gpt2medium-355m received
2024-09-18 13:03:52,665 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,690 Request with ID 9ed9371f for model gpt2medium-355m received
2024-09-18 13:03:52,690 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:52,782 Request with ID 700eeb10 for model gpt2-124m received
2024-09-18 13:03:52,782 Batch size condition met for model gpt2-124m
2024-09-18 13:03:52,815 Request with ID de531133 for model distilgpt2-124m received
2024-09-18 13:03:52,815 127.0.0.1 - - [18/Sep/2024 13:03:52] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,376 Request with ID 3bdff885 for model gpt2medium-355m received
2024-09-18 13:03:53,376 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,403 Request with ID 6ca392ee for model gpt2medium-355m received
2024-09-18 13:03:53,403 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,407 Request with ID c1956d69 for model distilgpt2-124m received
2024-09-18 13:03:53,407 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,606 Request with ID 363a452c for model gpt2-124m received
2024-09-18 13:03:53,606 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,657 Request with ID d8ceae4f for model gpt2-124m received
2024-09-18 13:03:53,657 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:53,878 Request with ID 728ed2c8 for model distilgpt2-124m received
2024-09-18 13:03:53,878 127.0.0.1 - - [18/Sep/2024 13:03:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,077 Request with ID a67cd005 for model gpt2medium-355m received
2024-09-18 13:03:54,077 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,186 Request with ID 28bb9232 for model gpt2medium-355m received
2024-09-18 13:03:54,186 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,221 Request with ID f2878a06 for model distilgpt2-124m received
2024-09-18 13:03:54,221 Batch size condition met for model distilgpt2-124m
2024-09-18 13:03:54,252 Request with ID 260edcea for model distilgpt2-124m received
2024-09-18 13:03:54,252 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,329 Request with ID d2e6116a for model gpt2-124m received
2024-09-18 13:03:54,329 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,556 Request with ID 95292cba for model gpt2-124m received
2024-09-18 13:03:54,556 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,564 Request with ID 1d8b3f40 for model distilgpt2-124m received
2024-09-18 13:03:54,564 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,655 Request with ID 60b8912d for model distilgpt2-124m received
2024-09-18 13:03:54,655 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,734 Request with ID 84471ef3 for model gpt2-124m received
2024-09-18 13:03:54,734 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:54,868 Request with ID 6dbcce39 for model gpt2medium-355m received
2024-09-18 13:03:54,868 127.0.0.1 - - [18/Sep/2024 13:03:54] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,072 Request with ID 44439efd for model gpt2-124m received
2024-09-18 13:03:55,072 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,090 Request with ID 54b81bdc for model gpt2-124m received
2024-09-18 13:03:55,091 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,274 Request with ID f2108dc4 for model distilgpt2-124m received
2024-09-18 13:03:55,274 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,286 Request with ID 6c9b2eeb for model gpt2medium-355m received
2024-09-18 13:03:55,287 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,501 Request with ID 573a15f3 for model distilgpt2-124m received
2024-09-18 13:03:55,502 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,566 Request with ID c52dc6d5 for model gpt2medium-355m received
2024-09-18 13:03:55,568 Request with ID 1ef269a8 for model gpt2medium-355m received
2024-09-18 13:03:55,568 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,569 Request with ID fc08fa75 for model gpt2-124m received
2024-09-18 13:03:55,569 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,569 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,598 Request with ID 107acb6f for model gpt2-124m received
2024-09-18 13:03:55,598 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,696 Request with ID 298003d9 for model gpt2-124m received
2024-09-18 13:03:55,696 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,796 Request with ID 60977022 for model gpt2-124m received
2024-09-18 13:03:55,796 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:55,950 Request with ID 1398439f for model gpt2medium-355m received
2024-09-18 13:03:55,950 127.0.0.1 - - [18/Sep/2024 13:03:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:56,095 Request with ID 932c0242 for model gpt2-124m received
2024-09-18 13:03:56,095 127.0.0.1 - - [18/Sep/2024 13:03:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:56,426 Request with ID c768a1e3 for model gpt2medium-355m received
2024-09-18 13:03:56,426 127.0.0.1 - - [18/Sep/2024 13:03:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:56,694 Request with ID 9680a3d8 for model distilgpt2-124m received
2024-09-18 13:03:56,694 127.0.0.1 - - [18/Sep/2024 13:03:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:56,973 Request with ID 8a61f37f for model distilgpt2-124m received
2024-09-18 13:03:56,973 127.0.0.1 - - [18/Sep/2024 13:03:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,268 Request with ID d8ef8dfd for model distilgpt2-124m received
2024-09-18 13:03:57,268 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,393 Request with ID 584ffc7d for model gpt2-124m received
2024-09-18 13:03:57,394 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,400 Request with ID 5f5ba0ca for model gpt2medium-355m received
2024-09-18 13:03:57,400 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,749 Request with ID 1a7cce91 for model gpt2medium-355m received
2024-09-18 13:03:57,749 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,840 Request with ID 583e6eba for model gpt2-124m received
2024-09-18 13:03:57,841 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,842 Request with ID 37ac377f for model gpt2medium-355m received
2024-09-18 13:03:57,842 Batch size condition met for model gpt2medium-355m
2024-09-18 13:03:57,874 Request with ID 430b445b for model distilgpt2-124m received
2024-09-18 13:03:57,874 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:57,892 Request with ID 1c48d239 for model gpt2-124m received
2024-09-18 13:03:57,893 127.0.0.1 - - [18/Sep/2024 13:03:57] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:58,182 Request with ID 6560336a for model gpt2medium-355m received
2024-09-18 13:03:58,182 127.0.0.1 - - [18/Sep/2024 13:03:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:58,343 Request with ID 28ccbe49 for model distilgpt2-124m received
2024-09-18 13:03:58,343 127.0.0.1 - - [18/Sep/2024 13:03:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:58,492 Request with ID 9d9c2d4f for model gpt2medium-355m received
2024-09-18 13:03:58,492 127.0.0.1 - - [18/Sep/2024 13:03:58] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:58,795 Request with ID 128e26c9 for model gpt2-124m received
2024-09-18 13:03:58,795 Batch size condition met for model gpt2-124m
2024-09-18 13:03:59,036 Request with ID 4fe8923a for model gpt2medium-355m received
2024-09-18 13:03:59,036 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,373 Request with ID 5f6fe93d for model gpt2-124m received
2024-09-18 13:03:59,373 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,432 Request with ID eb2c5661 for model gpt2-124m received
2024-09-18 13:03:59,432 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,472 Request with ID 020326b5 for model gpt2medium-355m received
2024-09-18 13:03:59,472 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,611 Request with ID 043ad9b9 for model gpt2-124m received
2024-09-18 13:03:59,611 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,717 Request with ID 5461f1c5 for model gpt2medium-355m received
2024-09-18 13:03:59,717 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,783 Request with ID 3fe1673a for model distilgpt2-124m received
2024-09-18 13:03:59,783 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,807 Request with ID a75bb773 for model gpt2medium-355m received
2024-09-18 13:03:59,807 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,869 Request with ID a2f17e6d for model gpt2medium-355m received
2024-09-18 13:03:59,870 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:03:59,968 Request with ID 05efc94b for model gpt2-124m received
2024-09-18 13:03:59,968 127.0.0.1 - - [18/Sep/2024 13:03:59] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,212 Request with ID f55343cb for model gpt2medium-355m received
2024-09-18 13:04:00,212 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,235 Request with ID 167a2f62 for model gpt2-124m received
2024-09-18 13:04:00,235 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,236 Request with ID 32dedc18 for model gpt2-124m received
2024-09-18 13:04:00,237 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,259 Request with ID 2764be8a for model gpt2medium-355m received
2024-09-18 13:04:00,259 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,366 Request with ID f083ccbf for model distilgpt2-124m received
2024-09-18 13:04:00,366 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,527 Request with ID e72280a3 for model gpt2-124m received
2024-09-18 13:04:00,528 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,545 Request with ID 63842053 for model distilgpt2-124m received
2024-09-18 13:04:00,545 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,595 Request with ID e97d415c for model gpt2medium-355m received
2024-09-18 13:04:00,595 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,690 Request with ID 03ab50bd for model gpt2medium-355m received
2024-09-18 13:04:00,690 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:00,810 Request with ID 9e605921 for model distilgpt2-124m received
2024-09-18 13:04:00,810 127.0.0.1 - - [18/Sep/2024 13:04:00] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,049 Request with ID b80b516b for model distilgpt2-124m received
2024-09-18 13:04:01,049 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,189 Request with ID e89ca7ef for model gpt2-124m received
2024-09-18 13:04:01,189 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,201 Request with ID d781feb9 for model gpt2medium-355m received
2024-09-18 13:04:01,201 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,271 Request with ID 7adb940b for model gpt2-124m received
2024-09-18 13:04:01,272 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,355 Request with ID e4c5d42b for model distilgpt2-124m received
2024-09-18 13:04:01,355 Batch size condition met for model distilgpt2-124m
2024-09-18 13:04:01,397 Request with ID 3117b825 for model gpt2-124m received
2024-09-18 13:04:01,397 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,406 Request with ID a958a599 for model gpt2-124m received
2024-09-18 13:04:01,407 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,582 Request with ID 1b25a9dd for model gpt2medium-355m received
2024-09-18 13:04:01,582 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,597 Request with ID 17aab7dd for model distilgpt2-124m received
2024-09-18 13:04:01,597 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,718 Request with ID 92f18f4f for model distilgpt2-124m received
2024-09-18 13:04:01,718 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:01,722 Request with ID df1b1cfa for model gpt2-124m received
2024-09-18 13:04:01,722 127.0.0.1 - - [18/Sep/2024 13:04:01] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,026 Request with ID 3e5e5df8 for model gpt2medium-355m received
2024-09-18 13:04:02,026 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,254 Request with ID dcded236 for model distilgpt2-124m received
2024-09-18 13:04:02,254 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,352 Request with ID a437b000 for model gpt2-124m received
2024-09-18 13:04:02,352 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,476 Request with ID bdcb65e8 for model distilgpt2-124m received
2024-09-18 13:04:02,476 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,573 Processed batch: ['0aa184af', '5c946f7f', 'f60e89d4', '29e4388e', '86af8064', 'd3800bec', '93a90da7', '3c9507bd', 'f355d5c5', 'c2e4074b', '51211a0e', '1a3fe8a5', '464f3423', '4da8b51e', 'f2526dfa', 'a99919df'] with model gpt2medium-355m in 13.2349 seconds
2024-09-18 13:04:02,573 Latency for request 0aa184af with model gpt2medium-355m: 18.3150 seconds
2024-09-18 13:04:02,573 Saving results without gpu monitoring
2024-09-18 13:04:02,573 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:04:02,573 Next: call load_model for distilgpt2-124m
2024-09-18 13:04:02,573 127.0.0.1 - - [18/Sep/2024 13:04:02] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:02,573 Unloaded previous model
2024-09-18 13:04:02,612 Request with ID 90a2eb58 for model distilgpt2-124m received
2024-09-18 13:04:02,612 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,647 Loaded model distilgpt2-124m
2024-09-18 13:04:02,647 Batch processing started for model distilgpt2-124m
2024-09-18 13:04:02,671 Request with ID 9ab4d243 for model gpt2medium-355m received
2024-09-18 13:04:02,671 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:02,898 Request with ID 9ae8222d for model distilgpt2-124m received
2024-09-18 13:04:02,898 127.0.0.1 - - [18/Sep/2024 13:04:02] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,069 Request with ID bf8bd1d8 for model gpt2medium-355m received
2024-09-18 13:04:03,069 Batch size condition met for model gpt2medium-355m
2024-09-18 13:04:03,180 Request with ID 7a8cc7e7 for model distilgpt2-124m received
2024-09-18 13:04:03,180 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,236 Request with ID 5861539f for model gpt2-124m received
2024-09-18 13:04:03,236 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,303 Request with ID fff3cc7d for model distilgpt2-124m received
2024-09-18 13:04:03,304 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,345 Request with ID ab34eadd for model gpt2-124m received
2024-09-18 13:04:03,345 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,365 Request with ID b47aec4f for model gpt2medium-355m received
2024-09-18 13:04:03,365 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,385 Request with ID b465abc7 for model distilgpt2-124m received
2024-09-18 13:04:03,385 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,492 Request with ID 991f264c for model gpt2-124m received
2024-09-18 13:04:03,492 Batch size condition met for model gpt2-124m
2024-09-18 13:04:03,561 Request with ID ff74b790 for model gpt2medium-355m received
2024-09-18 13:04:03,561 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,677 Request with ID ab7de215 for model distilgpt2-124m received
2024-09-18 13:04:03,677 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,679 Request with ID 047cc305 for model gpt2medium-355m received
2024-09-18 13:04:03,679 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,745 Request with ID e64946a8 for model gpt2medium-355m received
2024-09-18 13:04:03,745 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,799 Request with ID bda2b540 for model gpt2-124m received
2024-09-18 13:04:03,799 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:03,843 Request with ID 6bbfce36 for model distilgpt2-124m received
2024-09-18 13:04:03,844 127.0.0.1 - - [18/Sep/2024 13:04:03] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,284 Request with ID 0cd627f9 for model gpt2medium-355m received
2024-09-18 13:04:04,284 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,372 Request with ID 321a3cea for model gpt2-124m received
2024-09-18 13:04:04,372 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,423 Request with ID 5e3f9bdd for model distilgpt2-124m received
2024-09-18 13:04:04,423 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,533 Request with ID 6246dd0d for model gpt2medium-355m received
2024-09-18 13:04:04,533 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,566 Request with ID f5a7ad44 for model gpt2medium-355m received
2024-09-18 13:04:04,567 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,665 Request with ID 5402dc10 for model gpt2medium-355m received
2024-09-18 13:04:04,665 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,694 Request with ID 8553cea5 for model distilgpt2-124m received
2024-09-18 13:04:04,694 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:04,715 Request with ID c00b3508 for model distilgpt2-124m received
2024-09-18 13:04:04,715 127.0.0.1 - - [18/Sep/2024 13:04:04] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,072 Request with ID 7590bce1 for model gpt2-124m received
2024-09-18 13:04:05,072 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,198 Processed batch: ['260edcea', '1d8b3f40', '60b8912d', 'f2108dc4', '573a15f3', '9680a3d8', '8a61f37f', 'd8ef8dfd', '430b445b', '28ccbe49', '3fe1673a', 'f083ccbf', '63842053', '9e605921', 'b80b516b', 'e4c5d42b'] with model distilgpt2-124m in 2.5513 seconds
2024-09-18 13:04:05,198 Latency for request 260edcea with model distilgpt2-124m: 10.9460 seconds
2024-09-18 13:04:05,198 Saving results without gpu monitoring
2024-09-18 13:04:05,199 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:04:05,199 Next: call load_model for gpt2-124m
2024-09-18 13:04:05,199 127.0.0.1 - - [18/Sep/2024 13:04:05] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:05,199 Unloaded previous model
2024-09-18 13:04:05,267 Request with ID 6d8fc1d1 for model distilgpt2-124m received
2024-09-18 13:04:05,267 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,277 Request with ID c7621912 for model gpt2medium-355m received
2024-09-18 13:04:05,277 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,297 Loaded model gpt2-124m
2024-09-18 13:04:05,297 Batch processing started for model gpt2-124m
2024-09-18 13:04:05,368 Request with ID df09860f for model distilgpt2-124m received
2024-09-18 13:04:05,368 Batch size condition met for model distilgpt2-124m
2024-09-18 13:04:05,378 Request with ID dbc12544 for model distilgpt2-124m received
2024-09-18 13:04:05,379 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,575 Request with ID c749c36d for model gpt2medium-355m received
2024-09-18 13:04:05,575 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,665 Request with ID e1844974 for model gpt2-124m received
2024-09-18 13:04:05,665 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,694 Request with ID c4cb26af for model distilgpt2-124m received
2024-09-18 13:04:05,694 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,736 Request with ID 06e9091e for model distilgpt2-124m received
2024-09-18 13:04:05,736 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,848 Request with ID 06daa1d9 for model gpt2-124m received
2024-09-18 13:04:05,848 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,878 Request with ID 4ab4865e for model gpt2-124m received
2024-09-18 13:04:05,878 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:05,909 Request with ID 3ed607cc for model gpt2-124m received
2024-09-18 13:04:05,909 127.0.0.1 - - [18/Sep/2024 13:04:05] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:06,002 Waiting for running processes to finish
2024-09-18 13:04:07,007 Waiting for running processes to finish
2024-09-18 13:04:08,013 Waiting for running processes to finish
2024-09-18 13:04:09,018 Waiting for running processes to finish
2024-09-18 13:04:09,064 Processed batch: ['5f6fe93d', 'eb2c5661', '043ad9b9', '05efc94b', '167a2f62', '32dedc18', 'e72280a3', 'e89ca7ef', '7adb940b', '3117b825', 'a958a599', 'df1b1cfa', 'a437b000', '5861539f', 'ab34eadd', '991f264c'] with model gpt2-124m in 3.7674 seconds
2024-09-18 13:04:09,064 Latency for request 5f6fe93d with model gpt2-124m: 9.6910 seconds
2024-09-18 13:04:09,064 Saving results without gpu monitoring
2024-09-18 13:04:09,064 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:04:09,065 No batch to process for model gpt2-124m
2024-09-18 13:04:09,065 127.0.0.1 - - [18/Sep/2024 13:04:09] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:09,065 127.0.0.1 - - [18/Sep/2024 13:04:09] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:09,065 Next: call load_model for distilgpt2-124m
2024-09-18 13:04:09,065 Unloaded previous model
2024-09-18 13:04:09,132 Loaded model distilgpt2-124m
2024-09-18 13:04:09,132 Batch processing started for model distilgpt2-124m
2024-09-18 13:04:10,023 Waiting for running processes to finish
2024-09-18 13:04:11,026 Waiting for running processes to finish
2024-09-18 13:04:11,805 Processed batch: ['17aab7dd', '92f18f4f', 'dcded236', 'bdcb65e8', '90a2eb58', '9ae8222d', '7a8cc7e7', 'fff3cc7d', 'b465abc7', 'ab7de215', '6bbfce36', '5e3f9bdd', '8553cea5', 'c00b3508', '6d8fc1d1', 'df09860f'] with model distilgpt2-124m in 2.6728 seconds
2024-09-18 13:04:11,805 Latency for request 17aab7dd with model distilgpt2-124m: 10.2080 seconds
2024-09-18 13:04:11,805 Saving results without gpu monitoring
2024-09-18 13:04:11,805 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:04:11,805 Next: call load_model for gpt2medium-355m
2024-09-18 13:04:11,806 127.0.0.1 - - [18/Sep/2024 13:04:11] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:11,806 Unloaded previous model
2024-09-18 13:04:11,945 Loaded model gpt2medium-355m
2024-09-18 13:04:11,946 Batch processing started for model gpt2medium-355m
2024-09-18 13:04:12,032 Waiting for running processes to finish
2024-09-18 13:04:13,037 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 390, in inference
    total_time = last_batch_processed_time - first_request_time
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'last_batch_processed_time' is not defined
2024-09-18 13:04:13,040 127.0.0.1 - - [18/Sep/2024 13:04:13] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:21,425 Processed batch: ['6560336a', '9d9c2d4f', '4fe8923a', '020326b5', '5461f1c5', 'a75bb773', 'a2f17e6d', 'f55343cb', '2764be8a', 'e97d415c', '03ab50bd', 'd781feb9', '1b25a9dd', '3e5e5df8', '9ab4d243', 'bf8bd1d8'] with model gpt2medium-355m in 9.4794 seconds
2024-09-18 13:04:21,425 Latency for request 6560336a with model gpt2medium-355m: 23.2430 seconds
2024-09-18 13:04:21,425 Saving results without gpu monitoring
2024-09-18 13:04:21,425 Exception on /inference [POST]
Traceback (most recent call last):
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/miniconda3/envs/confidentialinference/lib/python3.12/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 498, in inference
    completed_inference_ids = process_batch(model_alias, "Batch size", max(allowed_batch_sizes))
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 287, in process_batch
    save_measurements(request_id, request_time, model_alias, current_batch_size, latency, batch_inference_time, batch_throughput)
  File "/Users/amartinezi/Documents/SINCERE/api_scheduler_experiments.py", line 142, in save_measurements
    "arrival time": time.strftime("%Y-%m-%d %H:%M:%S", request_time),
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Tuple or struct_time argument required
2024-09-18 13:04:21,426 No batch to process for model gpt2-124m
2024-09-18 13:04:21,426 127.0.0.1 - - [18/Sep/2024 13:04:21] "[35m[1mPOST /inference HTTP/1.1[0m" 500 -
2024-09-18 13:04:21,426 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,426 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,426 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,426 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,427 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,427 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,427 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,427 No batch to process for model gpt2-124m
2024-09-18 13:04:21,427 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,427 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,427 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,427 No batch to process for model gpt2-124m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model gpt2-124m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,428 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,428 No batch to process for model gpt2-124m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model gpt2-124m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model gpt2medium-355m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model gpt2-124m
2024-09-18 13:04:21,429 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:04:21,429 No batch to process for model distilgpt2-124m
2024-09-18 13:04:21,430 127.0.0.1 - - [18/Sep/2024 13:04:21] "POST /inference HTTP/1.1" 200 -
