2024-09-10 10:40:38,269 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 10:40:38,269 [33mPress CTRL+C to quit[0m
2024-09-10 10:40:38,336 Request with ID b978d645 for model gpt2-124m received
2024-09-10 10:40:38,336 Adjusted time limit for model gpt2-124m: 2.3502 seconds
2024-09-10 10:40:38,337 127.0.0.1 - - [10/Sep/2024 10:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:38,341 Request with ID 3656c890 for model gpt2medium-355m received
2024-09-10 10:40:38,341 Adjusted time limit for model gpt2medium-355m: 0.0000 seconds
2024-09-10 10:40:38,342 127.0.0.1 - - [10/Sep/2024 10:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:38,445 Time limit condition met for model gpt2medium-355m
2024-09-10 10:40:38,446 Loading model gpt2medium-355m
2024-09-10 10:40:38,448 Request with ID 8f234a89 for model gpt2-124m received
2024-09-10 10:40:38,449 127.0.0.1 - - [10/Sep/2024 10:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:38,513 Request with ID acd6ecaa for model gpt2-124m received
2024-09-10 10:40:38,513 127.0.0.1 - - [10/Sep/2024 10:40:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:39,479 Request with ID 7f9c312d for model distilgpt2-124m received
2024-09-10 10:40:39,479 Adjusted time limit for model distilgpt2-124m: 3.2043 seconds
2024-09-10 10:40:39,479 127.0.0.1 - - [10/Sep/2024 10:40:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:39,609 Request with ID bb171a8f for model gpt2-124m received
2024-09-10 10:40:39,609 Batch size condition met for model gpt2-124m
2024-09-10 10:40:40,393 Request with ID 7e81c388 for model distilgpt2-124m received
2024-09-10 10:40:40,393 127.0.0.1 - - [10/Sep/2024 10:40:40] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:41,241 Request with ID ab63a7be for model gpt2medium-355m received
2024-09-10 10:40:41,242 127.0.0.1 - - [10/Sep/2024 10:40:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:41,829 Request with ID c736cf58 for model distilgpt2-124m received
2024-09-10 10:40:41,830 127.0.0.1 - - [10/Sep/2024 10:40:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:41,970 Processed batch: ['3656c890', '69dc', 'b165', '8b55'] with model gpt2medium-355m in 3.3550 seconds
2024-09-10 10:40:41,970 Latency for request 3656c890 with model gpt2medium-355m: 3.6292 seconds
2024-09-10 10:40:41,973 Latency for request 69dc with model gpt2medium-355m: 3.5243 seconds
2024-09-10 10:40:41,973 Latency for request b165 with model gpt2medium-355m: 3.5242 seconds
2024-09-10 10:40:41,974 Latency for request 8b55 with model gpt2medium-355m: 3.5242 seconds
2024-09-10 10:40:41,974 Loading model gpt2-124m
2024-09-10 10:40:42,512 Request with ID 26e7509b for model gpt2medium-355m received
2024-09-10 10:40:42,512 Adjusted time limit for model gpt2medium-355m: 0.0000 seconds
2024-09-10 10:40:42,512 127.0.0.1 - - [10/Sep/2024 10:40:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:42,603 Time limit condition met for model gpt2medium-355m
2024-09-10 10:40:43,246 Request with ID 9d8f7fb5 for model gpt2-124m received
2024-09-10 10:40:43,246 127.0.0.1 - - [10/Sep/2024 10:40:43] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:43,417 Processed batch: ['b978d645', '8f234a89', 'acd6ecaa', 'bb171a8f'] with model gpt2-124m in 1.3728 seconds
2024-09-10 10:40:43,417 Latency for request b978d645 with model gpt2-124m: 5.0813 seconds
2024-09-10 10:40:43,418 Latency for request 8f234a89 with model gpt2-124m: 4.9688 seconds
2024-09-10 10:40:43,418 Latency for request acd6ecaa with model gpt2-124m: 4.9039 seconds
2024-09-10 10:40:43,418 Latency for request bb171a8f with model gpt2-124m: 3.8082 seconds
2024-09-10 10:40:43,419 127.0.0.1 - - [10/Sep/2024 10:40:43] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:43,419 Loading model gpt2medium-355m
2024-09-10 10:40:44,162 Request with ID b874de64 for model gpt2medium-355m received
2024-09-10 10:40:44,162 127.0.0.1 - - [10/Sep/2024 10:40:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:44,706 Request with ID f02b3958 for model gpt2-124m received
2024-09-10 10:40:44,706 Adjusted time limit for model gpt2-124m: 2.3395 seconds
2024-09-10 10:40:44,706 127.0.0.1 - - [10/Sep/2024 10:40:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:45,210 Request with ID e1b02b0c for model distilgpt2-124m received
2024-09-10 10:40:45,210 Batch size condition met for model distilgpt2-124m
2024-09-10 10:40:45,870 Processed batch: ['ab63a7be', '26e7509b', '657a', '39bc'] with model gpt2medium-355m in 2.3046 seconds
2024-09-10 10:40:45,870 Latency for request ab63a7be with model gpt2medium-355m: 4.6288 seconds
2024-09-10 10:40:45,871 Latency for request 26e7509b with model gpt2medium-355m: 3.3581 seconds
2024-09-10 10:40:45,872 Latency for request 657a with model gpt2medium-355m: 2.4513 seconds
2024-09-10 10:40:45,872 Latency for request 39bc with model gpt2medium-355m: 2.4513 seconds
2024-09-10 10:40:45,872 Loading model distilgpt2-124m
2024-09-10 10:40:46,818 Request with ID 8a5d7054 for model gpt2-124m received
2024-09-10 10:40:46,818 127.0.0.1 - - [10/Sep/2024 10:40:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:47,070 Processed batch: ['7f9c312d', '7e81c388', 'c736cf58', 'e1b02b0c'] with model distilgpt2-124m in 1.1363 seconds
2024-09-10 10:40:47,070 Latency for request 7f9c312d with model distilgpt2-124m: 7.5914 seconds
2024-09-10 10:40:47,071 Latency for request 7e81c388 with model distilgpt2-124m: 6.6778 seconds
2024-09-10 10:40:47,072 Latency for request c736cf58 with model distilgpt2-124m: 5.2411 seconds
2024-09-10 10:40:47,072 Latency for request e1b02b0c with model distilgpt2-124m: 1.8604 seconds
2024-09-10 10:40:47,072 127.0.0.1 - - [10/Sep/2024 10:40:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:47,119 Time limit condition met for model gpt2-124m
2024-09-10 10:40:47,119 Loading model gpt2-124m
2024-09-10 10:40:48,030 Processed batch: ['9d8f7fb5', 'f02b3958', '8a5d7054', 'ef4c'] with model gpt2-124m in 0.8446 seconds
2024-09-10 10:40:48,030 Latency for request 9d8f7fb5 with model gpt2-124m: 4.7840 seconds
2024-09-10 10:40:48,031 Latency for request f02b3958 with model gpt2-124m: 3.3241 seconds
2024-09-10 10:40:48,032 Latency for request 8a5d7054 with model gpt2-124m: 1.2117 seconds
2024-09-10 10:40:48,032 Latency for request ef4c with model gpt2-124m: 0.9107 seconds
2024-09-10 10:40:48,619 Request with ID 5ce255f9 for model gpt2-124m received
2024-09-10 10:40:48,619 Adjusted time limit for model gpt2-124m: 2.3434 seconds
2024-09-10 10:40:48,619 127.0.0.1 - - [10/Sep/2024 10:40:48] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:49,719 Request with ID 8605dd5a for model distilgpt2-124m received
2024-09-10 10:40:49,719 Adjusted time limit for model distilgpt2-124m: 3.2082 seconds
2024-09-10 10:40:49,720 127.0.0.1 - - [10/Sep/2024 10:40:49] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:51,046 Time limit condition met for model gpt2-124m
2024-09-10 10:40:51,047 Loading model gpt2-124m
2024-09-10 10:40:51,832 Processed batch: ['5ce255f9', '21a1', 'd9cb', '8714'] with model gpt2-124m in 0.7854 seconds
2024-09-10 10:40:51,833 Latency for request 5ce255f9 with model gpt2-124m: 3.2140 seconds
2024-09-10 10:40:51,833 Latency for request 21a1 with model gpt2-124m: 0.7859 seconds
2024-09-10 10:40:51,833 Latency for request d9cb with model gpt2-124m: 0.7859 seconds
2024-09-10 10:40:51,834 Latency for request 8714 with model gpt2-124m: 0.7859 seconds
2024-09-10 10:40:52,985 Time limit condition met for model distilgpt2-124m
2024-09-10 10:40:52,985 Loading model distilgpt2-124m
2024-09-10 10:40:53,555 Processed batch: ['8605dd5a', 'b2bd', '9c8f', '67bf'] with model distilgpt2-124m in 0.5026 seconds
2024-09-10 10:40:53,555 Latency for request 8605dd5a with model distilgpt2-124m: 3.8361 seconds
2024-09-10 10:40:53,556 Latency for request b2bd with model distilgpt2-124m: 0.5694 seconds
2024-09-10 10:40:53,556 Latency for request 9c8f with model distilgpt2-124m: 0.5694 seconds
2024-09-10 10:40:53,556 Latency for request 67bf with model distilgpt2-124m: 0.5693 seconds
2024-09-10 10:40:53,941 Request with ID ef56af11 for model gpt2-124m received
2024-09-10 10:40:53,941 Adjusted time limit for model gpt2-124m: 2.3439 seconds
2024-09-10 10:40:53,942 127.0.0.1 - - [10/Sep/2024 10:40:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:55,809 Request with ID d1e9336d for model gpt2medium-355m received
2024-09-10 10:40:55,810 Adjusted time limit for model gpt2medium-355m: 0.0000 seconds
2024-09-10 10:40:55,810 127.0.0.1 - - [10/Sep/2024 10:40:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:55,843 Time limit condition met for model gpt2medium-355m
2024-09-10 10:40:55,843 Loading model gpt2medium-355m
2024-09-10 10:40:57,216 Request with ID 8e2a6cae for model gpt2medium-355m received
2024-09-10 10:40:57,216 127.0.0.1 - - [10/Sep/2024 10:40:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:58,165 Request with ID f3a98091 for model gpt2medium-355m received
2024-09-10 10:40:58,165 127.0.0.1 - - [10/Sep/2024 10:40:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:58,554 Processed batch: ['b874de64', 'd1e9336d', '4e55', 'c6ca'] with model gpt2medium-355m in 2.5641 seconds
2024-09-10 10:40:58,554 Latency for request b874de64 with model gpt2medium-355m: 14.3927 seconds
2024-09-10 10:40:58,555 Latency for request d1e9336d with model gpt2medium-355m: 2.7449 seconds
2024-09-10 10:40:58,555 Latency for request 4e55 with model gpt2medium-355m: 2.7111 seconds
2024-09-10 10:40:58,556 Latency for request c6ca with model gpt2medium-355m: 2.7111 seconds
2024-09-10 10:40:58,661 Time limit condition met for model gpt2-124m
2024-09-10 10:40:58,661 Loading model gpt2-124m
2024-09-10 10:40:59,568 Processed batch: ['ef56af11', '8e38', '7386', '4865'] with model gpt2-124m in 0.8391 seconds
2024-09-10 10:40:59,568 Latency for request ef56af11 with model gpt2-124m: 5.6266 seconds
2024-09-10 10:40:59,569 Latency for request 8e38 with model gpt2-124m: 0.9067 seconds
2024-09-10 10:40:59,569 Latency for request 7386 with model gpt2-124m: 0.9067 seconds
2024-09-10 10:40:59,569 Latency for request 4865 with model gpt2-124m: 0.9067 seconds
2024-09-10 10:40:59,903 Request with ID b98fa100 for model gpt2medium-355m received
2024-09-10 10:40:59,903 Adjusted time limit for model gpt2medium-355m: 0.0000 seconds
2024-09-10 10:40:59,903 127.0.0.1 - - [10/Sep/2024 10:40:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:40:59,988 Time limit condition met for model gpt2medium-355m
2024-09-10 10:40:59,989 Loading model gpt2medium-355m
2024-09-10 10:41:00,962 Request with ID c11dfb78 for model gpt2-124m received
2024-09-10 10:41:00,962 Adjusted time limit for model gpt2-124m: 2.3395 seconds
2024-09-10 10:41:00,962 127.0.0.1 - - [10/Sep/2024 10:41:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:41:01,572 Request with ID caa303b2 for model distilgpt2-124m received
2024-09-10 10:41:01,572 Adjusted time limit for model distilgpt2-124m: 3.2043 seconds
2024-09-10 10:41:01,572 127.0.0.1 - - [10/Sep/2024 10:41:01] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:41:02,866 Processed batch: ['8e2a6cae', 'f3a98091', 'b98fa100', '585a'] with model gpt2medium-355m in 2.7432 seconds
2024-09-10 10:41:02,866 Latency for request 8e2a6cae with model gpt2medium-355m: 5.6505 seconds
2024-09-10 10:41:02,867 Latency for request f3a98091 with model gpt2medium-355m: 4.7013 seconds
2024-09-10 10:41:02,868 Latency for request b98fa100 with model gpt2medium-355m: 2.9634 seconds
2024-09-10 10:41:02,868 Latency for request 585a with model gpt2medium-355m: 2.8775 seconds
2024-09-10 10:41:03,164 Request with ID efec3eb3 for model distilgpt2-124m received
2024-09-10 10:41:03,164 127.0.0.1 - - [10/Sep/2024 10:41:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:41:03,383 Time limit condition met for model gpt2-124m
2024-09-10 10:41:03,383 Loading model gpt2-124m
2024-09-10 10:41:04,450 Processed batch: ['c11dfb78', '8d13', '3d4e', 'f3c8'] with model gpt2-124m in 0.9888 seconds
2024-09-10 10:41:04,450 Latency for request c11dfb78 with model gpt2-124m: 3.4885 seconds
2024-09-10 10:41:04,452 Latency for request 8d13 with model gpt2-124m: 1.0675 seconds
2024-09-10 10:41:04,452 Latency for request 3d4e with model gpt2-124m: 1.0675 seconds
2024-09-10 10:41:04,452 Latency for request f3c8 with model gpt2-124m: 1.0675 seconds
2024-09-10 10:41:04,517 Request with ID d3a067e1 for model distilgpt2-124m received
2024-09-10 10:41:04,517 127.0.0.1 - - [10/Sep/2024 10:41:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:41:04,707 Request with ID ba355c84 for model gpt2-124m received
2024-09-10 10:41:04,708 Adjusted time limit for model gpt2-124m: 2.3434 seconds
2024-09-10 10:41:04,708 127.0.0.1 - - [10/Sep/2024 10:41:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:41:04,870 Time limit condition met for model distilgpt2-124m
2024-09-10 10:41:04,870 Loading model distilgpt2-124m
2024-09-10 10:41:05,701 Processed batch: ['caa303b2', 'efec3eb3', 'd3a067e1', '0ea4'] with model distilgpt2-124m in 0.7452 seconds
2024-09-10 10:41:05,701 Latency for request caa303b2 with model distilgpt2-124m: 4.1291 seconds
2024-09-10 10:41:05,702 Latency for request efec3eb3 with model distilgpt2-124m: 2.5372 seconds
2024-09-10 10:41:05,702 Latency for request d3a067e1 with model distilgpt2-124m: 1.1838 seconds
2024-09-10 10:41:05,702 Latency for request 0ea4 with model distilgpt2-124m: 0.8307 seconds
2024-09-10 10:41:07,151 Time limit condition met for model gpt2-124m
2024-09-10 10:41:07,152 Loading model gpt2-124m
2024-09-10 10:41:07,949 Processed batch: ['ba355c84', '26c3', '11f8', '8787'] with model gpt2-124m in 0.7183 seconds
2024-09-10 10:41:07,950 Latency for request ba355c84 with model gpt2-124m: 3.2420 seconds
2024-09-10 10:41:07,951 Latency for request 26c3 with model gpt2-124m: 0.7978 seconds
2024-09-10 10:41:07,951 Latency for request 11f8 with model gpt2-124m: 0.7977 seconds
2024-09-10 10:41:07,951 Latency for request 8787 with model gpt2-124m: 0.7977 seconds
