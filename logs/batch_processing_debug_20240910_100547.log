2024-09-10 10:05:52,418 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 10:05:52,419 [33mPress CTRL+C to quit[0m
2024-09-10 10:05:52,682 Request with ID 8c63b931 for model gpt2-124m received
2024-09-10 10:05:52,682 Adjusted time limit for model gpt2-124m: 4.7502 seconds
2024-09-10 10:05:52,683 127.0.0.1 - - [10/Sep/2024 10:05:52] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:52,811 Request with ID d3824d10 for model gpt2-124m received
2024-09-10 10:05:52,811 127.0.0.1 - - [10/Sep/2024 10:05:52] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:53,310 Request with ID c4b58805 for model gpt2medium-355m received
2024-09-10 10:05:53,311 Adjusted time limit for model gpt2medium-355m: 1.3550 seconds
2024-09-10 10:05:53,311 127.0.0.1 - - [10/Sep/2024 10:05:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:53,405 Request with ID 6177d2de for model gpt2-124m received
2024-09-10 10:05:53,405 127.0.0.1 - - [10/Sep/2024 10:05:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:53,961 Request with ID 8ee21e51 for model gpt2-124m received
2024-09-10 10:05:53,961 Batch size condition met for model gpt2-124m
2024-09-10 10:05:53,961 Loading model gpt2-124m
2024-09-10 10:05:54,700 Time limit condition met for model gpt2medium-355m
2024-09-10 10:05:54,700 Loading model gpt2medium-355m
2024-09-10 10:05:54,952 Request with ID 8670d000 for model distilgpt2-124m received
2024-09-10 10:05:54,952 Adjusted time limit for model distilgpt2-124m: 5.6043 seconds
2024-09-10 10:05:54,952 127.0.0.1 - - [10/Sep/2024 10:05:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:55,570 Processed batch: ['8c63b931', 'd3824d10', '6177d2de', '8ee21e51'] with model gpt2-124m in 1.4834 seconds
2024-09-10 10:05:55,570 Latency for request 8c63b931 with model gpt2-124m: 2.8878 seconds
2024-09-10 10:05:55,572 Latency for request d3824d10 with model gpt2-124m: 2.7591 seconds
2024-09-10 10:05:55,572 Latency for request 6177d2de with model gpt2-124m: 2.1652 seconds
2024-09-10 10:05:55,572 Latency for request 8ee21e51 with model gpt2-124m: 1.6091 seconds
2024-09-10 10:05:55,578 127.0.0.1 - - [10/Sep/2024 10:05:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:55,866 Request with ID 232c8186 for model distilgpt2-124m received
2024-09-10 10:05:55,866 127.0.0.1 - - [10/Sep/2024 10:05:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:56,715 Request with ID 70a47412 for model gpt2medium-355m received
2024-09-10 10:05:56,715 127.0.0.1 - - [10/Sep/2024 10:05:56] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:57,302 Request with ID 7a8fdc5c for model distilgpt2-124m received
2024-09-10 10:05:57,303 127.0.0.1 - - [10/Sep/2024 10:05:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:57,985 Request with ID c2c1a791 for model gpt2medium-355m received
2024-09-10 10:05:57,986 127.0.0.1 - - [10/Sep/2024 10:05:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:58,719 Request with ID e14348ce for model gpt2-124m received
2024-09-10 10:05:58,719 Adjusted time limit for model gpt2-124m: 4.7395 seconds
2024-09-10 10:05:58,719 127.0.0.1 - - [10/Sep/2024 10:05:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:05:59,635 Request with ID ea3574df for model gpt2medium-355m received
2024-09-10 10:05:59,635 127.0.0.1 - - [10/Sep/2024 10:05:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:00,040 Processed batch: ['c4b58805', '82cf', '136e', '1373'] with model gpt2medium-355m in 5.2446 seconds
2024-09-10 10:06:00,040 Latency for request c4b58805 with model gpt2medium-355m: 6.7300 seconds
2024-09-10 10:06:00,041 Latency for request 82cf with model gpt2medium-355m: 5.3399 seconds
2024-09-10 10:06:00,041 Latency for request 136e with model gpt2medium-355m: 5.3399 seconds
2024-09-10 10:06:00,042 Latency for request 1373 with model gpt2medium-355m: 5.3399 seconds
2024-09-10 10:06:00,179 Request with ID 401e35e7 for model gpt2-124m received
2024-09-10 10:06:00,179 127.0.0.1 - - [10/Sep/2024 10:06:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:00,563 Time limit condition met for model distilgpt2-124m
2024-09-10 10:06:00,563 Loading model distilgpt2-124m
2024-09-10 10:06:00,683 Request with ID 205ef1dc for model distilgpt2-124m received
2024-09-10 10:06:00,683 127.0.0.1 - - [10/Sep/2024 10:06:00] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:01,875 Processed batch: ['8670d000', '232c8186', '7a8fdc5c', 'b6a1'] with model distilgpt2-124m in 1.2244 seconds
2024-09-10 10:06:01,875 Latency for request 8670d000 with model distilgpt2-124m: 6.9231 seconds
2024-09-10 10:06:01,876 Latency for request 232c8186 with model distilgpt2-124m: 6.0097 seconds
2024-09-10 10:06:01,876 Latency for request 7a8fdc5c with model distilgpt2-124m: 4.5728 seconds
2024-09-10 10:06:01,877 Latency for request b6a1 with model distilgpt2-124m: 1.3122 seconds
2024-09-10 10:06:02,294 Request with ID c232e002 for model gpt2-124m received
2024-09-10 10:06:02,295 127.0.0.1 - - [10/Sep/2024 10:06:02] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:03,535 Time limit condition met for model gpt2-124m
2024-09-10 10:06:03,535 Loading model gpt2-124m
2024-09-10 10:06:04,090 Request with ID b3385000 for model gpt2-124m received
2024-09-10 10:06:04,091 127.0.0.1 - - [10/Sep/2024 10:06:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:04,427 Processed batch: ['e14348ce', '401e35e7', 'c232e002', '89b6'] with model gpt2-124m in 0.8024 seconds
2024-09-10 10:06:04,427 Latency for request e14348ce with model gpt2-124m: 5.7088 seconds
2024-09-10 10:06:04,428 Latency for request 401e35e7 with model gpt2-124m: 4.2486 seconds
2024-09-10 10:06:04,428 Latency for request c232e002 with model gpt2-124m: 2.1334 seconds
2024-09-10 10:06:04,429 Latency for request 89b6 with model gpt2-124m: 0.8919 seconds
2024-09-10 10:06:05,190 Request with ID a67dcbf4 for model distilgpt2-124m received
2024-09-10 10:06:05,190 Adjusted time limit for model distilgpt2-124m: 5.6082 seconds
2024-09-10 10:06:05,190 127.0.0.1 - - [10/Sep/2024 10:06:05] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:09,414 Request with ID d8ebc5de for model gpt2-124m received
2024-09-10 10:06:09,414 Adjusted time limit for model gpt2-124m: 4.7434 seconds
2024-09-10 10:06:09,415 127.0.0.1 - - [10/Sep/2024 10:06:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:10,886 Time limit condition met for model distilgpt2-124m
2024-09-10 10:06:10,886 Loading model distilgpt2-124m
2024-09-10 10:06:11,279 Request with ID cffb2469 for model gpt2medium-355m received
2024-09-10 10:06:11,280 Adjusted time limit for model gpt2medium-355m: 1.3487 seconds
2024-09-10 10:06:11,280 Batch size condition met for model gpt2medium-355m
2024-09-10 10:06:11,280 Loading model gpt2medium-355m
2024-09-10 10:06:11,727 Processed batch: ['205ef1dc', 'a67dcbf4', 'b0d3', '9aba'] with model distilgpt2-124m in 0.7608 seconds
2024-09-10 10:06:11,727 Latency for request 205ef1dc with model distilgpt2-124m: 11.0447 seconds
2024-09-10 10:06:11,728 Latency for request a67dcbf4 with model distilgpt2-124m: 6.5377 seconds
2024-09-10 10:06:11,728 Latency for request b0d3 with model distilgpt2-124m: 0.8409 seconds
2024-09-10 10:06:11,729 Latency for request 9aba with model distilgpt2-124m: 0.8409 seconds
2024-09-10 10:06:12,687 Request with ID 3f7d5e59 for model gpt2medium-355m received
2024-09-10 10:06:12,688 127.0.0.1 - - [10/Sep/2024 10:06:12] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:12,764 Time limit condition met for model gpt2medium-355m
2024-09-10 10:06:12,765 Loading model gpt2medium-355m
2024-09-10 10:06:13,638 Request with ID 00dead9f for model gpt2medium-355m received
2024-09-10 10:06:13,639 127.0.0.1 - - [10/Sep/2024 10:06:13] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:14,618 Processed batch: ['70a47412', 'c2c1a791', 'ea3574df', 'cffb2469'] with model gpt2medium-355m in 3.1786 seconds
2024-09-10 10:06:14,618 Latency for request 70a47412 with model gpt2medium-355m: 17.9036 seconds
2024-09-10 10:06:14,619 Latency for request c2c1a791 with model gpt2medium-355m: 16.6330 seconds
2024-09-10 10:06:14,620 Latency for request ea3574df with model gpt2medium-355m: 14.9836 seconds
2024-09-10 10:06:14,620 Latency for request cffb2469 with model gpt2medium-355m: 3.3389 seconds
2024-09-10 10:06:14,620 127.0.0.1 - - [10/Sep/2024 10:06:14] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:15,373 Request with ID 2458aa0c for model gpt2medium-355m received
2024-09-10 10:06:15,373 Adjusted time limit for model gpt2medium-355m: 1.3443 seconds
2024-09-10 10:06:15,373 127.0.0.1 - - [10/Sep/2024 10:06:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:16,000 Processed batch: ['3f7d5e59', 'e29b', 'a43a', 'b2fb'] with model gpt2medium-355m in 3.2348 seconds
2024-09-10 10:06:16,000 Latency for request 3f7d5e59 with model gpt2medium-355m: 3.3121 seconds
2024-09-10 10:06:16,000 Latency for request e29b with model gpt2medium-355m: 3.2350 seconds
2024-09-10 10:06:16,001 Latency for request a43a with model gpt2medium-355m: 3.2350 seconds
2024-09-10 10:06:16,001 Latency for request b2fb with model gpt2medium-355m: 3.2350 seconds
2024-09-10 10:06:16,102 Time limit condition met for model gpt2-124m
2024-09-10 10:06:16,102 Loading model gpt2-124m
2024-09-10 10:06:16,434 Request with ID d1f45a61 for model gpt2-124m received
2024-09-10 10:06:16,434 127.0.0.1 - - [10/Sep/2024 10:06:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:16,927 Processed batch: ['b3385000', 'd8ebc5de', '8eef', '5d7a'] with model gpt2-124m in 0.7571 seconds
2024-09-10 10:06:16,927 Latency for request b3385000 with model gpt2-124m: 12.8365 seconds
2024-09-10 10:06:16,928 Latency for request d8ebc5de with model gpt2-124m: 7.5128 seconds
2024-09-10 10:06:16,928 Latency for request 8eef with model gpt2-124m: 0.8252 seconds
2024-09-10 10:06:16,928 Latency for request 5d7a with model gpt2-124m: 0.8252 seconds
2024-09-10 10:06:17,043 Request with ID 40f2e00a for model distilgpt2-124m received
2024-09-10 10:06:17,043 Adjusted time limit for model distilgpt2-124m: 5.6082 seconds
2024-09-10 10:06:17,043 127.0.0.1 - - [10/Sep/2024 10:06:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:18,640 Request with ID 26650d5c for model distilgpt2-124m received
2024-09-10 10:06:18,640 127.0.0.1 - - [10/Sep/2024 10:06:18] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:19,993 Request with ID 373debcf for model distilgpt2-124m received
2024-09-10 10:06:19,994 127.0.0.1 - - [10/Sep/2024 10:06:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:20,181 Request with ID c51ad6d9 for model gpt2-124m received
2024-09-10 10:06:20,181 Adjusted time limit for model gpt2-124m: 4.7434 seconds
2024-09-10 10:06:20,181 127.0.0.1 - - [10/Sep/2024 10:06:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 10:06:22,749 Time limit condition met for model distilgpt2-124m
2024-09-10 10:06:22,749 Loading model distilgpt2-124m
2024-09-10 10:06:23,510 Processed batch: ['40f2e00a', '26650d5c', '373debcf', '28e3'] with model distilgpt2-124m in 0.6912 seconds
2024-09-10 10:06:23,510 Latency for request 40f2e00a with model distilgpt2-124m: 6.4667 seconds
2024-09-10 10:06:23,511 Latency for request 26650d5c with model distilgpt2-124m: 4.8702 seconds
2024-09-10 10:06:23,511 Latency for request 373debcf with model distilgpt2-124m: 3.5166 seconds
2024-09-10 10:06:23,511 Latency for request 28e3 with model distilgpt2-124m: 0.7605 seconds
2024-09-10 10:06:24,966 Time limit condition met for model gpt2-124m
2024-09-10 10:06:24,967 Loading model gpt2-124m
2024-09-10 10:06:25,927 Processed batch: ['d1f45a61', 'c51ad6d9', '6af2', '6957'] with model gpt2-124m in 0.8739 seconds
2024-09-10 10:06:25,927 Latency for request d1f45a61 with model gpt2-124m: 9.4929 seconds
2024-09-10 10:06:25,927 Latency for request c51ad6d9 with model gpt2-124m: 5.7454 seconds
2024-09-10 10:06:25,928 Latency for request 6af2 with model gpt2-124m: 0.9599 seconds
2024-09-10 10:06:25,928 Latency for request 6957 with model gpt2-124m: 0.9599 seconds
