2024-09-18 13:13:29,908 Using device: cpu
2024-09-18 13:13:29,908 Scheduling mode set as batchedFCFS
2024-09-18 13:13:29,925 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-18 13:13:29,926 [33mPress CTRL+C to quit[0m
2024-09-18 13:13:32,543 Request with ID 5270c9bc for model gpt2medium-355m received
2024-09-18 13:13:32,543 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:32,678 Request with ID 87d9e826 for model distilgpt2-124m received
2024-09-18 13:13:32,678 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:32,695 Request with ID c04ebf3e for model gpt2medium-355m received
2024-09-18 13:13:32,695 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:32,902 Request with ID d57b6b9e for model distilgpt2-124m received
2024-09-18 13:13:32,902 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:32,908 Request with ID 811dc5d5 for model distilgpt2-124m received
2024-09-18 13:13:32,908 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:32,934 Request with ID 6a8974e4 for model gpt2-124m received
2024-09-18 13:13:32,934 127.0.0.1 - - [18/Sep/2024 13:13:32] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,032 Request with ID ae47ad4e for model gpt2-124m received
2024-09-18 13:13:33,033 127.0.0.1 - - [18/Sep/2024 13:13:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,091 Request with ID da110fc7 for model gpt2medium-355m received
2024-09-18 13:13:33,092 127.0.0.1 - - [18/Sep/2024 13:13:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,145 Request with ID 21e8df6a for model gpt2medium-355m received
2024-09-18 13:13:33,146 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:33,146 Next: call load_model for gpt2medium-355m
2024-09-18 13:13:33,208 Request with ID ccaf366f for model gpt2medium-355m received
2024-09-18 13:13:33,208 127.0.0.1 - - [18/Sep/2024 13:13:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,303 Loaded model gpt2medium-355m
2024-09-18 13:13:33,303 Batch processing started for model gpt2medium-355m
2024-09-18 13:13:33,614 Request with ID 76474255 for model gpt2medium-355m received
2024-09-18 13:13:33,615 127.0.0.1 - - [18/Sep/2024 13:13:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,812 Request with ID 95ef4008 for model gpt2-124m received
2024-09-18 13:13:33,812 127.0.0.1 - - [18/Sep/2024 13:13:33] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:33,873 Request with ID 86f74825 for model gpt2-124m received
2024-09-18 13:13:33,873 Batch size condition met for model gpt2-124m
2024-09-18 13:13:34,212 Request with ID d7ed3a99 for model distilgpt2-124m received
2024-09-18 13:13:34,212 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:34,249 Request with ID 0ad78bc2 for model gpt2medium-355m received
2024-09-18 13:13:34,250 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,366 Request with ID 381d51e4 for model gpt2medium-355m received
2024-09-18 13:13:34,366 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:34,386 Request with ID dff6692f for model distilgpt2-124m received
2024-09-18 13:13:34,387 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,496 Request with ID 0d4ca0c3 for model distilgpt2-124m received
2024-09-18 13:13:34,496 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,520 Request with ID 71e1cf28 for model gpt2medium-355m received
2024-09-18 13:13:34,521 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,707 Request with ID 56f91208 for model distilgpt2-124m received
2024-09-18 13:13:34,707 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,758 Request with ID 200a9e30 for model distilgpt2-124m received
2024-09-18 13:13:34,758 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:34,890 Request with ID 3085f566 for model distilgpt2-124m received
2024-09-18 13:13:34,890 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,913 Request with ID a0d7756e for model distilgpt2-124m received
2024-09-18 13:13:34,913 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:34,953 Request with ID 4639102f for model gpt2medium-355m received
2024-09-18 13:13:34,954 127.0.0.1 - - [18/Sep/2024 13:13:34] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,131 Request with ID 8b7756de for model gpt2-124m received
2024-09-18 13:13:35,131 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,148 Request with ID 4b70ec06 for model gpt2-124m received
2024-09-18 13:13:35,148 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,156 Request with ID b8ac29f3 for model gpt2-124m received
2024-09-18 13:13:35,156 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,180 Request with ID 5993c0f2 for model gpt2medium-355m received
2024-09-18 13:13:35,180 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,350 Request with ID 93b9360c for model gpt2medium-355m received
2024-09-18 13:13:35,350 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:35,508 Request with ID 92378cc0 for model gpt2-124m received
2024-09-18 13:13:35,508 Batch size condition met for model gpt2-124m
2024-09-18 13:13:35,770 Request with ID 732e3e30 for model gpt2medium-355m received
2024-09-18 13:13:35,770 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,961 Request with ID 013fea9f for model distilgpt2-124m received
2024-09-18 13:13:35,961 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:35,972 Request with ID ca4ee00c for model gpt2medium-355m received
2024-09-18 13:13:35,972 127.0.0.1 - - [18/Sep/2024 13:13:35] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,084 Request with ID 81c5cfff for model gpt2medium-355m received
2024-09-18 13:13:36,084 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,175 Request with ID 6f49007c for model gpt2-124m received
2024-09-18 13:13:36,175 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,225 Request with ID e6b39408 for model gpt2medium-355m received
2024-09-18 13:13:36,225 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:36,369 Request with ID d402a237 for model gpt2medium-355m received
2024-09-18 13:13:36,369 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,400 Request with ID 83ad3a09 for model gpt2medium-355m received
2024-09-18 13:13:36,400 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,457 Request with ID 6e0b77d3 for model gpt2medium-355m received
2024-09-18 13:13:36,457 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,460 Request with ID e46a2114 for model gpt2-124m received
2024-09-18 13:13:36,460 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,533 Request with ID 89cee963 for model gpt2-124m received
2024-09-18 13:13:36,533 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,563 Request with ID 7954bf75 for model gpt2medium-355m received
2024-09-18 13:13:36,563 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:36,587 Request with ID f4f2ff4f for model gpt2medium-355m received
2024-09-18 13:13:36,587 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,622 Request with ID e0ce9445 for model distilgpt2-124m received
2024-09-18 13:13:36,622 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:36,721 Request with ID ae24a7e5 for model gpt2-124m received
2024-09-18 13:13:36,722 Batch size condition met for model gpt2-124m
2024-09-18 13:13:36,735 Request with ID 44411546 for model distilgpt2-124m received
2024-09-18 13:13:36,735 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,950 Request with ID 24a56eb1 for model distilgpt2-124m received
2024-09-18 13:13:36,950 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:36,965 Request with ID 504488e6 for model gpt2medium-355m received
2024-09-18 13:13:36,965 127.0.0.1 - - [18/Sep/2024 13:13:36] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,203 Request with ID 369f1ccd for model gpt2-124m received
2024-09-18 13:13:37,203 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,309 Request with ID b7671164 for model gpt2-124m received
2024-09-18 13:13:37,309 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,347 Request with ID 2e20bcce for model gpt2medium-355m received
2024-09-18 13:13:37,347 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,402 Request with ID 18d911c1 for model gpt2medium-355m received
2024-09-18 13:13:37,402 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:37,422 Request with ID c1e41940 for model gpt2-124m received
2024-09-18 13:13:37,422 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,475 Request with ID e3ec9173 for model gpt2-124m received
2024-09-18 13:13:37,475 Batch size condition met for model gpt2-124m
2024-09-18 13:13:37,506 Request with ID 5f52f537 for model distilgpt2-124m received
2024-09-18 13:13:37,506 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:37,626 Request with ID b18960ec for model gpt2-124m received
2024-09-18 13:13:37,626 127.0.0.1 - - [18/Sep/2024 13:13:37] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,020 Processed batch: ['5270c9bc', 'c04ebf3e', 'da110fc7', '21e8df6a'] with model gpt2medium-355m in 4.7167 seconds
2024-09-18 13:13:38,020 Latency for request 5270c9bc with model gpt2medium-355m: 5.4770 seconds
2024-09-18 13:13:38,020 Saving results without gpu monitoring
2024-09-18 13:13:38,022 Latency for request c04ebf3e with model gpt2medium-355m: 5.3250 seconds
2024-09-18 13:13:38,022 Saving results without gpu monitoring
2024-09-18 13:13:38,023 Latency for request da110fc7 with model gpt2medium-355m: 4.9290 seconds
2024-09-18 13:13:38,023 Saving results without gpu monitoring
2024-09-18 13:13:38,023 Latency for request 21e8df6a with model gpt2medium-355m: 4.8750 seconds
2024-09-18 13:13:38,023 Saving results without gpu monitoring
2024-09-18 13:13:38,023 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,023 Next: call load_model for gpt2-124m
2024-09-18 13:13:38,032 Unloaded previous model
2024-09-18 13:13:38,098 Loaded model gpt2-124m
2024-09-18 13:13:38,098 Batch processing started for model gpt2-124m
2024-09-18 13:13:38,237 Request with ID 62ccb72c for model gpt2-124m received
2024-09-18 13:13:38,237 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,613 Request with ID 2325ac43 for model gpt2medium-355m received
2024-09-18 13:13:38,613 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,647 Request with ID e65e4e18 for model distilgpt2-124m received
2024-09-18 13:13:38,647 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:38,724 Request with ID 51de8d8d for model distilgpt2-124m received
2024-09-18 13:13:38,724 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,753 Request with ID 2dfce560 for model gpt2medium-355m received
2024-09-18 13:13:38,753 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:38,898 Request with ID 49cccce2 for model gpt2-124m received
2024-09-18 13:13:38,898 127.0.0.1 - - [18/Sep/2024 13:13:38] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,030 Request with ID e06342f7 for model distilgpt2-124m received
2024-09-18 13:13:39,030 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,108 Request with ID 219f095e for model gpt2-124m received
2024-09-18 13:13:39,108 Batch size condition met for model gpt2-124m
2024-09-18 13:13:39,148 Request with ID 4211145a for model gpt2medium-355m received
2024-09-18 13:13:39,148 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,174 Request with ID 7c420fdf for model gpt2medium-355m received
2024-09-18 13:13:39,174 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:39,342 Request with ID 32ffb0d8 for model gpt2-124m received
2024-09-18 13:13:39,342 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,460 Request with ID a861b6d6 for model gpt2medium-355m received
2024-09-18 13:13:39,460 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,738 Request with ID e36cfd6b for model gpt2medium-355m received
2024-09-18 13:13:39,738 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,781 Request with ID 23e9b002 for model gpt2medium-355m received
2024-09-18 13:13:39,781 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:39,860 Request with ID 3ea15fbe for model gpt2medium-355m received
2024-09-18 13:13:39,861 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:39,969 Request with ID 320e78b0 for model gpt2-124m received
2024-09-18 13:13:39,969 127.0.0.1 - - [18/Sep/2024 13:13:39] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:40,043 Request with ID afe4d51d for model distilgpt2-124m received
2024-09-18 13:13:40,043 127.0.0.1 - - [18/Sep/2024 13:13:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:40,278 Processed batch: ['369f1ccd', 'b7671164', 'c1e41940', 'e3ec9173'] with model gpt2-124m in 2.1798 seconds
2024-09-18 13:13:40,278 Latency for request 369f1ccd with model gpt2-124m: 3.0750 seconds
2024-09-18 13:13:40,278 Saving results without gpu monitoring
2024-09-18 13:13:40,279 Latency for request b7671164 with model gpt2-124m: 2.9690 seconds
2024-09-18 13:13:40,279 Saving results without gpu monitoring
2024-09-18 13:13:40,280 Latency for request c1e41940 with model gpt2-124m: 2.8560 seconds
2024-09-18 13:13:40,280 Saving results without gpu monitoring
2024-09-18 13:13:40,280 Latency for request e3ec9173 with model gpt2-124m: 2.8030 seconds
2024-09-18 13:13:40,280 Saving results without gpu monitoring
2024-09-18 13:13:40,280 127.0.0.1 - - [18/Sep/2024 13:13:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:40,280 Next: call load_model for distilgpt2-124m
2024-09-18 13:13:40,287 Unloaded previous model
2024-09-18 13:13:40,333 Loaded model distilgpt2-124m
2024-09-18 13:13:40,333 Batch processing started for model distilgpt2-124m
2024-09-18 13:13:40,603 Request with ID 66e480df for model gpt2-124m received
2024-09-18 13:13:40,603 127.0.0.1 - - [18/Sep/2024 13:13:40] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:40,818 Request with ID b2982c1d for model gpt2-124m received
2024-09-18 13:13:40,818 Batch size condition met for model gpt2-124m
2024-09-18 13:13:40,863 Request with ID 98d0cbe8 for model distilgpt2-124m received
2024-09-18 13:13:40,863 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:41,015 Request with ID dbdeac00 for model gpt2medium-355m received
2024-09-18 13:13:41,015 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,291 Request with ID c5a05000 for model distilgpt2-124m received
2024-09-18 13:13:41,291 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,402 Request with ID 16daf942 for model gpt2medium-355m received
2024-09-18 13:13:41,402 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,450 Request with ID 1dcc4c54 for model gpt2medium-355m received
2024-09-18 13:13:41,451 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,816 Request with ID f5c32606 for model distilgpt2-124m received
2024-09-18 13:13:41,816 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,897 Request with ID 5403b80e for model gpt2-124m received
2024-09-18 13:13:41,897 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,917 Request with ID 55e9b1e2 for model gpt2-124m received
2024-09-18 13:13:41,917 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,963 Processed batch: ['44411546', '24a56eb1', '5f52f537', 'e65e4e18'] with model distilgpt2-124m in 1.6301 seconds
2024-09-18 13:13:41,964 Latency for request 44411546 with model distilgpt2-124m: 5.2290 seconds
2024-09-18 13:13:41,964 Saving results without gpu monitoring
2024-09-18 13:13:41,964 Latency for request 24a56eb1 with model distilgpt2-124m: 5.0130 seconds
2024-09-18 13:13:41,964 Saving results without gpu monitoring
2024-09-18 13:13:41,965 Latency for request 5f52f537 with model distilgpt2-124m: 4.4580 seconds
2024-09-18 13:13:41,965 Saving results without gpu monitoring
2024-09-18 13:13:41,965 Latency for request e65e4e18 with model distilgpt2-124m: 3.3160 seconds
2024-09-18 13:13:41,965 Saving results without gpu monitoring
2024-09-18 13:13:41,965 127.0.0.1 - - [18/Sep/2024 13:13:41] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:41,965 Next: call load_model for gpt2medium-355m
2024-09-18 13:13:41,971 Unloaded previous model
2024-09-18 13:13:42,005 Request with ID 9ace7123 for model gpt2medium-355m received
2024-09-18 13:13:42,005 Batch size condition met for model gpt2medium-355m
2024-09-18 13:13:42,115 Request with ID c963a163 for model gpt2-124m received
2024-09-18 13:13:42,115 127.0.0.1 - - [18/Sep/2024 13:13:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:42,134 Loaded model gpt2medium-355m
2024-09-18 13:13:42,134 Batch processing started for model gpt2medium-355m
2024-09-18 13:13:42,195 Request with ID 9b524c69 for model gpt2medium-355m received
2024-09-18 13:13:42,195 127.0.0.1 - - [18/Sep/2024 13:13:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:42,297 Request with ID 5381ac9a for model distilgpt2-124m received
2024-09-18 13:13:42,297 127.0.0.1 - - [18/Sep/2024 13:13:42] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:42,388 Request with ID 39b34cc7 for model distilgpt2-124m received
2024-09-18 13:13:42,388 Batch size condition met for model distilgpt2-124m
2024-09-18 13:13:42,544 Waiting for running processes to finish
2024-09-18 13:13:43,549 Waiting for running processes to finish
2024-09-18 13:13:44,550 Waiting for running processes to finish
2024-09-18 13:13:45,554 Waiting for running processes to finish
2024-09-18 13:13:46,559 Waiting for running processes to finish
2024-09-18 13:13:47,280 Processed batch: ['a861b6d6', 'e36cfd6b', '23e9b002', '3ea15fbe'] with model gpt2medium-355m in 5.1463 seconds
2024-09-18 13:13:47,280 Latency for request a861b6d6 with model gpt2medium-355m: 7.8200 seconds
2024-09-18 13:13:47,280 Saving results without gpu monitoring
2024-09-18 13:13:47,281 Latency for request e36cfd6b with model gpt2medium-355m: 7.5430 seconds
2024-09-18 13:13:47,281 Saving results without gpu monitoring
2024-09-18 13:13:47,281 Latency for request 23e9b002 with model gpt2medium-355m: 7.5000 seconds
2024-09-18 13:13:47,281 Saving results without gpu monitoring
2024-09-18 13:13:47,282 Latency for request 3ea15fbe with model gpt2medium-355m: 7.4200 seconds
2024-09-18 13:13:47,282 Saving results without gpu monitoring
2024-09-18 13:13:47,282 127.0.0.1 - - [18/Sep/2024 13:13:47] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:47,282 Next: call load_model for distilgpt2-124m
2024-09-18 13:13:47,295 Unloaded previous model
2024-09-18 13:13:47,346 Loaded model distilgpt2-124m
2024-09-18 13:13:47,347 Batch processing started for model distilgpt2-124m
2024-09-18 13:13:47,564 Waiting for running processes to finish
2024-09-18 13:13:48,378 Processed batch: ['c5a05000', 'f5c32606', '5381ac9a', '39b34cc7'] with model distilgpt2-124m in 1.0318 seconds
2024-09-18 13:13:48,378 Latency for request c5a05000 with model distilgpt2-124m: 7.0870 seconds
2024-09-18 13:13:48,378 Saving results without gpu monitoring
2024-09-18 13:13:48,379 Latency for request f5c32606 with model distilgpt2-124m: 6.5620 seconds
2024-09-18 13:13:48,379 Saving results without gpu monitoring
2024-09-18 13:13:48,380 Latency for request 5381ac9a with model distilgpt2-124m: 6.0810 seconds
2024-09-18 13:13:48,380 Saving results without gpu monitoring
2024-09-18 13:13:48,380 Latency for request 39b34cc7 with model distilgpt2-124m: 5.9910 seconds
2024-09-18 13:13:48,380 Saving results without gpu monitoring
2024-09-18 13:13:48,380 127.0.0.1 - - [18/Sep/2024 13:13:48] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:48,380 Next: call load_model for gpt2medium-355m
2024-09-18 13:13:48,389 Unloaded previous model
2024-09-18 13:13:48,501 Loaded model gpt2medium-355m
2024-09-18 13:13:48,501 Batch processing started for model gpt2medium-355m
2024-09-18 13:13:48,569 Waiting for running processes to finish
2024-09-18 13:13:49,575 Waiting for running processes to finish
2024-09-18 13:13:50,576 Waiting for running processes to finish
2024-09-18 13:13:51,581 Waiting for running processes to finish
2024-09-18 13:13:52,587 Waiting for running processes to finish
2024-09-18 13:13:53,593 Waiting for running processes to finish
2024-09-18 13:13:53,946 Processed batch: ['dbdeac00', '16daf942', '1dcc4c54', '9ace7123'] with model gpt2medium-355m in 5.4442 seconds
2024-09-18 13:13:53,946 Latency for request dbdeac00 with model gpt2medium-355m: 12.9310 seconds
2024-09-18 13:13:53,946 Saving results without gpu monitoring
2024-09-18 13:13:53,946 Latency for request 16daf942 with model gpt2medium-355m: 12.5430 seconds
2024-09-18 13:13:53,946 Saving results without gpu monitoring
2024-09-18 13:13:53,947 Latency for request 1dcc4c54 with model gpt2medium-355m: 12.4950 seconds
2024-09-18 13:13:53,947 Saving results without gpu monitoring
2024-09-18 13:13:53,947 Latency for request 9ace7123 with model gpt2medium-355m: 11.9410 seconds
2024-09-18 13:13:53,947 Saving results without gpu monitoring
2024-09-18 13:13:53,947 127.0.0.1 - - [18/Sep/2024 13:13:53] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:53,947 Next: call load_model for gpt2-124m
2024-09-18 13:13:53,961 Unloaded previous model
2024-09-18 13:13:54,031 Loaded model gpt2-124m
2024-09-18 13:13:54,032 Batch processing started for model gpt2-124m
2024-09-18 13:13:54,598 Waiting for running processes to finish
2024-09-18 13:13:55,602 Total time: 21.4042 seconds
2024-09-18 13:13:55,603 Total inference time: 20.1490 seconds
2024-09-18 13:13:55,603 Inference time as percentage of total time: 94.14%
2024-09-18 13:13:55,603 END
2024-09-18 13:13:55,603 127.0.0.1 - - [18/Sep/2024 13:13:55] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,009 Processed batch: ['32ffb0d8', '320e78b0', '66e480df', 'b2982c1d'] with model gpt2-124m in 1.9779 seconds
2024-09-18 13:13:56,010 Latency for request 32ffb0d8 with model gpt2-124m: 16.6670 seconds
2024-09-18 13:13:56,010 Saving results without gpu monitoring
2024-09-18 13:13:56,010 Latency for request 320e78b0 with model gpt2-124m: 16.0410 seconds
2024-09-18 13:13:56,010 Saving results without gpu monitoring
2024-09-18 13:13:56,011 Latency for request 66e480df with model gpt2-124m: 15.4070 seconds
2024-09-18 13:13:56,011 Saving results without gpu monitoring
2024-09-18 13:13:56,011 Latency for request b2982c1d with model gpt2-124m: 15.1910 seconds
2024-09-18 13:13:56,011 Saving results without gpu monitoring
2024-09-18 13:13:56,011 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,011 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model distilgpt2-124m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model gpt2-124m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model gpt2-124m
2024-09-18 13:13:56,012 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,012 No batch to process for model distilgpt2-124m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model gpt2-124m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model gpt2-124m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model distilgpt2-124m
2024-09-18 13:13:56,013 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,013 No batch to process for model gpt2medium-355m
2024-09-18 13:13:56,014 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
2024-09-18 13:13:56,014 No batch to process for model distilgpt2-124m
2024-09-18 13:13:56,014 127.0.0.1 - - [18/Sep/2024 13:13:56] "POST /inference HTTP/1.1" 200 -
