2024-09-10 15:25:36,183 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 15:25:36,183 [33mPress CTRL+C to quit[0m
2024-09-10 15:25:36,189 Request with ID cab0b86e for model gpt2-124m received
2024-09-10 15:25:36,189 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:36,189 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 15:25:36,189 Batch size condition met for model gpt2-124m
2024-09-10 15:25:36,189 Updated batch size:1
2024-09-10 15:25:36,189 Loading model gpt2-124m
2024-09-10 15:25:36,264 Request with ID 9feb2549 for model gpt2medium-355m received
2024-09-10 15:25:36,264 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:36,264 Request with ID 25ceffb8 for model gpt2-124m received
2024-09-10 15:25:36,264 Adjusted time limit for model gpt2medium-355m: 11.8866 seconds
2024-09-10 15:25:36,264 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 15:25:36,265 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:36,265 Batch size condition met for model gpt2-124m
2024-09-10 15:25:36,311 Request with ID 6e9e5390 for model gpt2-124m received
2024-09-10 15:25:36,311 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:36,312 Batch size condition met for model gpt2-124m
2024-09-10 15:25:36,497 Request with ID b6cbc9fe for model gpt2-124m received
2024-09-10 15:25:36,497 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:36,497 Batch size condition met for model gpt2-124m
2024-09-10 15:25:36,662 Processed batch: ['cab0b86e'] with model gpt2-124m in 0.3833 seconds
2024-09-10 15:25:36,662 Latency for request cab0b86e with model gpt2-124m: 0.4725 seconds
2024-09-10 15:25:36,664 Total time: 0.4745 seconds
2024-09-10 15:25:36,664 Total inference time: 0.3833 seconds
2024-09-10 15:25:36,664 Inference time as percentage of total time: 80.78%
2024-09-10 15:25:36,664 127.0.0.1 - - [10/Sep/2024 15:25:36] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:36,664 Updated batch size:1
2024-09-10 15:25:36,664 Loading model gpt2medium-355m
2024-09-10 15:25:36,751 Request with ID cf33cb32 for model distilgpt2-124m received
2024-09-10 15:25:36,751 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:36,752 Adjusted time limit for model distilgpt2-124m: 14.1882 seconds
2024-09-10 15:25:36,752 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:37,076 Request with ID 3142128c for model distilgpt2-124m received
2024-09-10 15:25:37,076 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:37,076 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:37,417 Request with ID 279ad6d0 for model gpt2medium-355m received
2024-09-10 15:25:37,417 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:37,417 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:37,653 Request with ID 038a8f63 for model distilgpt2-124m received
2024-09-10 15:25:37,653 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:37,653 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:37,927 Request with ID d610fd3c for model gpt2medium-355m received
2024-09-10 15:25:37,927 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:37,927 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:38,155 Processed batch: ['9feb2549'] with model gpt2medium-355m in 1.3596 seconds
2024-09-10 15:25:38,155 Latency for request 9feb2549 with model gpt2medium-355m: 1.8909 seconds
2024-09-10 15:25:38,156 Total time: 1.9666 seconds
2024-09-10 15:25:38,156 Total inference time: 1.7429 seconds
2024-09-10 15:25:38,156 Inference time as percentage of total time: 88.62%
2024-09-10 15:25:38,156 127.0.0.1 - - [10/Sep/2024 15:25:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:38,156 Updated batch size:1
2024-09-10 15:25:38,156 Loading model gpt2-124m
2024-09-10 15:25:38,221 Request with ID c10ebe61 for model gpt2-124m received
2024-09-10 15:25:38,221 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:38,221 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:25:38,221 Batch size condition met for model gpt2-124m
2024-09-10 15:25:38,588 Request with ID 6299f2f6 for model gpt2medium-355m received
2024-09-10 15:25:38,589 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:38,589 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:25:38,589 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:38,628 Processed batch: ['b6cbc9fe'] with model gpt2-124m in 0.4080 seconds
2024-09-10 15:25:38,628 Latency for request b6cbc9fe with model gpt2-124m: 2.1312 seconds
2024-09-10 15:25:38,629 Total time: 2.4398 seconds
2024-09-10 15:25:38,629 Total inference time: 2.1509 seconds
2024-09-10 15:25:38,629 Inference time as percentage of total time: 88.16%
2024-09-10 15:25:38,629 127.0.0.1 - - [10/Sep/2024 15:25:38] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:38,629 Updated batch size:1
2024-09-10 15:25:38,629 Loading model gpt2-124m
2024-09-10 15:25:38,807 Request with ID 16018d7b for model gpt2-124m received
2024-09-10 15:25:38,807 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:38,807 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:25:38,807 Batch size condition met for model gpt2-124m
2024-09-10 15:25:39,009 Request with ID 1efa83d3 for model distilgpt2-124m received
2024-09-10 15:25:39,009 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:39,009 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:39,079 Processed batch: ['c10ebe61'] with model gpt2-124m in 0.4496 seconds
2024-09-10 15:25:39,079 Latency for request c10ebe61 with model gpt2-124m: 0.8577 seconds
2024-09-10 15:25:39,080 Total time: 2.8904 seconds
2024-09-10 15:25:39,080 Total inference time: 2.6006 seconds
2024-09-10 15:25:39,080 Inference time as percentage of total time: 89.97%
2024-09-10 15:25:39,080 127.0.0.1 - - [10/Sep/2024 15:25:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:39,080 Updated batch size:1
2024-09-10 15:25:39,080 Loading model gpt2-124m
2024-09-10 15:25:39,439 Processed batch: ['16018d7b'] with model gpt2-124m in 0.3591 seconds
2024-09-10 15:25:39,439 Latency for request 16018d7b with model gpt2-124m: 0.6321 seconds
2024-09-10 15:25:39,440 Total time: 3.2506 seconds
2024-09-10 15:25:39,440 Total inference time: 2.9597 seconds
2024-09-10 15:25:39,440 Inference time as percentage of total time: 91.05%
2024-09-10 15:25:39,440 127.0.0.1 - - [10/Sep/2024 15:25:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:39,440 Updated batch size:1
2024-09-10 15:25:39,440 Loading model distilgpt2-124m
2024-09-10 15:25:39,654 Request with ID 8e282c12 for model gpt2-124m received
2024-09-10 15:25:39,654 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:39,654 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:25:39,654 Batch size condition met for model gpt2-124m
2024-09-10 15:25:39,823 Processed batch: ['1efa83d3'] with model distilgpt2-124m in 0.2876 seconds
2024-09-10 15:25:39,823 Latency for request 1efa83d3 with model distilgpt2-124m: 0.8137 seconds
2024-09-10 15:25:39,823 Total time: 3.6344 seconds
2024-09-10 15:25:39,823 Total inference time: 3.2473 seconds
2024-09-10 15:25:39,823 Inference time as percentage of total time: 89.35%
2024-09-10 15:25:39,824 127.0.0.1 - - [10/Sep/2024 15:25:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:39,824 No batch to process for model distilgpt2-124m
2024-09-10 15:25:39,824 127.0.0.1 - - [10/Sep/2024 15:25:39] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:39,824 Updated batch size:1
2024-09-10 15:25:39,824 Loading model gpt2medium-355m
2024-09-10 15:25:40,373 Request with ID b5cbda69 for model gpt2-124m received
2024-09-10 15:25:40,373 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:40,374 Batch size condition met for model gpt2-124m
2024-09-10 15:25:40,813 Request with ID a5f45b25 for model distilgpt2-124m received
2024-09-10 15:25:40,814 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:40,814 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:25:40,814 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:41,246 Processed batch: ['6299f2f6'] with model gpt2medium-355m in 1.3097 seconds
2024-09-10 15:25:41,246 Latency for request 6299f2f6 with model gpt2medium-355m: 2.6578 seconds
2024-09-10 15:25:41,247 Total time: 5.0578 seconds
2024-09-10 15:25:41,247 Total inference time: 4.5571 seconds
2024-09-10 15:25:41,247 Inference time as percentage of total time: 90.10%
2024-09-10 15:25:41,247 127.0.0.1 - - [10/Sep/2024 15:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:41,247 Updated batch size:1
2024-09-10 15:25:41,247 Loading model distilgpt2-124m
2024-09-10 15:25:41,582 Processed batch: ['a5f45b25'] with model distilgpt2-124m in 0.2826 seconds
2024-09-10 15:25:41,582 Latency for request a5f45b25 with model distilgpt2-124m: 0.7689 seconds
2024-09-10 15:25:41,583 Total time: 5.3940 seconds
2024-09-10 15:25:41,583 Total inference time: 4.8396 seconds
2024-09-10 15:25:41,583 Inference time as percentage of total time: 89.72%
2024-09-10 15:25:41,583 127.0.0.1 - - [10/Sep/2024 15:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:41,583 No batch to process for model gpt2medium-355m
2024-09-10 15:25:41,583 127.0.0.1 - - [10/Sep/2024 15:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:41,583 Updated batch size:1
2024-09-10 15:25:41,584 Loading model gpt2-124m
2024-09-10 15:25:42,065 Processed batch: ['b5cbda69'] with model gpt2-124m in 0.4199 seconds
2024-09-10 15:25:42,065 Latency for request b5cbda69 with model gpt2-124m: 1.6921 seconds
2024-09-10 15:25:42,066 Total time: 5.8769 seconds
2024-09-10 15:25:42,066 Total inference time: 5.2595 seconds
2024-09-10 15:25:42,066 Inference time as percentage of total time: 89.49%
2024-09-10 15:25:42,066 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,066 No batch to process for model gpt2medium-355m
2024-09-10 15:25:42,066 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,066 No batch to process for model gpt2-124m
2024-09-10 15:25:42,066 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,067 No batch to process for model distilgpt2-124m
2024-09-10 15:25:42,067 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,067 No batch to process for model gpt2-124m
2024-09-10 15:25:42,067 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,067 No batch to process for model gpt2-124m
2024-09-10 15:25:42,067 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,068 No batch to process for model distilgpt2-124m
2024-09-10 15:25:42,068 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:42,506 Request with ID cf9f2c33 for model gpt2-124m received
2024-09-10 15:25:42,507 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:42,507 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:25:42,507 Batch size condition met for model gpt2-124m
2024-09-10 15:25:42,507 Updated batch size:1
2024-09-10 15:25:42,507 Loading model gpt2-124m
2024-09-10 15:25:42,892 Processed batch: ['cf9f2c33'] with model gpt2-124m in 0.3838 seconds
2024-09-10 15:25:42,892 Latency for request cf9f2c33 with model gpt2-124m: 0.3854 seconds
2024-09-10 15:25:42,892 Total time: 6.7031 seconds
2024-09-10 15:25:42,892 Total inference time: 5.6434 seconds
2024-09-10 15:25:42,892 Inference time as percentage of total time: 84.19%
2024-09-10 15:25:42,892 127.0.0.1 - - [10/Sep/2024 15:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:43,253 Request with ID 7faa0d30 for model gpt2medium-355m received
2024-09-10 15:25:43,254 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:43,254 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:25:43,254 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:43,254 Updated batch size:1
2024-09-10 15:25:43,254 Loading model gpt2medium-355m
2024-09-10 15:25:43,816 Request with ID b46483c8 for model gpt2medium-355m received
2024-09-10 15:25:43,816 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:43,816 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:44,197 Request with ID 47874b90 for model gpt2medium-355m received
2024-09-10 15:25:44,197 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:44,197 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:44,614 Processed batch: ['7faa0d30'] with model gpt2medium-355m in 1.2442 seconds
2024-09-10 15:25:44,614 Latency for request 7faa0d30 with model gpt2medium-355m: 1.3607 seconds
2024-09-10 15:25:44,615 Total time: 8.4256 seconds
2024-09-10 15:25:44,615 Total inference time: 6.8876 seconds
2024-09-10 15:25:44,615 Inference time as percentage of total time: 81.75%
2024-09-10 15:25:44,615 127.0.0.1 - - [10/Sep/2024 15:25:44] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:44,615 Updated batch size:1
2024-09-10 15:25:44,615 Loading model gpt2medium-355m
2024-09-10 15:25:44,892 Request with ID c628715a for model gpt2medium-355m received
2024-09-10 15:25:44,892 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:44,892 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:25:44,892 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:45,318 Request with ID 1ade6c76 for model gpt2-124m received
2024-09-10 15:25:45,318 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:45,318 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:25:45,318 Batch size condition met for model gpt2-124m
2024-09-10 15:25:45,563 Request with ID 33701eb8 for model distilgpt2-124m received
2024-09-10 15:25:45,563 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:45,563 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:25:45,563 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:45,780 Processed batch: ['47874b90'] with model gpt2medium-355m in 1.1648 seconds
2024-09-10 15:25:45,780 Latency for request 47874b90 with model gpt2medium-355m: 1.5827 seconds
2024-09-10 15:25:45,781 Total time: 9.5917 seconds
2024-09-10 15:25:45,781 Total inference time: 8.0523 seconds
2024-09-10 15:25:45,781 Inference time as percentage of total time: 83.95%
2024-09-10 15:25:45,781 127.0.0.1 - - [10/Sep/2024 15:25:45] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:45,781 Updated batch size:1
2024-09-10 15:25:45,781 Loading model gpt2medium-355m
2024-09-10 15:25:46,200 Request with ID d8670053 for model distilgpt2-124m received
2024-09-10 15:25:46,200 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:46,200 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:46,742 Request with ID 43c3c7a4 for model distilgpt2-124m received
2024-09-10 15:25:46,742 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:46,742 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:46,819 Request with ID a688a765 for model gpt2-124m received
2024-09-10 15:25:46,819 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:46,819 Batch size condition met for model gpt2-124m
2024-09-10 15:25:46,859 Processed batch: ['c628715a'] with model gpt2medium-355m in 1.0776 seconds
2024-09-10 15:25:46,859 Latency for request c628715a with model gpt2medium-355m: 1.9668 seconds
2024-09-10 15:25:46,860 Total time: 10.6706 seconds
2024-09-10 15:25:46,860 Total inference time: 9.1299 seconds
2024-09-10 15:25:46,860 Inference time as percentage of total time: 85.56%
2024-09-10 15:25:46,860 127.0.0.1 - - [10/Sep/2024 15:25:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:46,860 No batch to process for model gpt2medium-355m
2024-09-10 15:25:46,860 127.0.0.1 - - [10/Sep/2024 15:25:46] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:46,860 Updated batch size:1
2024-09-10 15:25:46,860 Loading model gpt2-124m
2024-09-10 15:25:47,380 Processed batch: ['a688a765'] with model gpt2-124m in 0.4433 seconds
2024-09-10 15:25:47,380 Latency for request a688a765 with model gpt2-124m: 0.5603 seconds
2024-09-10 15:25:47,381 Total time: 11.1914 seconds
2024-09-10 15:25:47,381 Total inference time: 9.5732 seconds
2024-09-10 15:25:47,381 Inference time as percentage of total time: 85.54%
2024-09-10 15:25:47,381 127.0.0.1 - - [10/Sep/2024 15:25:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:47,381 Updated batch size:1
2024-09-10 15:25:47,381 Loading model distilgpt2-124m
2024-09-10 15:25:47,527 Request with ID d801edea for model gpt2medium-355m received
2024-09-10 15:25:47,527 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:47,527 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:25:47,527 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:47,773 Processed batch: ['43c3c7a4'] with model distilgpt2-124m in 0.3330 seconds
2024-09-10 15:25:47,773 Latency for request 43c3c7a4 with model distilgpt2-124m: 1.0311 seconds
2024-09-10 15:25:47,774 Total time: 11.5851 seconds
2024-09-10 15:25:47,774 Total inference time: 9.9062 seconds
2024-09-10 15:25:47,774 Inference time as percentage of total time: 85.51%
2024-09-10 15:25:47,774 127.0.0.1 - - [10/Sep/2024 15:25:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:47,774 No batch to process for model distilgpt2-124m
2024-09-10 15:25:47,775 127.0.0.1 - - [10/Sep/2024 15:25:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:47,775 No batch to process for model distilgpt2-124m
2024-09-10 15:25:47,775 127.0.0.1 - - [10/Sep/2024 15:25:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:47,775 No batch to process for model gpt2-124m
2024-09-10 15:25:47,775 127.0.0.1 - - [10/Sep/2024 15:25:47] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:47,775 Updated batch size:1
2024-09-10 15:25:47,775 Loading model gpt2medium-355m
2024-09-10 15:25:47,994 Request with ID e357926f for model gpt2medium-355m received
2024-09-10 15:25:47,994 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:47,994 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:48,055 Request with ID d18f9a97 for model gpt2medium-355m received
2024-09-10 15:25:48,055 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:48,055 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:48,945 Request with ID c1f3c2f6 for model gpt2medium-355m received
2024-09-10 15:25:48,945 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:48,945 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:49,118 Processed batch: ['d801edea'] with model gpt2medium-355m in 1.2172 seconds
2024-09-10 15:25:49,118 Latency for request d801edea with model gpt2medium-355m: 1.5912 seconds
2024-09-10 15:25:49,119 Total time: 12.9297 seconds
2024-09-10 15:25:49,119 Total inference time: 11.1234 seconds
2024-09-10 15:25:49,119 Inference time as percentage of total time: 86.03%
2024-09-10 15:25:49,119 127.0.0.1 - - [10/Sep/2024 15:25:49] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:49,119 Updated batch size:1
2024-09-10 15:25:49,119 Loading model gpt2medium-355m
2024-09-10 15:25:49,375 Request with ID 8082a97f for model gpt2-124m received
2024-09-10 15:25:49,375 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:49,375 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:25:49,375 Batch size condition met for model gpt2-124m
2024-09-10 15:25:49,732 Request with ID eb54c703 for model gpt2medium-355m received
2024-09-10 15:25:49,732 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:49,732 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:25:49,733 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:49,996 Request with ID 36e9de23 for model gpt2medium-355m received
2024-09-10 15:25:49,996 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:49,996 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:50,165 Request with ID af6b1794 for model gpt2-124m received
2024-09-10 15:25:50,165 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:50,165 Batch size condition met for model gpt2-124m
2024-09-10 15:25:50,296 Processed batch: ['c1f3c2f6'] with model gpt2medium-355m in 1.1767 seconds
2024-09-10 15:25:50,296 Latency for request c1f3c2f6 with model gpt2medium-355m: 1.3514 seconds
2024-09-10 15:25:50,297 Total time: 14.1078 seconds
2024-09-10 15:25:50,297 Total inference time: 12.3002 seconds
2024-09-10 15:25:50,297 Inference time as percentage of total time: 87.19%
2024-09-10 15:25:50,297 127.0.0.1 - - [10/Sep/2024 15:25:50] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:50,297 Updated batch size:1
2024-09-10 15:25:50,297 Loading model gpt2medium-355m
2024-09-10 15:25:50,731 Request with ID c7014d37 for model gpt2medium-355m received
2024-09-10 15:25:50,732 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:50,732 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:25:50,732 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:51,363 Request with ID a1881974 for model distilgpt2-124m received
2024-09-10 15:25:51,363 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:51,363 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:25:51,363 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:51,482 Processed batch: ['36e9de23'] with model gpt2medium-355m in 1.1845 seconds
2024-09-10 15:25:51,482 Latency for request 36e9de23 with model gpt2medium-355m: 1.4864 seconds
2024-09-10 15:25:51,483 Total time: 15.2940 seconds
2024-09-10 15:25:51,483 Total inference time: 13.4846 seconds
2024-09-10 15:25:51,483 Inference time as percentage of total time: 88.17%
2024-09-10 15:25:51,483 127.0.0.1 - - [10/Sep/2024 15:25:51] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:51,483 Updated batch size:1
2024-09-10 15:25:51,483 Loading model gpt2medium-355m
2024-09-10 15:25:51,790 Request with ID ca97e941 for model gpt2-124m received
2024-09-10 15:25:51,790 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:51,790 Batch size condition met for model gpt2-124m
2024-09-10 15:25:51,962 Request with ID af42338d for model gpt2medium-355m received
2024-09-10 15:25:51,962 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:51,962 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:25:51,962 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:52,212 Request with ID 3e4a3c0b for model gpt2-124m received
2024-09-10 15:25:52,212 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:52,213 Batch size condition met for model gpt2-124m
2024-09-10 15:25:52,224 Request with ID 0d25e575 for model gpt2-124m received
2024-09-10 15:25:52,225 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:52,225 Batch size condition met for model gpt2-124m
2024-09-10 15:25:52,656 Request with ID 5148c01f for model gpt2-124m received
2024-09-10 15:25:52,656 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:52,656 Batch size condition met for model gpt2-124m
2024-09-10 15:25:52,829 Processed batch: ['c7014d37'] with model gpt2medium-355m in 1.3455 seconds
2024-09-10 15:25:52,829 Latency for request c7014d37 with model gpt2medium-355m: 2.0976 seconds
2024-09-10 15:25:52,830 Total time: 16.6407 seconds
2024-09-10 15:25:52,830 Total inference time: 14.8302 seconds
2024-09-10 15:25:52,830 Inference time as percentage of total time: 89.12%
2024-09-10 15:25:52,830 127.0.0.1 - - [10/Sep/2024 15:25:52] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:52,830 Updated batch size:1
2024-09-10 15:25:52,830 Loading model gpt2-124m
2024-09-10 15:25:52,902 Request with ID 4ddfeb68 for model distilgpt2-124m received
2024-09-10 15:25:52,903 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:52,903 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:53,089 Request with ID 897d7ee4 for model distilgpt2-124m received
2024-09-10 15:25:53,089 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:53,089 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:53,463 Processed batch: ['5148c01f'] with model gpt2-124m in 0.5394 seconds
2024-09-10 15:25:53,463 Latency for request 5148c01f with model gpt2-124m: 0.8068 seconds
2024-09-10 15:25:53,463 Total time: 17.2743 seconds
2024-09-10 15:25:53,463 Total inference time: 15.3695 seconds
2024-09-10 15:25:53,464 Inference time as percentage of total time: 88.97%
2024-09-10 15:25:53,464 127.0.0.1 - - [10/Sep/2024 15:25:53] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:53,464 Updated batch size:1
2024-09-10 15:25:53,464 Loading model gpt2medium-355m
2024-09-10 15:25:53,682 Request with ID cbf2c2a1 for model distilgpt2-124m received
2024-09-10 15:25:53,682 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:53,682 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:53,812 Request with ID d30c1c39 for model distilgpt2-124m received
2024-09-10 15:25:53,812 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:53,812 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:54,163 Request with ID 5e48277f for model distilgpt2-124m received
2024-09-10 15:25:54,163 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:54,163 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:54,370 Request with ID 46004f75 for model distilgpt2-124m received
2024-09-10 15:25:54,370 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:54,370 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:54,607 Request with ID 2b73d647 for model distilgpt2-124m received
2024-09-10 15:25:54,607 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:54,607 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:54,842 Processed batch: ['af42338d'] with model gpt2medium-355m in 1.2428 seconds
2024-09-10 15:25:54,842 Latency for request af42338d with model gpt2medium-355m: 2.8801 seconds
2024-09-10 15:25:54,843 Total time: 18.6541 seconds
2024-09-10 15:25:54,843 Total inference time: 16.6123 seconds
2024-09-10 15:25:54,843 Inference time as percentage of total time: 89.05%
2024-09-10 15:25:54,843 127.0.0.1 - - [10/Sep/2024 15:25:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:54,843 No batch to process for model gpt2medium-355m
2024-09-10 15:25:54,844 127.0.0.1 - - [10/Sep/2024 15:25:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:54,844 No batch to process for model gpt2-124m
2024-09-10 15:25:54,844 127.0.0.1 - - [10/Sep/2024 15:25:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:54,844 No batch to process for model gpt2medium-355m
2024-09-10 15:25:54,844 127.0.0.1 - - [10/Sep/2024 15:25:54] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:54,844 Updated batch size:1
2024-09-10 15:25:54,844 Loading model distilgpt2-124m
2024-09-10 15:25:54,892 Request with ID c0a4e874 for model distilgpt2-124m received
2024-09-10 15:25:54,893 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:54,899 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:55,076 Request with ID a8af4adb for model distilgpt2-124m received
2024-09-10 15:25:55,076 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:55,076 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:55,269 Processed batch: ['2b73d647'] with model distilgpt2-124m in 0.3550 seconds
2024-09-10 15:25:55,269 Latency for request 2b73d647 with model distilgpt2-124m: 0.6621 seconds
2024-09-10 15:25:55,270 Total time: 19.0812 seconds
2024-09-10 15:25:55,270 Total inference time: 16.9673 seconds
2024-09-10 15:25:55,270 Inference time as percentage of total time: 88.92%
2024-09-10 15:25:55,271 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,271 No batch to process for model gpt2-124m
2024-09-10 15:25:55,271 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,271 No batch to process for model gpt2medium-355m
2024-09-10 15:25:55,271 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,271 No batch to process for model gpt2-124m
2024-09-10 15:25:55,271 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,271 No batch to process for model gpt2-124m
2024-09-10 15:25:55,272 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,272 No batch to process for model gpt2-124m
2024-09-10 15:25:55,272 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,272 Updated batch size:1
2024-09-10 15:25:55,272 Loading model distilgpt2-124m
2024-09-10 15:25:55,659 Processed batch: ['a8af4adb'] with model distilgpt2-124m in 0.3865 seconds
2024-09-10 15:25:55,659 Latency for request a8af4adb with model distilgpt2-124m: 0.5830 seconds
2024-09-10 15:25:55,659 Total time: 19.4703 seconds
2024-09-10 15:25:55,660 Total inference time: 17.3538 seconds
2024-09-10 15:25:55,660 Inference time as percentage of total time: 89.13%
2024-09-10 15:25:55,660 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,660 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,660 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,660 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,660 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,660 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,660 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,661 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,661 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,661 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,661 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,661 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,661 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,661 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,661 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,661 No batch to process for model distilgpt2-124m
2024-09-10 15:25:55,662 127.0.0.1 - - [10/Sep/2024 15:25:55] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:55,833 Request with ID f73e5f46 for model gpt2medium-355m received
2024-09-10 15:25:55,833 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:55,833 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:25:55,833 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:55,833 Updated batch size:1
2024-09-10 15:25:55,833 Loading model gpt2medium-355m
2024-09-10 15:25:56,077 Request with ID 855a3341 for model gpt2-124m received
2024-09-10 15:25:56,077 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:56,077 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:25:56,077 Batch size condition met for model gpt2-124m
2024-09-10 15:25:57,075 Request with ID 77444a89 for model gpt2-124m received
2024-09-10 15:25:57,075 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:57,075 Batch size condition met for model gpt2-124m
2024-09-10 15:25:57,164 Processed batch: ['f73e5f46'] with model gpt2medium-355m in 1.2070 seconds
2024-09-10 15:25:57,164 Latency for request f73e5f46 with model gpt2medium-355m: 1.3308 seconds
2024-09-10 15:25:57,165 Total time: 20.9755 seconds
2024-09-10 15:25:57,165 Total inference time: 18.5609 seconds
2024-09-10 15:25:57,165 Inference time as percentage of total time: 88.49%
2024-09-10 15:25:57,165 127.0.0.1 - - [10/Sep/2024 15:25:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:57,165 Updated batch size:1
2024-09-10 15:25:57,165 Loading model gpt2-124m
2024-09-10 15:25:57,690 Request with ID e3e3bac8 for model distilgpt2-124m received
2024-09-10 15:25:57,690 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:57,690 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 15:25:57,690 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:57,692 Processed batch: ['77444a89'] with model gpt2-124m in 0.4490 seconds
2024-09-10 15:25:57,692 Latency for request 77444a89 with model gpt2-124m: 0.6174 seconds
2024-09-10 15:25:57,693 Total time: 21.5040 seconds
2024-09-10 15:25:57,693 Total inference time: 19.0098 seconds
2024-09-10 15:25:57,693 Inference time as percentage of total time: 88.40%
2024-09-10 15:25:57,693 127.0.0.1 - - [10/Sep/2024 15:25:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:57,693 No batch to process for model gpt2-124m
2024-09-10 15:25:57,693 127.0.0.1 - - [10/Sep/2024 15:25:57] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:57,694 Updated batch size:1
2024-09-10 15:25:57,694 Loading model distilgpt2-124m
2024-09-10 15:25:58,087 Processed batch: ['e3e3bac8'] with model distilgpt2-124m in 0.3283 seconds
2024-09-10 15:25:58,087 Latency for request e3e3bac8 with model distilgpt2-124m: 0.3970 seconds
2024-09-10 15:25:58,087 Total time: 21.8982 seconds
2024-09-10 15:25:58,087 Total inference time: 19.3381 seconds
2024-09-10 15:25:58,087 Inference time as percentage of total time: 88.31%
2024-09-10 15:25:58,088 127.0.0.1 - - [10/Sep/2024 15:25:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:58,182 Request with ID cfee0dd0 for model distilgpt2-124m received
2024-09-10 15:25:58,182 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:58,182 Adjusted time limit for model distilgpt2-124m: 14.1819 seconds
2024-09-10 15:25:58,182 Batch size condition met for model distilgpt2-124m
2024-09-10 15:25:58,182 Updated batch size:1
2024-09-10 15:25:58,182 Loading model distilgpt2-124m
2024-09-10 15:25:58,333 Request with ID 36bf99d8 for model gpt2-124m received
2024-09-10 15:25:58,333 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:58,333 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:25:58,333 Batch size condition met for model gpt2-124m
2024-09-10 15:25:58,502 Processed batch: ['cfee0dd0'] with model distilgpt2-124m in 0.3191 seconds
2024-09-10 15:25:58,502 Latency for request cfee0dd0 with model distilgpt2-124m: 0.3194 seconds
2024-09-10 15:25:58,503 Total time: 22.3134 seconds
2024-09-10 15:25:58,503 Total inference time: 19.6572 seconds
2024-09-10 15:25:58,503 Inference time as percentage of total time: 88.10%
2024-09-10 15:25:58,503 127.0.0.1 - - [10/Sep/2024 15:25:58] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:58,503 Updated batch size:1
2024-09-10 15:25:58,503 Loading model gpt2-124m
2024-09-10 15:25:58,924 Request with ID 1e2a06fb for model gpt2-124m received
2024-09-10 15:25:58,925 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:58,925 Batch size condition met for model gpt2-124m
2024-09-10 15:25:59,031 Request with ID 4c20fbab for model gpt2-124m received
2024-09-10 15:25:59,031 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:59,031 Batch size condition met for model gpt2-124m
2024-09-10 15:25:59,033 Processed batch: ['36bf99d8'] with model gpt2-124m in 0.4597 seconds
2024-09-10 15:25:59,033 Latency for request 36bf99d8 with model gpt2-124m: 0.7000 seconds
2024-09-10 15:25:59,033 Total time: 22.8440 seconds
2024-09-10 15:25:59,033 Total inference time: 20.1169 seconds
2024-09-10 15:25:59,033 Inference time as percentage of total time: 88.06%
2024-09-10 15:25:59,033 127.0.0.1 - - [10/Sep/2024 15:25:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:59,033 Updated batch size:1
2024-09-10 15:25:59,033 Loading model gpt2-124m
2024-09-10 15:25:59,290 Request with ID 08d5e105 for model gpt2-124m received
2024-09-10 15:25:59,290 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:59,290 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:25:59,290 Batch size condition met for model gpt2-124m
2024-09-10 15:25:59,518 Processed batch: ['4c20fbab'] with model gpt2-124m in 0.4842 seconds
2024-09-10 15:25:59,518 Latency for request 4c20fbab with model gpt2-124m: 0.4867 seconds
2024-09-10 15:25:59,519 Total time: 23.3296 seconds
2024-09-10 15:25:59,519 Total inference time: 20.6012 seconds
2024-09-10 15:25:59,519 Inference time as percentage of total time: 88.30%
2024-09-10 15:25:59,519 127.0.0.1 - - [10/Sep/2024 15:25:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:59,519 Updated batch size:1
2024-09-10 15:25:59,519 Loading model gpt2-124m
2024-09-10 15:25:59,574 Request with ID 8d37664a for model gpt2medium-355m received
2024-09-10 15:25:59,574 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:25:59,574 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:25:59,574 Batch size condition met for model gpt2medium-355m
2024-09-10 15:25:59,921 Processed batch: ['08d5e105'] with model gpt2-124m in 0.4017 seconds
2024-09-10 15:25:59,921 Latency for request 08d5e105 with model gpt2-124m: 0.6310 seconds
2024-09-10 15:25:59,922 Total time: 23.7326 seconds
2024-09-10 15:25:59,922 Total inference time: 21.0029 seconds
2024-09-10 15:25:59,922 Inference time as percentage of total time: 88.50%
2024-09-10 15:25:59,922 127.0.0.1 - - [10/Sep/2024 15:25:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:59,922 No batch to process for model gpt2-124m
2024-09-10 15:25:59,922 127.0.0.1 - - [10/Sep/2024 15:25:59] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:25:59,922 Updated batch size:1
2024-09-10 15:25:59,922 Loading model gpt2medium-355m
2024-09-10 15:26:00,225 Request with ID bb3cc181 for model gpt2medium-355m received
2024-09-10 15:26:00,225 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:00,225 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:00,353 Request with ID 42b675ae for model gpt2-124m received
2024-09-10 15:26:00,353 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:00,353 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:26:00,353 Batch size condition met for model gpt2-124m
2024-09-10 15:26:00,541 Request with ID 471c3f68 for model gpt2-124m received
2024-09-10 15:26:00,542 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:00,542 Batch size condition met for model gpt2-124m
2024-09-10 15:26:00,940 Request with ID 4fb12290 for model distilgpt2-124m received
2024-09-10 15:26:00,940 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:00,940 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:00,940 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:01,322 Request with ID d69c09f4 for model gpt2-124m received
2024-09-10 15:26:01,323 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:01,323 Batch size condition met for model gpt2-124m
2024-09-10 15:26:01,431 Request with ID 556f16f3 for model gpt2medium-355m received
2024-09-10 15:26:01,431 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:01,431 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:01,482 Processed batch: ['8d37664a'] with model gpt2medium-355m in 1.4257 seconds
2024-09-10 15:26:01,482 Latency for request 8d37664a with model gpt2medium-355m: 1.9084 seconds
2024-09-10 15:26:01,483 Total time: 25.2937 seconds
2024-09-10 15:26:01,483 Total inference time: 22.4286 seconds
2024-09-10 15:26:01,483 Inference time as percentage of total time: 88.67%
2024-09-10 15:26:01,483 127.0.0.1 - - [10/Sep/2024 15:26:01] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:01,483 Updated batch size:1
2024-09-10 15:26:01,483 Loading model gpt2medium-355m
2024-09-10 15:26:01,764 Request with ID 46d2af56 for model distilgpt2-124m received
2024-09-10 15:26:01,765 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:01,765 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:02,010 Request with ID 06cd8cd5 for model gpt2-124m received
2024-09-10 15:26:02,010 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:02,010 Batch size condition met for model gpt2-124m
2024-09-10 15:26:02,153 Request with ID a993b1cf for model gpt2medium-355m received
2024-09-10 15:26:02,153 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:02,153 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:02,153 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:02,799 Processed batch: ['556f16f3'] with model gpt2medium-355m in 1.3154 seconds
2024-09-10 15:26:02,799 Latency for request 556f16f3 with model gpt2medium-355m: 1.3680 seconds
2024-09-10 15:26:02,800 Total time: 26.6104 seconds
2024-09-10 15:26:02,800 Total inference time: 23.7439 seconds
2024-09-10 15:26:02,800 Inference time as percentage of total time: 89.23%
2024-09-10 15:26:02,800 127.0.0.1 - - [10/Sep/2024 15:26:02] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:02,800 Updated batch size:1
2024-09-10 15:26:02,800 Loading model gpt2-124m
2024-09-10 15:26:02,864 Request with ID d62bb579 for model gpt2-124m received
2024-09-10 15:26:02,865 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:02,865 Batch size condition met for model gpt2-124m
2024-09-10 15:26:03,401 Processed batch: ['06cd8cd5'] with model gpt2-124m in 0.5166 seconds
2024-09-10 15:26:03,401 Latency for request 06cd8cd5 with model gpt2-124m: 1.3914 seconds
2024-09-10 15:26:03,402 Total time: 27.2126 seconds
2024-09-10 15:26:03,402 Total inference time: 24.2605 seconds
2024-09-10 15:26:03,402 Inference time as percentage of total time: 89.15%
2024-09-10 15:26:03,402 127.0.0.1 - - [10/Sep/2024 15:26:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:03,402 Updated batch size:1
2024-09-10 15:26:03,402 Loading model gpt2-124m
2024-09-10 15:26:03,479 Request with ID 2a8f6ca6 for model gpt2-124m received
2024-09-10 15:26:03,479 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:03,479 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:26:03,479 Batch size condition met for model gpt2-124m
2024-09-10 15:26:03,756 Request with ID 2f3407ea for model gpt2medium-355m received
2024-09-10 15:26:03,756 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:03,756 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:26:03,756 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:03,845 Processed batch: ['d62bb579'] with model gpt2-124m in 0.4428 seconds
2024-09-10 15:26:03,845 Latency for request d62bb579 with model gpt2-124m: 0.9807 seconds
2024-09-10 15:26:03,846 Total time: 27.6567 seconds
2024-09-10 15:26:03,846 Total inference time: 24.7033 seconds
2024-09-10 15:26:03,846 Inference time as percentage of total time: 89.32%
2024-09-10 15:26:03,846 127.0.0.1 - - [10/Sep/2024 15:26:03] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:03,846 Updated batch size:1
2024-09-10 15:26:03,846 Loading model distilgpt2-124m
2024-09-10 15:26:04,003 Request with ID 3bb62671 for model gpt2medium-355m received
2024-09-10 15:26:04,003 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:04,003 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:04,275 Processed batch: ['46d2af56'] with model distilgpt2-124m in 0.3672 seconds
2024-09-10 15:26:04,275 Latency for request 46d2af56 with model distilgpt2-124m: 2.5107 seconds
2024-09-10 15:26:04,276 Total time: 28.0868 seconds
2024-09-10 15:26:04,276 Total inference time: 25.0704 seconds
2024-09-10 15:26:04,276 Inference time as percentage of total time: 89.26%
2024-09-10 15:26:04,276 127.0.0.1 - - [10/Sep/2024 15:26:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:04,276 Updated batch size:1
2024-09-10 15:26:04,276 Loading model gpt2-124m
2024-09-10 15:26:04,485 Request with ID d8a65468 for model distilgpt2-124m received
2024-09-10 15:26:04,486 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:04,486 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 15:26:04,486 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:04,818 Request with ID 75a8324a for model gpt2medium-355m received
2024-09-10 15:26:04,818 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:04,818 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:04,926 Processed batch: ['2a8f6ca6'] with model gpt2-124m in 0.5745 seconds
2024-09-10 15:26:04,926 Latency for request 2a8f6ca6 with model gpt2-124m: 1.4470 seconds
2024-09-10 15:26:04,927 Total time: 28.7377 seconds
2024-09-10 15:26:04,927 Total inference time: 25.6449 seconds
2024-09-10 15:26:04,927 Inference time as percentage of total time: 89.24%
2024-09-10 15:26:04,927 127.0.0.1 - - [10/Sep/2024 15:26:04] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:04,927 Updated batch size:1
2024-09-10 15:26:04,927 Loading model gpt2medium-355m
2024-09-10 15:26:05,203 Request with ID ea9d4336 for model distilgpt2-124m received
2024-09-10 15:26:05,203 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:05,203 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:05,507 Request with ID 0f61b410 for model distilgpt2-124m received
2024-09-10 15:26:05,507 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:05,507 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:06,071 Request with ID 9d3bef49 for model gpt2-124m received
2024-09-10 15:26:06,071 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:06,072 Adjusted time limit for model gpt2-124m: 13.6819 seconds
2024-09-10 15:26:06,072 Batch size condition met for model gpt2-124m
2024-09-10 15:26:06,216 Request with ID 396f0563 for model distilgpt2-124m received
2024-09-10 15:26:06,217 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:06,217 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:06,419 Request with ID cabd3ec3 for model distilgpt2-124m received
2024-09-10 15:26:06,420 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:06,420 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:06,485 Processed batch: ['75a8324a'] with model gpt2medium-355m in 1.4204 seconds
2024-09-10 15:26:06,485 Latency for request 75a8324a with model gpt2medium-355m: 1.6670 seconds
2024-09-10 15:26:06,486 Total time: 30.2964 seconds
2024-09-10 15:26:06,486 Total inference time: 27.0653 seconds
2024-09-10 15:26:06,486 Inference time as percentage of total time: 89.34%
2024-09-10 15:26:06,486 127.0.0.1 - - [10/Sep/2024 15:26:06] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:06,486 Updated batch size:1
2024-09-10 15:26:06,486 Loading model distilgpt2-124m
2024-09-10 15:26:06,542 Request with ID 753407ae for model distilgpt2-124m received
2024-09-10 15:26:06,543 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:06,545 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:06,921 Processed batch: ['cabd3ec3'] with model distilgpt2-124m in 0.3684 seconds
2024-09-10 15:26:06,921 Latency for request cabd3ec3 with model distilgpt2-124m: 0.5018 seconds
2024-09-10 15:26:06,922 Total time: 30.7328 seconds
2024-09-10 15:26:06,922 Total inference time: 27.4338 seconds
2024-09-10 15:26:06,922 Inference time as percentage of total time: 89.27%
2024-09-10 15:26:06,922 127.0.0.1 - - [10/Sep/2024 15:26:06] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:06,922 Updated batch size:1
2024-09-10 15:26:06,922 Loading model gpt2-124m
2024-09-10 15:26:07,375 Request with ID 6ef463ae for model distilgpt2-124m received
2024-09-10 15:26:07,375 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:07,375 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 15:26:07,375 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:07,465 Request with ID b332cf2a for model distilgpt2-124m received
2024-09-10 15:26:07,465 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:07,465 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:07,611 Request with ID c7942de9 for model gpt2medium-355m received
2024-09-10 15:26:07,611 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:07,611 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:26:07,612 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:07,631 Processed batch: ['9d3bef49'] with model gpt2-124m in 0.6292 seconds
2024-09-10 15:26:07,631 Latency for request 9d3bef49 with model gpt2-124m: 1.5597 seconds
2024-09-10 15:26:07,632 Total time: 31.4427 seconds
2024-09-10 15:26:07,632 Total inference time: 28.0629 seconds
2024-09-10 15:26:07,632 Inference time as percentage of total time: 89.25%
2024-09-10 15:26:07,632 127.0.0.1 - - [10/Sep/2024 15:26:07] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:07,632 Updated batch size:1
2024-09-10 15:26:07,632 Loading model gpt2medium-355m
2024-09-10 15:26:07,736 Request with ID 6ebd09e8 for model gpt2-124m received
2024-09-10 15:26:07,736 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:07,736 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 15:26:07,736 Batch size condition met for model gpt2-124m
2024-09-10 15:26:08,204 Request with ID a8981145 for model gpt2-124m received
2024-09-10 15:26:08,204 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:08,204 Batch size condition met for model gpt2-124m
2024-09-10 15:26:08,321 Request with ID deec3ef2 for model gpt2-124m received
2024-09-10 15:26:08,321 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:08,321 Batch size condition met for model gpt2-124m
2024-09-10 15:26:08,399 Request with ID 591678f1 for model gpt2medium-355m received
2024-09-10 15:26:08,399 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:08,399 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:08,638 Request with ID 8c5d57a7 for model gpt2-124m received
2024-09-10 15:26:08,638 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:08,638 Batch size condition met for model gpt2-124m
2024-09-10 15:26:09,080 Processed batch: ['c7942de9'] with model gpt2medium-355m in 1.3096 seconds
2024-09-10 15:26:09,080 Latency for request c7942de9 with model gpt2medium-355m: 1.4689 seconds
2024-09-10 15:26:09,081 Total time: 32.8919 seconds
2024-09-10 15:26:09,081 Total inference time: 29.3725 seconds
2024-09-10 15:26:09,081 Inference time as percentage of total time: 89.30%
2024-09-10 15:26:09,081 127.0.0.1 - - [10/Sep/2024 15:26:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:09,081 Updated batch size:1
2024-09-10 15:26:09,082 Loading model gpt2-124m
2024-09-10 15:26:09,151 Request with ID e52aceca for model distilgpt2-124m received
2024-09-10 15:26:09,152 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:09,152 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:09,650 Request with ID 7cba80e7 for model gpt2-124m received
2024-09-10 15:26:09,651 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:09,651 Batch size condition met for model gpt2-124m
2024-09-10 15:26:09,668 Processed batch: ['8c5d57a7'] with model gpt2-124m in 0.5059 seconds
2024-09-10 15:26:09,668 Latency for request 8c5d57a7 with model gpt2-124m: 1.0294 seconds
2024-09-10 15:26:09,669 Total time: 33.4796 seconds
2024-09-10 15:26:09,669 Total inference time: 29.8784 seconds
2024-09-10 15:26:09,669 Inference time as percentage of total time: 89.24%
2024-09-10 15:26:09,669 127.0.0.1 - - [10/Sep/2024 15:26:09] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:09,669 Updated batch size:1
2024-09-10 15:26:09,669 Loading model gpt2-124m
2024-09-10 15:26:09,752 Request with ID 7e7c8a67 for model gpt2medium-355m received
2024-09-10 15:26:09,753 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:09,753 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:26:09,753 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:09,925 Request with ID 944a9ff1 for model distilgpt2-124m received
2024-09-10 15:26:09,926 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:09,926 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:10,259 Processed batch: ['7cba80e7'] with model gpt2-124m in 0.5898 seconds
2024-09-10 15:26:10,259 Latency for request 7cba80e7 with model gpt2-124m: 0.6089 seconds
2024-09-10 15:26:10,261 Total time: 34.0713 seconds
2024-09-10 15:26:10,261 Total inference time: 30.4682 seconds
2024-09-10 15:26:10,261 Inference time as percentage of total time: 89.42%
2024-09-10 15:26:10,261 127.0.0.1 - - [10/Sep/2024 15:26:10] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:10,261 Updated batch size:1
2024-09-10 15:26:10,261 Loading model gpt2medium-355m
2024-09-10 15:26:10,382 Request with ID c02dfca4 for model distilgpt2-124m received
2024-09-10 15:26:10,382 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:10,383 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:10,690 Request with ID af2df702 for model distilgpt2-124m received
2024-09-10 15:26:10,690 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:10,690 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:10,914 Request with ID 24369dac for model gpt2medium-355m received
2024-09-10 15:26:10,914 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:10,914 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:11,806 Processed batch: ['7e7c8a67'] with model gpt2medium-355m in 1.3976 seconds
2024-09-10 15:26:11,806 Latency for request 7e7c8a67 with model gpt2medium-355m: 2.0535 seconds
2024-09-10 15:26:11,808 Total time: 35.6184 seconds
2024-09-10 15:26:11,808 Total inference time: 31.8658 seconds
2024-09-10 15:26:11,808 Inference time as percentage of total time: 89.46%
2024-09-10 15:26:11,808 127.0.0.1 - - [10/Sep/2024 15:26:11] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:11,808 Updated batch size:1
2024-09-10 15:26:11,808 Loading model gpt2medium-355m
2024-09-10 15:26:11,952 Request with ID 32527a18 for model distilgpt2-124m received
2024-09-10 15:26:11,953 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:11,953 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:12,141 Request with ID 61eafcd6 for model gpt2medium-355m received
2024-09-10 15:26:12,141 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:12,141 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:12,141 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:12,966 Processed batch: ['24369dac'] with model gpt2medium-355m in 1.1576 seconds
2024-09-10 15:26:12,966 Latency for request 24369dac with model gpt2medium-355m: 2.0519 seconds
2024-09-10 15:26:12,967 Total time: 36.7775 seconds
2024-09-10 15:26:12,967 Total inference time: 33.0234 seconds
2024-09-10 15:26:12,967 Inference time as percentage of total time: 89.79%
2024-09-10 15:26:12,967 127.0.0.1 - - [10/Sep/2024 15:26:12] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:12,967 Updated batch size:1
2024-09-10 15:26:12,967 Loading model distilgpt2-124m
2024-09-10 15:26:13,171 Request with ID da097cf8 for model distilgpt2-124m received
2024-09-10 15:26:13,171 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:13,171 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:13,394 Processed batch: ['32527a18'] with model distilgpt2-124m in 0.3627 seconds
2024-09-10 15:26:13,394 Latency for request 32527a18 with model distilgpt2-124m: 1.4416 seconds
2024-09-10 15:26:13,395 Total time: 37.2057 seconds
2024-09-10 15:26:13,395 Total inference time: 33.3861 seconds
2024-09-10 15:26:13,395 Inference time as percentage of total time: 89.73%
2024-09-10 15:26:13,395 127.0.0.1 - - [10/Sep/2024 15:26:13] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:13,395 Updated batch size:1
2024-09-10 15:26:13,395 Loading model gpt2medium-355m
2024-09-10 15:26:13,623 Request with ID e3c8d3a5 for model distilgpt2-124m received
2024-09-10 15:26:13,623 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:13,623 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:13,623 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:14,860 Request with ID f57a912d for model gpt2medium-355m received
2024-09-10 15:26:14,860 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:14,860 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:14,861 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:14,935 Processed batch: ['61eafcd6'] with model gpt2medium-355m in 1.4083 seconds
2024-09-10 15:26:14,935 Latency for request 61eafcd6 with model gpt2medium-355m: 2.7938 seconds
2024-09-10 15:26:14,937 Total time: 38.7473 seconds
2024-09-10 15:26:14,937 Total inference time: 34.7944 seconds
2024-09-10 15:26:14,937 Inference time as percentage of total time: 89.80%
2024-09-10 15:26:14,937 127.0.0.1 - - [10/Sep/2024 15:26:14] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:14,937 Updated batch size:1
2024-09-10 15:26:14,937 Loading model distilgpt2-124m
2024-09-10 15:26:15,363 Processed batch: ['e3c8d3a5'] with model distilgpt2-124m in 0.3621 seconds
2024-09-10 15:26:15,364 Latency for request e3c8d3a5 with model distilgpt2-124m: 1.7406 seconds
2024-09-10 15:26:15,364 Total time: 39.1750 seconds
2024-09-10 15:26:15,364 Total inference time: 35.1565 seconds
2024-09-10 15:26:15,364 Inference time as percentage of total time: 89.74%
2024-09-10 15:26:15,364 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,364 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,365 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,365 No batch to process for model gpt2-124m
2024-09-10 15:26:15,365 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,365 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,365 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,365 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,365 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,365 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,366 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,366 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,366 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,366 No batch to process for model distilgpt2-124m
2024-09-10 15:26:15,366 127.0.0.1 - - [10/Sep/2024 15:26:15] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:15,366 Updated batch size:1
2024-09-10 15:26:15,366 Loading model gpt2medium-355m
2024-09-10 15:26:15,538 Request with ID 1bdc48e5 for model gpt2-124m received
2024-09-10 15:26:15,538 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:15,538 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 15:26:15,538 Batch size condition met for model gpt2-124m
2024-09-10 15:26:15,792 Request with ID 95d7a90a for model gpt2-124m received
2024-09-10 15:26:15,793 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:15,793 Batch size condition met for model gpt2-124m
2024-09-10 15:26:15,958 Request with ID 81c52f6a for model distilgpt2-124m received
2024-09-10 15:26:15,958 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:15,958 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:15,958 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:16,235 Request with ID 8f465a87 for model gpt2medium-355m received
2024-09-10 15:26:16,235 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:16,236 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:16,236 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:16,907 Processed batch: ['f57a912d'] with model gpt2medium-355m in 1.3551 seconds
2024-09-10 15:26:16,907 Latency for request f57a912d with model gpt2medium-355m: 2.0465 seconds
2024-09-10 15:26:16,908 Total time: 40.7187 seconds
2024-09-10 15:26:16,908 Total inference time: 36.5116 seconds
2024-09-10 15:26:16,908 Inference time as percentage of total time: 89.67%
2024-09-10 15:26:16,908 127.0.0.1 - - [10/Sep/2024 15:26:16] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:16,908 Updated batch size:1
2024-09-10 15:26:16,908 Loading model gpt2-124m
2024-09-10 15:26:17,325 Request with ID 6a117101 for model distilgpt2-124m received
2024-09-10 15:26:17,325 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:17,325 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:17,517 Processed batch: ['95d7a90a'] with model gpt2-124m in 0.5275 seconds
2024-09-10 15:26:17,517 Latency for request 95d7a90a with model gpt2-124m: 1.7240 seconds
2024-09-10 15:26:17,518 Total time: 41.3285 seconds
2024-09-10 15:26:17,518 Total inference time: 37.0391 seconds
2024-09-10 15:26:17,518 Inference time as percentage of total time: 89.62%
2024-09-10 15:26:17,518 127.0.0.1 - - [10/Sep/2024 15:26:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:17,518 No batch to process for model gpt2-124m
2024-09-10 15:26:17,518 127.0.0.1 - - [10/Sep/2024 15:26:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:17,518 No batch to process for model gpt2-124m
2024-09-10 15:26:17,518 127.0.0.1 - - [10/Sep/2024 15:26:17] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:17,518 Updated batch size:1
2024-09-10 15:26:17,518 Loading model gpt2medium-355m
2024-09-10 15:26:17,564 Request with ID cfa76554 for model gpt2medium-355m received
2024-09-10 15:26:17,564 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:17,593 Adjusted time limit for model gpt2medium-355m: 11.8866 seconds
2024-09-10 15:26:17,615 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:17,934 Request with ID 60dad374 for model distilgpt2-124m received
2024-09-10 15:26:17,935 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:17,935 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:18,589 Request with ID 95b5bb5d for model gpt2medium-355m received
2024-09-10 15:26:18,589 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:18,589 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:19,137 Processed batch: ['8f465a87'] with model gpt2medium-355m in 1.4657 seconds
2024-09-10 15:26:19,138 Latency for request 8f465a87 with model gpt2medium-355m: 2.9021 seconds
2024-09-10 15:26:19,138 Total time: 42.9490 seconds
2024-09-10 15:26:19,138 Total inference time: 38.5048 seconds
2024-09-10 15:26:19,138 Inference time as percentage of total time: 89.65%
2024-09-10 15:26:19,139 127.0.0.1 - - [10/Sep/2024 15:26:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:19,139 No batch to process for model gpt2-124m
2024-09-10 15:26:19,139 127.0.0.1 - - [10/Sep/2024 15:26:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:19,139 Updated batch size:1
2024-09-10 15:26:19,139 Loading model distilgpt2-124m
2024-09-10 15:26:19,502 Request with ID 0d9bccb7 for model distilgpt2-124m received
2024-09-10 15:26:19,503 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:19,503 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:19,547 Request with ID 4075b8e3 for model gpt2-124m received
2024-09-10 15:26:19,547 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:19,547 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:26:19,547 Batch size condition met for model gpt2-124m
2024-09-10 15:26:19,580 Processed batch: ['60dad374'] with model distilgpt2-124m in 0.3755 seconds
2024-09-10 15:26:19,580 Latency for request 60dad374 with model distilgpt2-124m: 1.6457 seconds
2024-09-10 15:26:19,582 Total time: 43.3923 seconds
2024-09-10 15:26:19,582 Total inference time: 38.8803 seconds
2024-09-10 15:26:19,582 Inference time as percentage of total time: 89.60%
2024-09-10 15:26:19,582 127.0.0.1 - - [10/Sep/2024 15:26:19] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:19,582 Updated batch size:1
2024-09-10 15:26:19,582 Loading model gpt2-124m
2024-09-10 15:26:20,136 Processed batch: ['4075b8e3'] with model gpt2-124m in 0.4793 seconds
2024-09-10 15:26:20,136 Latency for request 4075b8e3 with model gpt2-124m: 0.5889 seconds
2024-09-10 15:26:20,137 Total time: 43.9479 seconds
2024-09-10 15:26:20,137 Total inference time: 39.3597 seconds
2024-09-10 15:26:20,137 Inference time as percentage of total time: 89.56%
2024-09-10 15:26:20,137 127.0.0.1 - - [10/Sep/2024 15:26:20] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:20,137 Updated batch size:1
2024-09-10 15:26:20,138 Loading model gpt2medium-355m
2024-09-10 15:26:20,334 Request with ID 35edd9ce for model distilgpt2-124m received
2024-09-10 15:26:20,334 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:20,334 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:20,334 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:21,057 Request with ID c64376ee for model gpt2medium-355m received
2024-09-10 15:26:21,057 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:21,057 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:21,057 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:21,521 Request with ID c2e49a6b for model distilgpt2-124m received
2024-09-10 15:26:21,521 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:21,521 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:21,659 Processed batch: ['95b5bb5d'] with model gpt2medium-355m in 1.3868 seconds
2024-09-10 15:26:21,659 Latency for request 95b5bb5d with model gpt2medium-355m: 3.0704 seconds
2024-09-10 15:26:21,660 Total time: 45.4708 seconds
2024-09-10 15:26:21,660 Total inference time: 40.7464 seconds
2024-09-10 15:26:21,660 Inference time as percentage of total time: 89.61%
2024-09-10 15:26:21,660 127.0.0.1 - - [10/Sep/2024 15:26:21] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:21,660 Updated batch size:1
2024-09-10 15:26:21,660 Loading model distilgpt2-124m
2024-09-10 15:26:21,934 Request with ID cdb9ae3d for model distilgpt2-124m received
2024-09-10 15:26:21,934 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:21,934 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:22,123 Processed batch: ['c2e49a6b'] with model distilgpt2-124m in 0.3952 seconds
2024-09-10 15:26:22,123 Latency for request c2e49a6b with model distilgpt2-124m: 0.6022 seconds
2024-09-10 15:26:22,125 Total time: 45.9359 seconds
2024-09-10 15:26:22,125 Total inference time: 41.1417 seconds
2024-09-10 15:26:22,125 Inference time as percentage of total time: 89.56%
2024-09-10 15:26:22,125 127.0.0.1 - - [10/Sep/2024 15:26:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:22,125 Updated batch size:1
2024-09-10 15:26:22,126 Loading model distilgpt2-124m
2024-09-10 15:26:22,133 Request with ID a0d84c63 for model gpt2medium-355m received
2024-09-10 15:26:22,133 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:22,133 Adjusted time limit for model gpt2medium-355m: 11.8803 seconds
2024-09-10 15:26:22,133 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:22,238 Request with ID 4be3fcfe for model distilgpt2-124m received
2024-09-10 15:26:22,238 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:22,238 Adjusted time limit for model distilgpt2-124m: 14.1819 seconds
2024-09-10 15:26:22,238 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:22,402 Request with ID 47385ad9 for model gpt2medium-355m received
2024-09-10 15:26:22,402 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:22,402 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:22,475 Processed batch: ['cdb9ae3d'] with model distilgpt2-124m in 0.3497 seconds
2024-09-10 15:26:22,475 Latency for request cdb9ae3d with model distilgpt2-124m: 0.5417 seconds
2024-09-10 15:26:22,476 Total time: 46.2869 seconds
2024-09-10 15:26:22,476 Total inference time: 41.4914 seconds
2024-09-10 15:26:22,476 Inference time as percentage of total time: 89.64%
2024-09-10 15:26:22,476 127.0.0.1 - - [10/Sep/2024 15:26:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:22,476 Updated batch size:1
2024-09-10 15:26:22,477 Loading model distilgpt2-124m
2024-09-10 15:26:22,800 Processed batch: ['4be3fcfe'] with model distilgpt2-124m in 0.3231 seconds
2024-09-10 15:26:22,800 Latency for request 4be3fcfe with model distilgpt2-124m: 0.5615 seconds
2024-09-10 15:26:22,801 Total time: 46.6113 seconds
2024-09-10 15:26:22,801 Total inference time: 41.8145 seconds
2024-09-10 15:26:22,801 Inference time as percentage of total time: 89.71%
2024-09-10 15:26:22,801 127.0.0.1 - - [10/Sep/2024 15:26:22] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:22,801 Updated batch size:1
2024-09-10 15:26:22,801 Loading model gpt2medium-355m
2024-09-10 15:26:22,911 Request with ID 89b465b1 for model gpt2-124m received
2024-09-10 15:26:22,911 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:22,911 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 15:26:22,911 Batch size condition met for model gpt2-124m
2024-09-10 15:26:23,181 Request with ID 4acbb357 for model gpt2-124m received
2024-09-10 15:26:23,181 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:23,182 Batch size condition met for model gpt2-124m
2024-09-10 15:26:23,235 Request with ID 124033bc for model gpt2medium-355m received
2024-09-10 15:26:23,235 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:23,235 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:23,343 Request with ID de19ac38 for model gpt2-124m received
2024-09-10 15:26:23,344 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:23,344 Batch size condition met for model gpt2-124m
2024-09-10 15:26:23,917 Request with ID 36a42452 for model gpt2medium-355m received
2024-09-10 15:26:23,917 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:23,917 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:24,039 Request with ID 2e140ea4 for model gpt2-124m received
2024-09-10 15:26:24,039 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:24,039 Batch size condition met for model gpt2-124m
2024-09-10 15:26:24,057 Request with ID bc1ceab3 for model distilgpt2-124m received
2024-09-10 15:26:24,057 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:24,058 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:24,058 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:24,372 Request with ID f3955212 for model gpt2medium-355m received
2024-09-10 15:26:24,373 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:24,373 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:24,457 Processed batch: ['47385ad9'] with model gpt2medium-355m in 1.5081 seconds
2024-09-10 15:26:24,457 Latency for request 47385ad9 with model gpt2medium-355m: 2.0546 seconds
2024-09-10 15:26:24,458 Total time: 48.2686 seconds
2024-09-10 15:26:24,458 Total inference time: 43.3225 seconds
2024-09-10 15:26:24,458 Inference time as percentage of total time: 89.75%
2024-09-10 15:26:24,458 127.0.0.1 - - [10/Sep/2024 15:26:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:24,458 Updated batch size:1
2024-09-10 15:26:24,458 Loading model distilgpt2-124m
2024-09-10 15:26:24,819 Request with ID 42b4e6ea for model distilgpt2-124m received
2024-09-10 15:26:24,819 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:24,819 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:24,881 Processed batch: ['bc1ceab3'] with model distilgpt2-124m in 0.3459 seconds
2024-09-10 15:26:24,881 Latency for request bc1ceab3 with model distilgpt2-124m: 0.8237 seconds
2024-09-10 15:26:24,882 Total time: 48.6926 seconds
2024-09-10 15:26:24,882 Total inference time: 43.6684 seconds
2024-09-10 15:26:24,882 Inference time as percentage of total time: 89.68%
2024-09-10 15:26:24,882 127.0.0.1 - - [10/Sep/2024 15:26:24] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:24,882 Updated batch size:1
2024-09-10 15:26:24,882 Loading model gpt2medium-355m
2024-09-10 15:26:25,122 Request with ID 594047b9 for model distilgpt2-124m received
2024-09-10 15:26:25,122 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:25,122 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:25,122 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:25,175 Request with ID d92b565a for model gpt2-124m received
2024-09-10 15:26:25,175 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:25,176 Batch size condition met for model gpt2-124m
2024-09-10 15:26:25,407 Request with ID 32d314d8 for model gpt2-124m received
2024-09-10 15:26:25,407 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:25,407 Batch size condition met for model gpt2-124m
2024-09-10 15:26:25,592 Request with ID 49169154 for model gpt2-124m received
2024-09-10 15:26:25,592 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:25,592 Batch size condition met for model gpt2-124m
2024-09-10 15:26:25,925 Request with ID 90ecc144 for model gpt2-124m received
2024-09-10 15:26:25,925 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:25,925 Batch size condition met for model gpt2-124m
2024-09-10 15:26:26,089 Request with ID 5e2c4610 for model gpt2medium-355m received
2024-09-10 15:26:26,089 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:26,089 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:26,089 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:26,347 Request with ID ebe0e61d for model gpt2medium-355m received
2024-09-10 15:26:26,347 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:26,347 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:26,431 Request with ID e2d8971a for model gpt2-124m received
2024-09-10 15:26:26,431 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:26,432 Batch size condition met for model gpt2-124m
2024-09-10 15:26:26,485 Processed batch: ['f3955212'] with model gpt2medium-355m in 1.4676 seconds
2024-09-10 15:26:26,485 Latency for request f3955212 with model gpt2medium-355m: 2.1122 seconds
2024-09-10 15:26:26,486 Total time: 50.2962 seconds
2024-09-10 15:26:26,486 Total inference time: 45.1360 seconds
2024-09-10 15:26:26,486 Inference time as percentage of total time: 89.74%
2024-09-10 15:26:26,486 127.0.0.1 - - [10/Sep/2024 15:26:26] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:26,486 Updated batch size:1
2024-09-10 15:26:26,486 Loading model distilgpt2-124m
2024-09-10 15:26:26,678 Request with ID 1935cbc1 for model gpt2-124m received
2024-09-10 15:26:26,678 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:26,678 Batch size condition met for model gpt2-124m
2024-09-10 15:26:26,928 Processed batch: ['594047b9'] with model distilgpt2-124m in 0.3751 seconds
2024-09-10 15:26:26,928 Latency for request 594047b9 with model distilgpt2-124m: 1.8057 seconds
2024-09-10 15:26:26,929 Total time: 50.7394 seconds
2024-09-10 15:26:26,929 Total inference time: 45.5111 seconds
2024-09-10 15:26:26,929 Inference time as percentage of total time: 89.70%
2024-09-10 15:26:26,929 127.0.0.1 - - [10/Sep/2024 15:26:26] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:26,929 No batch to process for model distilgpt2-124m
2024-09-10 15:26:26,929 127.0.0.1 - - [10/Sep/2024 15:26:26] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:26,929 Updated batch size:1
2024-09-10 15:26:26,929 Loading model gpt2medium-355m
2024-09-10 15:26:27,407 Request with ID 68ba0c45 for model gpt2-124m received
2024-09-10 15:26:27,407 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:27,407 Batch size condition met for model gpt2-124m
2024-09-10 15:26:27,633 Request with ID 785a843a for model gpt2medium-355m received
2024-09-10 15:26:27,633 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:27,633 Adjusted time limit for model gpt2medium-355m: 11.8759 seconds
2024-09-10 15:26:27,634 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:27,815 Request with ID 288f9fff for model gpt2medium-355m received
2024-09-10 15:26:27,815 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:27,815 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:28,098 Request with ID 45b97d0c for model gpt2-124m received
2024-09-10 15:26:28,098 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:28,098 Batch size condition met for model gpt2-124m
2024-09-10 15:26:28,361 Request with ID f573bc92 for model distilgpt2-124m received
2024-09-10 15:26:28,361 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:28,361 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:28,361 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:28,448 Processed batch: ['ebe0e61d'] with model gpt2medium-355m in 1.3793 seconds
2024-09-10 15:26:28,448 Latency for request ebe0e61d with model gpt2medium-355m: 2.1005 seconds
2024-09-10 15:26:28,449 Total time: 52.2594 seconds
2024-09-10 15:26:28,449 Total inference time: 46.8904 seconds
2024-09-10 15:26:28,449 Inference time as percentage of total time: 89.73%
2024-09-10 15:26:28,449 127.0.0.1 - - [10/Sep/2024 15:26:28] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:28,449 Updated batch size:1
2024-09-10 15:26:28,449 Loading model gpt2-124m
2024-09-10 15:26:28,694 Request with ID bdab2710 for model gpt2medium-355m received
2024-09-10 15:26:28,694 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:28,694 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:26:28,694 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:29,003 Processed batch: ['45b97d0c'] with model gpt2-124m in 0.4741 seconds
2024-09-10 15:26:29,003 Latency for request 45b97d0c with model gpt2-124m: 0.9058 seconds
2024-09-10 15:26:29,004 Total time: 52.8149 seconds
2024-09-10 15:26:29,004 Total inference time: 47.3644 seconds
2024-09-10 15:26:29,004 Inference time as percentage of total time: 89.68%
2024-09-10 15:26:29,004 127.0.0.1 - - [10/Sep/2024 15:26:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:29,004 No batch to process for model gpt2-124m
2024-09-10 15:26:29,005 127.0.0.1 - - [10/Sep/2024 15:26:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:29,005 Updated batch size:1
2024-09-10 15:26:29,005 Loading model distilgpt2-124m
2024-09-10 15:26:29,118 Request with ID 52e59e2f for model gpt2-124m received
2024-09-10 15:26:29,118 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:29,119 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:26:29,119 Batch size condition met for model gpt2-124m
2024-09-10 15:26:29,282 Request with ID d8538874 for model distilgpt2-124m received
2024-09-10 15:26:29,282 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:29,282 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:29,366 Request with ID 12b2fdc5 for model distilgpt2-124m received
2024-09-10 15:26:29,366 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:29,366 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:29,450 Processed batch: ['f573bc92'] with model distilgpt2-124m in 0.3804 seconds
2024-09-10 15:26:29,450 Latency for request f573bc92 with model distilgpt2-124m: 1.0894 seconds
2024-09-10 15:26:29,451 Total time: 53.2620 seconds
2024-09-10 15:26:29,451 Total inference time: 47.7448 seconds
2024-09-10 15:26:29,451 Inference time as percentage of total time: 89.64%
2024-09-10 15:26:29,452 127.0.0.1 - - [10/Sep/2024 15:26:29] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:29,452 Updated batch size:1
2024-09-10 15:26:29,452 Loading model gpt2medium-355m
2024-09-10 15:26:29,573 Request with ID ee1c53d4 for model gpt2-124m received
2024-09-10 15:26:29,574 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:29,574 Batch size condition met for model gpt2-124m
2024-09-10 15:26:29,922 Request with ID b5e1fd7e for model distilgpt2-124m received
2024-09-10 15:26:29,923 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:29,923 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:29,923 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:30,261 Request with ID 531923ba for model gpt2-124m received
2024-09-10 15:26:30,261 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:30,262 Batch size condition met for model gpt2-124m
2024-09-10 15:26:30,997 Processed batch: ['bdab2710'] with model gpt2medium-355m in 1.3873 seconds
2024-09-10 15:26:30,997 Latency for request bdab2710 with model gpt2medium-355m: 2.3027 seconds
2024-09-10 15:26:30,998 Total time: 54.8084 seconds
2024-09-10 15:26:30,998 Total inference time: 49.1321 seconds
2024-09-10 15:26:30,998 Inference time as percentage of total time: 89.64%
2024-09-10 15:26:30,998 127.0.0.1 - - [10/Sep/2024 15:26:30] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:30,998 Updated batch size:1
2024-09-10 15:26:30,998 Loading model distilgpt2-124m
2024-09-10 15:26:31,128 Request with ID c6c5cddf for model gpt2-124m received
2024-09-10 15:26:31,128 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:31,128 Batch size condition met for model gpt2-124m
2024-09-10 15:26:31,436 Processed batch: ['b5e1fd7e'] with model distilgpt2-124m in 0.3709 seconds
2024-09-10 15:26:31,436 Latency for request b5e1fd7e with model distilgpt2-124m: 1.5133 seconds
2024-09-10 15:26:31,437 Total time: 55.2481 seconds
2024-09-10 15:26:31,437 Total inference time: 49.5031 seconds
2024-09-10 15:26:31,437 Inference time as percentage of total time: 89.60%
2024-09-10 15:26:31,438 127.0.0.1 - - [10/Sep/2024 15:26:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:31,438 No batch to process for model gpt2medium-355m
2024-09-10 15:26:31,438 127.0.0.1 - - [10/Sep/2024 15:26:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:31,438 No batch to process for model distilgpt2-124m
2024-09-10 15:26:31,438 127.0.0.1 - - [10/Sep/2024 15:26:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:31,438 No batch to process for model gpt2medium-355m
2024-09-10 15:26:31,438 127.0.0.1 - - [10/Sep/2024 15:26:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:31,438 No batch to process for model distilgpt2-124m
2024-09-10 15:26:31,439 127.0.0.1 - - [10/Sep/2024 15:26:31] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:31,439 Updated batch size:1
2024-09-10 15:26:31,439 Loading model gpt2-124m
2024-09-10 15:26:31,777 Request with ID e229f7ce for model gpt2medium-355m received
2024-09-10 15:26:31,778 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:31,778 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 15:26:31,778 Batch size condition met for model gpt2medium-355m
2024-09-10 15:26:32,022 Processed batch: ['c6c5cddf'] with model gpt2-124m in 0.5050 seconds
2024-09-10 15:26:32,022 Latency for request c6c5cddf with model gpt2-124m: 0.8946 seconds
2024-09-10 15:26:32,024 Total time: 55.8348 seconds
2024-09-10 15:26:32,024 Total inference time: 50.0080 seconds
2024-09-10 15:26:32,024 Inference time as percentage of total time: 89.56%
2024-09-10 15:26:32,024 127.0.0.1 - - [10/Sep/2024 15:26:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:32,024 No batch to process for model distilgpt2-124m
2024-09-10 15:26:32,025 127.0.0.1 - - [10/Sep/2024 15:26:32] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:32,025 Updated batch size:1
2024-09-10 15:26:32,025 Loading model gpt2medium-355m
2024-09-10 15:26:33,057 Request with ID fa151d07 for model distilgpt2-124m received
2024-09-10 15:26:33,057 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:33,057 Adjusted time limit for model distilgpt2-124m: 14.1775 seconds
2024-09-10 15:26:33,057 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:33,223 Request with ID e62d94fc for model distilgpt2-124m received
2024-09-10 15:26:33,224 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:33,224 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:33,268 Request with ID 37d13146 for model distilgpt2-124m received
2024-09-10 15:26:33,268 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:33,268 Batch size condition met for model distilgpt2-124m
2024-09-10 15:26:33,475 Processed batch: ['e229f7ce'] with model gpt2medium-355m in 1.3029 seconds
2024-09-10 15:26:33,475 Latency for request e229f7ce with model gpt2medium-355m: 1.6975 seconds
2024-09-10 15:26:33,476 Total time: 57.2863 seconds
2024-09-10 15:26:33,476 Total inference time: 51.3109 seconds
2024-09-10 15:26:33,476 Inference time as percentage of total time: 89.57%
2024-09-10 15:26:33,476 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,476 Updated batch size:1
2024-09-10 15:26:33,476 Loading model distilgpt2-124m
2024-09-10 15:26:33,706 Request with ID d2769807 for model gpt2-124m received
2024-09-10 15:26:33,706 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:33,706 Adjusted time limit for model gpt2-124m: 13.6863 seconds
2024-09-10 15:26:33,706 Batch size condition met for model gpt2-124m
2024-09-10 15:26:33,803 Request with ID 59bc17bd for model gpt2-124m received
2024-09-10 15:26:33,803 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:33,803 Batch size condition met for model gpt2-124m
2024-09-10 15:26:33,915 Processed batch: ['37d13146'] with model distilgpt2-124m in 0.3746 seconds
2024-09-10 15:26:33,915 Latency for request 37d13146 with model distilgpt2-124m: 0.6467 seconds
2024-09-10 15:26:33,916 Total time: 57.7265 seconds
2024-09-10 15:26:33,916 Total inference time: 51.6855 seconds
2024-09-10 15:26:33,916 Inference time as percentage of total time: 89.54%
2024-09-10 15:26:33,916 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,916 No batch to process for model distilgpt2-124m
2024-09-10 15:26:33,916 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,916 No batch to process for model gpt2medium-355m
2024-09-10 15:26:33,916 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,917 No batch to process for model distilgpt2-124m
2024-09-10 15:26:33,917 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,917 No batch to process for model gpt2medium-355m
2024-09-10 15:26:33,917 127.0.0.1 - - [10/Sep/2024 15:26:33] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:33,917 Updated batch size:1
2024-09-10 15:26:33,917 Loading model gpt2-124m
2024-09-10 15:26:34,647 Processed batch: ['59bc17bd'] with model gpt2-124m in 0.6528 seconds
2024-09-10 15:26:34,647 Latency for request 59bc17bd with model gpt2-124m: 0.8446 seconds
2024-09-10 15:26:34,648 Total time: 58.4588 seconds
2024-09-10 15:26:34,648 Total inference time: 52.3383 seconds
2024-09-10 15:26:34,648 Inference time as percentage of total time: 89.53%
2024-09-10 15:26:34,648 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,648 No batch to process for model gpt2-124m
2024-09-10 15:26:34,649 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,649 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,649 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,649 No batch to process for model gpt2-124m
2024-09-10 15:26:34,649 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,649 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,649 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,649 No batch to process for model gpt2-124m
2024-09-10 15:26:34,650 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,650 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,650 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,650 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,650 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,650 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,650 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,650 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,650 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,650 No batch to process for model gpt2-124m
2024-09-10 15:26:34,651 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,651 No batch to process for model gpt2-124m
2024-09-10 15:26:34,651 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,651 No batch to process for model gpt2-124m
2024-09-10 15:26:34,651 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,651 No batch to process for model gpt2-124m
2024-09-10 15:26:34,651 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,651 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,651 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,651 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2-124m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2-124m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2-124m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,652 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,652 No batch to process for model gpt2-124m
2024-09-10 15:26:34,653 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,653 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,653 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,653 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,653 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,653 No batch to process for model gpt2-124m
2024-09-10 15:26:34,653 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,653 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,653 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,654 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,654 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,654 No batch to process for model gpt2-124m
2024-09-10 15:26:34,654 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,654 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,654 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,654 No batch to process for model gpt2-124m
2024-09-10 15:26:34,655 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,655 No batch to process for model gpt2-124m
2024-09-10 15:26:34,655 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,655 No batch to process for model gpt2medium-355m
2024-09-10 15:26:34,655 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,655 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,656 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,656 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,656 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,656 No batch to process for model distilgpt2-124m
2024-09-10 15:26:34,656 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,656 No batch to process for model gpt2-124m
2024-09-10 15:26:34,656 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,656 No batch to process for model gpt2-124m
2024-09-10 15:26:34,656 127.0.0.1 - - [10/Sep/2024 15:26:34] "POST /inference HTTP/1.1" 200 -
2024-09-10 15:26:34,870 Request with ID 70fcc7c5 for model gpt2-124m received
2024-09-10 15:26:34,870 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 15:26:34,870 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 15:26:34,870 Batch size condition met for model gpt2-124m
2024-09-10 15:26:34,870 Updated batch size:1
2024-09-10 15:26:34,870 Loading model gpt2-124m
2024-09-10 15:26:35,336 Processed batch: ['70fcc7c5'] with model gpt2-124m in 0.4654 seconds
2024-09-10 15:26:35,336 Latency for request 70fcc7c5 with model gpt2-124m: 0.4658 seconds
2024-09-10 15:26:35,337 Total time: 59.1472 seconds
2024-09-10 15:26:35,337 Total inference time: 52.8038 seconds
2024-09-10 15:26:35,337 Inference time as percentage of total time: 89.28%
2024-09-10 15:26:35,337 127.0.0.1 - - [10/Sep/2024 15:26:35] "POST /inference HTTP/1.1" 200 -
