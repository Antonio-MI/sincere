2024-09-10 16:25:38,143 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.11.77:5000
2024-09-10 16:25:38,144 [33mPress CTRL+C to quit[0m
2024-09-10 16:25:40,292 Request with ID ff3d9750 for model gpt2-124m received
2024-09-10 16:25:40,292 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 16:25:40,292 Adjusted time limit for model gpt2-124m: 13.6926 seconds
2024-09-10 16:25:40,292 127.0.0.1 - - [10/Sep/2024 16:25:40] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:40,388 Remaining requests condition met for model gpt2-124m
2024-09-10 16:25:40,388 Updated batch size:1
2024-09-10 16:25:40,388 Loading model gpt2-124m
2024-09-10 16:25:40,849 Processed batch: ['ff3d9750'] with model gpt2-124m in 0.3741 seconds
2024-09-10 16:25:40,849 Latency for request ff3d9750 with model gpt2-124m: 0.5564 seconds
2024-09-10 16:25:40,850 Total time: 0.5583 seconds
2024-09-10 16:25:40,850 Total inference time: 0.3741 seconds
2024-09-10 16:25:40,851 Inference time as percentage of total time: 67.00%
2024-09-10 16:25:40,851 Total time: 0.5584 seconds
2024-09-10 16:25:40,851 Total inference time: 0.3741 seconds
2024-09-10 16:25:40,851 Inference time as percentage of total time: 66.99%
2024-09-10 16:25:40,955 Total time: 0.6628 seconds
2024-09-10 16:25:40,955 Total inference time: 0.3741 seconds
2024-09-10 16:25:40,955 Inference time as percentage of total time: 56.44%
2024-09-10 16:25:41,056 Total time: 0.7641 seconds
2024-09-10 16:25:41,056 Total inference time: 0.3741 seconds
2024-09-10 16:25:41,057 Inference time as percentage of total time: 48.95%
2024-09-10 16:25:41,118 Request with ID ea5a09c0 for model gpt2-124m received
2024-09-10 16:25:41,118 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 16:25:41,118 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 16:25:41,118 127.0.0.1 - - [10/Sep/2024 16:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:41,162 Remaining requests condition met for model gpt2-124m
2024-09-10 16:25:41,162 Updated batch size:1
2024-09-10 16:25:41,162 Loading model gpt2-124m
2024-09-10 16:25:41,449 Request with ID 22210930 for model gpt2medium-355m received
2024-09-10 16:25:41,449 Adjusted time limit based on total queue size 1: 15.0000 seconds
2024-09-10 16:25:41,449 Adjusted time limit for model gpt2medium-355m: 11.8798 seconds
2024-09-10 16:25:41,449 127.0.0.1 - - [10/Sep/2024 16:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:41,512 Request with ID 209945fa for model gpt2-124m received
2024-09-10 16:25:41,512 Adjusted time limit based on total queue size 2: 15.0000 seconds
2024-09-10 16:25:41,512 127.0.0.1 - - [10/Sep/2024 16:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:41,524 Processed batch: ['ea5a09c0'] with model gpt2-124m in 0.3617 seconds
2024-09-10 16:25:41,524 Latency for request ea5a09c0 with model gpt2-124m: 0.4058 seconds
2024-09-10 16:25:41,883 Request with ID 4ee97dab for model gpt2-124m received
2024-09-10 16:25:41,884 Adjusted time limit based on total queue size 3: 15.0000 seconds
2024-09-10 16:25:41,884 Adjusted time limit for model gpt2-124m: 13.6858 seconds
2024-09-10 16:25:41,884 127.0.0.1 - - [10/Sep/2024 16:25:41] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:42,549 Request with ID 11c85551 for model distilgpt2-124m received
2024-09-10 16:25:42,550 Adjusted time limit based on total queue size 4: 9.0000 seconds
2024-09-10 16:25:42,550 Adjusted time limit for model distilgpt2-124m: 14.1814 seconds
2024-09-10 16:25:42,551 127.0.0.1 - - [10/Sep/2024 16:25:42] "POST /inference HTTP/1.1" 200 -
2024-09-10 16:25:43,159 Request with ID 013b53b2 for model distilgpt2-124m received
2024-09-10 16:25:43,159 Adjusted time limit based on total queue size 5: 9.0000 seconds
2024-09-10 16:25:43,160 127.0.0.1 - - [10/Sep/2024 16:25:43] "POST /inference HTTP/1.1" 200 -
