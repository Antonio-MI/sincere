2024-09-17 11:36:08,945 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.1.212:5000
2024-09-17 11:36:08,945 [33mPress CTRL+C to quit[0m
2024-09-17 11:36:11,356 Request with ID 492a1e36 for model gpt2medium-355m received
2024-09-17 11:36:11,356 Batch size condition met for model gpt2medium-355m
2024-09-17 11:36:11,356 Next: call load_model for gpt2medium-355m
2024-09-17 11:36:11,483 Loaded model gpt2medium-355m
2024-09-17 11:36:11,483 Batch processing started for model gpt2medium-355m
2024-09-17 11:36:12,683 Request with ID eeff91d5 for model gpt2medium-355m received
2024-09-17 11:36:12,683 Batch size condition met for model gpt2medium-355m
2024-09-17 11:36:13,703 Processed batch: ['492a1e36'] with model gpt2medium-355m in 2.2196 seconds
2024-09-17 11:36:13,703 Latency for request 492a1e36 with model gpt2medium-355m: 2.3466 seconds
2024-09-17 11:36:13,705 127.0.0.1 - - [17/Sep/2024 11:36:13] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:36:13,705 Next: call load_model for gpt2medium-355m
2024-09-17 11:36:13,705 Model gpt2medium-355m already loaded
2024-09-17 11:36:13,705 Batch processing started for model gpt2medium-355m
2024-09-17 11:36:13,824 Processed batch: ['eeff91d5'] with model gpt2medium-355m in 0.1186 seconds
2024-09-17 11:36:13,824 Latency for request eeff91d5 with model gpt2medium-355m: 1.1413 seconds
2024-09-17 11:36:13,825 127.0.0.1 - - [17/Sep/2024 11:36:13] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:36:14,942 Request with ID 6ea2cd11 for model gpt2-124m received
2024-09-17 11:36:14,942 Batch size condition met for model gpt2-124m
2024-09-17 11:36:14,943 Next: call load_model for gpt2-124m
2024-09-17 11:36:14,975 Unloaded previous model
2024-09-17 11:36:15,040 Loaded model gpt2-124m
2024-09-17 11:36:15,040 Batch processing started for model gpt2-124m
2024-09-17 11:36:15,784 Processed batch: ['6ea2cd11'] with model gpt2-124m in 0.7436 seconds
2024-09-17 11:36:15,784 Latency for request 6ea2cd11 with model gpt2-124m: 0.8419 seconds
2024-09-17 11:36:15,785 127.0.0.1 - - [17/Sep/2024 11:36:15] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:36:17,269 Request with ID 9768f24a for model distilgpt2-124m received
2024-09-17 11:36:17,270 Batch size condition met for model distilgpt2-124m
2024-09-17 11:36:17,270 Next: call load_model for distilgpt2-124m
2024-09-17 11:36:17,285 Unloaded previous model
2024-09-17 11:36:17,343 Loaded model distilgpt2-124m
2024-09-17 11:36:17,344 Batch processing started for model distilgpt2-124m
2024-09-17 11:36:17,873 Processed batch: ['9768f24a'] with model distilgpt2-124m in 0.5294 seconds
2024-09-17 11:36:17,873 Latency for request 9768f24a with model distilgpt2-124m: 0.6039 seconds
2024-09-17 11:36:17,874 127.0.0.1 - - [17/Sep/2024 11:36:17] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:36:18,704 Request with ID 9a26f732 for model gpt2-124m received
2024-09-17 11:36:18,705 Batch size condition met for model gpt2-124m
2024-09-17 11:36:18,705 Next: call load_model for gpt2-124m
2024-09-17 11:36:18,726 Unloaded previous model
2024-09-17 11:36:18,803 Loaded model gpt2-124m
2024-09-17 11:36:18,804 Batch processing started for model gpt2-124m
2024-09-17 11:36:19,617 Processed batch: ['9a26f732'] with model gpt2-124m in 0.8136 seconds
2024-09-17 11:36:19,617 Latency for request 9a26f732 with model gpt2-124m: 0.9127 seconds
2024-09-17 11:36:19,618 127.0.0.1 - - [17/Sep/2024 11:36:19] "POST /inference HTTP/1.1" 200 -
2024-09-17 11:36:20,885 Request with ID 94fe5b1f for model gpt2medium-355m received
2024-09-17 11:36:20,886 Batch size condition met for model gpt2medium-355m
2024-09-17 11:36:20,886 Next: call load_model for gpt2medium-355m
2024-09-17 11:36:20,908 Unloaded previous model
2024-09-17 11:36:21,042 Loaded model gpt2medium-355m
2024-09-17 11:36:21,042 Batch processing started for model gpt2medium-355m
2024-09-17 11:36:21,358 Total time: 1726484600.9692 seconds
2024-09-17 11:36:21,358 Total inference time: 4.4248 seconds
2024-09-17 11:36:21,358 Inference time as percentage of total time: 0.00%
2024-09-17 11:36:21,358 END
2024-09-17 11:36:21,358 Request with ID 96e30d36 for model Stop received
2024-09-17 11:36:21,358 127.0.0.1 - - [17/Sep/2024 11:36:21] "[31m[1mPOST /inference HTTP/1.1[0m" 400 -
2024-09-17 11:36:23,385 Processed batch: ['94fe5b1f'] with model gpt2medium-355m in 2.3426 seconds
2024-09-17 11:36:23,385 Latency for request 94fe5b1f with model gpt2medium-355m: 2.4997 seconds
2024-09-17 11:36:23,386 127.0.0.1 - - [17/Sep/2024 11:36:23] "POST /inference HTTP/1.1" 200 -
