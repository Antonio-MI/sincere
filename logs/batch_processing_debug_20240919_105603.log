2024-09-19 10:56:03,166 Using device: cpu
2024-09-19 10:56:03,166 Scheduling mode set as batchedFCFS+SLA
2024-09-19 10:56:03,362 [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://9.74.10.97:5000
2024-09-19 10:56:03,368 [33mPress CTRL+C to quit[0m
2024-09-19 10:56:07,604 Request with ID b01cb8bd for model gpt2-124m received
2024-09-19 10:56:07,611 127.0.0.1 - - [19/Sep/2024 10:56:07] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:07,747 Request with ID 49dceebc for model gpt2-124m received
2024-09-19 10:56:07,760 127.0.0.1 - - [19/Sep/2024 10:56:07] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:07,766 Request with ID 62e7db5f for model gpt2medium-355m received
2024-09-19 10:56:07,773 127.0.0.1 - - [19/Sep/2024 10:56:07] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:07,853 Request with ID 5f1608d9 for model gpt2-124m received
2024-09-19 10:56:07,859 127.0.0.1 - - [19/Sep/2024 10:56:07] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,217 Request with ID 30d0eb28 for model gpt2medium-355m received
2024-09-19 10:56:08,223 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,230 Request with ID 78e3d2c5 for model gpt2-124m received
2024-09-19 10:56:08,243 Request with ID ec88d03e for model gpt2-124m received
2024-09-19 10:56:08,243 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,249 Request with ID 052dfc59 for model distilgpt2-124m received
2024-09-19 10:56:08,249 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,256 Request with ID 78cce586 for model gpt2-124m received
2024-09-19 10:56:08,269 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,288 Request with ID c77ec48c for model gpt2-124m received
2024-09-19 10:56:08,313 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,345 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,681 Request with ID 6b98846c for model gpt2-124m received
2024-09-19 10:56:08,687 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,774 Request with ID 5d6cac70 for model gpt2-124m received
2024-09-19 10:56:08,774 Request with ID 3f94cc7b for model gpt2medium-355m received
2024-09-19 10:56:08,774 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,793 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:08,937 Request with ID f27324de for model gpt2-124m received
2024-09-19 10:56:08,943 127.0.0.1 - - [19/Sep/2024 10:56:08] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,053 Request with ID 5f0db1db for model gpt2-124m received
2024-09-19 10:56:09,059 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,307 Request with ID edf2f5d0 for model gpt2-124m received
2024-09-19 10:56:09,314 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,436 Request with ID 2414a5a5 for model distilgpt2-124m received
2024-09-19 10:56:09,443 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,524 Request with ID d7e71973 for model gpt2medium-355m received
2024-09-19 10:56:09,530 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,674 Request with ID 3f7d1359 for model gpt2-124m received
2024-09-19 10:56:09,681 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:09,923 Request with ID 3d5ceafb for model gpt2-124m received
2024-09-19 10:56:09,929 127.0.0.1 - - [19/Sep/2024 10:56:09] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,023 Request with ID 0ee72f8b for model gpt2-124m received
2024-09-19 10:56:10,029 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,048 Request with ID df68b33d for model distilgpt2-124m received
2024-09-19 10:56:10,055 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,150 Request with ID d0b7ae4a for model distilgpt2-124m received
2024-09-19 10:56:10,156 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,175 Request with ID 14c41925 for model gpt2medium-355m received
2024-09-19 10:56:10,175 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,200 Request with ID 5f92a3aa for model gpt2-124m received
2024-09-19 10:56:10,213 Batch size condition met for model gpt2-124m
2024-09-19 10:56:10,219 Request with ID 6f861d83 for model gpt2-124m received
2024-09-19 10:56:10,232 Next: call load_model for gpt2-124m
2024-09-19 10:56:10,250 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,358 Request with ID a90933ab for model distilgpt2-124m received
2024-09-19 10:56:10,364 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,370 Request with ID 7abb716a for model gpt2medium-355m received
2024-09-19 10:56:10,370 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,603 Request with ID 59ea75a5 for model gpt2-124m received
2024-09-19 10:56:10,616 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,622 Request with ID 0045875d for model distilgpt2-124m received
2024-09-19 10:56:10,629 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,642 Request with ID da156842 for model gpt2-124m received
2024-09-19 10:56:10,642 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,660 Request with ID 814f938e for model gpt2medium-355m received
2024-09-19 10:56:10,679 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,798 Request with ID b2ac3137 for model distilgpt2-124m received
2024-09-19 10:56:10,804 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,853 Request with ID cf17cf79 for model distilgpt2-124m received
2024-09-19 10:56:10,854 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:10,878 Request with ID 944fb584 for model gpt2-124m received
2024-09-19 10:56:10,884 127.0.0.1 - - [19/Sep/2024 10:56:10] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,086 Request with ID 253cc614 for model distilgpt2-124m received
2024-09-19 10:56:11,098 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,212 Request with ID 4d7e4362 for model distilgpt2-124m received
2024-09-19 10:56:11,218 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,236 Request with ID 5ed42bf0 for model distilgpt2-124m received
2024-09-19 10:56:11,237 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,375 Request with ID 8ee56042 for model gpt2medium-355m received
2024-09-19 10:56:11,382 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,537 Request with ID 7f2d1e04 for model gpt2medium-355m received
2024-09-19 10:56:11,544 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,562 Request with ID 18311996 for model distilgpt2-124m received
2024-09-19 10:56:11,575 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,707 Request with ID f9621cac for model gpt2-124m received
2024-09-19 10:56:11,707 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,725 Request with ID 6ff34f7b for model distilgpt2-124m received
2024-09-19 10:56:11,732 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,744 Request with ID 340cdf32 for model gpt2medium-355m received
2024-09-19 10:56:11,750 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,757 Request with ID 85bfc1e9 for model distilgpt2-124m received
2024-09-19 10:56:11,775 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:11,795 Request with ID b3278d14 for model gpt2-124m received
2024-09-19 10:56:11,801 127.0.0.1 - - [19/Sep/2024 10:56:11] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,086 Request with ID 682128dc for model distilgpt2-124m received
2024-09-19 10:56:12,098 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,153 Request with ID c3c89476 for model gpt2medium-355m received
2024-09-19 10:56:12,153 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,472 Request with ID 3797c3df for model gpt2-124m received
2024-09-19 10:56:12,472 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,495 Request with ID 04acea72 for model gpt2medium-355m received
2024-09-19 10:56:12,501 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,520 Request with ID 566442b8 for model gpt2-124m received
2024-09-19 10:56:12,527 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,759 Request with ID ef977df8 for model gpt2-124m received
2024-09-19 10:56:12,765 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,828 Request with ID a3e08df6 for model distilgpt2-124m received
2024-09-19 10:56:12,847 Batch size condition met for model distilgpt2-124m
2024-09-19 10:56:12,921 Request with ID 74e03c75 for model gpt2-124m received
2024-09-19 10:56:12,934 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:12,966 Request with ID 12c62ad4 for model gpt2-124m received
2024-09-19 10:56:12,972 127.0.0.1 - - [19/Sep/2024 10:56:12] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,048 Request with ID ca9e6699 for model gpt2-124m received
2024-09-19 10:56:13,054 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,223 Request with ID e8f44c6d for model gpt2-124m received
2024-09-19 10:56:13,236 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,286 Request with ID 8a425862 for model distilgpt2-124m received
2024-09-19 10:56:13,286 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,431 Request with ID f950c162 for model distilgpt2-124m received
2024-09-19 10:56:13,431 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,431 Request with ID d52511f6 for model gpt2-124m received
2024-09-19 10:56:13,469 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,550 Request with ID 946c97d8 for model distilgpt2-124m received
2024-09-19 10:56:13,556 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,737 Request with ID c4b04303 for model gpt2-124m received
2024-09-19 10:56:13,742 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,761 Request with ID 08fd33a9 for model gpt2medium-355m received
2024-09-19 10:56:13,768 127.0.0.1 - - [19/Sep/2024 10:56:13] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:13,793 Request with ID eb7a30f1 for model gpt2-124m received
2024-09-19 10:56:13,824 Batch size condition met for model gpt2-124m
2024-09-19 10:56:14,182 Request with ID b1c1045c for model gpt2-124m received
2024-09-19 10:56:14,187 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,212 Request with ID f1a2d5a7 for model gpt2medium-355m received
2024-09-19 10:56:14,219 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,307 Request with ID 5c8190e5 for model gpt2-124m received
2024-09-19 10:56:14,313 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,433 Request with ID bf7562a2 for model gpt2medium-355m received
2024-09-19 10:56:14,463 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,488 Request with ID 1a472692 for model distilgpt2-124m received
2024-09-19 10:56:14,488 Request with ID cb9732b9 for model gpt2medium-355m received
2024-09-19 10:56:14,494 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,513 Batch size condition met for model gpt2medium-355m
2024-09-19 10:56:14,574 Request with ID 1e943399 for model distilgpt2-124m received
2024-09-19 10:56:14,574 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,593 Request with ID e38c88d0 for model gpt2-124m received
2024-09-19 10:56:14,599 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,649 Request with ID 29eabcf6 for model gpt2-124m received
2024-09-19 10:56:14,661 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,774 Request with ID eb7848d2 for model gpt2-124m received
2024-09-19 10:56:14,780 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:14,902 Request with ID 7ff76623 for model distilgpt2-124m received
2024-09-19 10:56:14,927 127.0.0.1 - - [19/Sep/2024 10:56:14] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,041 Request with ID 78a9358a for model distilgpt2-124m received
2024-09-19 10:56:15,059 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,059 Request with ID 375d7248 for model gpt2-124m received
2024-09-19 10:56:15,078 Request with ID 8d295e20 for model gpt2medium-355m received
2024-09-19 10:56:15,083 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,102 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,145 Request with ID 29d67248 for model distilgpt2-124m received
2024-09-19 10:56:15,160 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,306 Request with ID 6c3938f1 for model distilgpt2-124m received
2024-09-19 10:56:15,306 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,430 Request with ID a38df03c for model gpt2-124m received
2024-09-19 10:56:15,437 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,543 Request with ID 9597b8be for model distilgpt2-124m received
2024-09-19 10:56:15,556 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,698 Request with ID d8af34dd for model gpt2medium-355m received
2024-09-19 10:56:15,704 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,752 Request with ID 764bc8df for model gpt2medium-355m received
2024-09-19 10:56:15,758 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,795 Request with ID ff443346 for model gpt2medium-355m received
2024-09-19 10:56:15,801 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:15,856 Request with ID c52de4ce for model gpt2medium-355m received
2024-09-19 10:56:15,869 127.0.0.1 - - [19/Sep/2024 10:56:15] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:16,172 Request with ID b2cf48cf for model gpt2-124m received
2024-09-19 10:56:16,178 127.0.0.1 - - [19/Sep/2024 10:56:16] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:16,219 Request with ID db9ed443 for model gpt2-124m received
2024-09-19 10:56:16,225 127.0.0.1 - - [19/Sep/2024 10:56:16] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:16,282 Request with ID a7ec0037 for model distilgpt2-124m received
2024-09-19 10:56:16,282 127.0.0.1 - - [19/Sep/2024 10:56:16] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:16,578 Request with ID 0c64f798 for model gpt2medium-355m received
2024-09-19 10:56:16,603 127.0.0.1 - - [19/Sep/2024 10:56:16] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:16,610 Request with ID 67bcb5be for model distilgpt2-124m received
2024-09-19 10:56:16,610 127.0.0.1 - - [19/Sep/2024 10:56:16] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,050 Request with ID 86d3fc8d for model gpt2-124m received
2024-09-19 10:56:17,063 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,088 Request with ID 3ba9cbcb for model gpt2-124m received
2024-09-19 10:56:17,094 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,114 Request with ID 0aaac094 for model gpt2medium-355m received
2024-09-19 10:56:17,114 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,321 Request with ID 775cc397 for model gpt2-124m received
2024-09-19 10:56:17,333 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,333 Request with ID 970f052e for model distilgpt2-124m received
2024-09-19 10:56:17,334 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,414 Request with ID e72066a2 for model gpt2-124m received
2024-09-19 10:56:17,414 127.0.0.1 - - [19/Sep/2024 10:56:17] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:17,607 Waiting for running processes to finish
2024-09-19 10:56:18,223 Loaded model gpt2-124m
2024-09-19 10:56:18,230 Batch processing started for model gpt2-124m
2024-09-19 10:56:18,617 Waiting for running processes to finish
2024-09-19 10:56:19,628 Waiting for running processes to finish
2024-09-19 10:56:20,639 Waiting for running processes to finish
2024-09-19 10:56:21,654 Waiting for running processes to finish
2024-09-19 10:56:22,387 Time limit condition met for model gpt2-124m
2024-09-19 10:56:22,671 Waiting for running processes to finish
2024-09-19 10:56:23,677 Waiting for running processes to finish
2024-09-19 10:56:24,681 Waiting for running processes to finish
2024-09-19 10:56:25,683 Waiting for running processes to finish
2024-09-19 10:56:25,877 Processed batch: ['b01cb8bd', '49dceebc', '5f1608d9', '78e3d2c5', 'ec88d03e', '78cce586', 'c77ec48c', '6b98846c', '5d6cac70', 'f27324de', '5f0db1db', 'edf2f5d0', '3f7d1359', '3d5ceafb', '0ee72f8b', '5f92a3aa'] with model gpt2-124m in 7.6469 seconds
2024-09-19 10:56:25,877 Latency for request b01cb8bd with model gpt2-124m: 18.2720 seconds
2024-09-19 10:56:25,877 Saving results without gpu monitoring
2024-09-19 10:56:25,878 Latency for request 49dceebc with model gpt2-124m: 18.1300 seconds
2024-09-19 10:56:25,878 Saving results without gpu monitoring
2024-09-19 10:56:25,879 Latency for request 5f1608d9 with model gpt2-124m: 18.0240 seconds
2024-09-19 10:56:25,879 Saving results without gpu monitoring
2024-09-19 10:56:25,879 Latency for request 78e3d2c5 with model gpt2-124m: 17.6470 seconds
2024-09-19 10:56:25,879 Saving results without gpu monitoring
2024-09-19 10:56:25,879 Latency for request ec88d03e with model gpt2-124m: 17.6340 seconds
2024-09-19 10:56:25,879 Saving results without gpu monitoring
2024-09-19 10:56:25,880 Latency for request 78cce586 with model gpt2-124m: 17.6200 seconds
2024-09-19 10:56:25,880 Saving results without gpu monitoring
2024-09-19 10:56:25,880 Latency for request c77ec48c with model gpt2-124m: 17.5890 seconds
2024-09-19 10:56:25,880 Saving results without gpu monitoring
2024-09-19 10:56:25,880 Latency for request 6b98846c with model gpt2-124m: 17.1950 seconds
2024-09-19 10:56:25,880 Saving results without gpu monitoring
2024-09-19 10:56:25,880 Latency for request 5d6cac70 with model gpt2-124m: 17.1020 seconds
2024-09-19 10:56:25,880 Saving results without gpu monitoring
2024-09-19 10:56:25,880 Latency for request f27324de with model gpt2-124m: 16.9400 seconds
2024-09-19 10:56:25,880 Saving results without gpu monitoring
2024-09-19 10:56:25,881 Latency for request 5f0db1db with model gpt2-124m: 16.8240 seconds
2024-09-19 10:56:25,881 Saving results without gpu monitoring
2024-09-19 10:56:25,881 Latency for request edf2f5d0 with model gpt2-124m: 16.5690 seconds
2024-09-19 10:56:25,881 Saving results without gpu monitoring
2024-09-19 10:56:25,882 Latency for request 3f7d1359 with model gpt2-124m: 16.2020 seconds
2024-09-19 10:56:25,882 Saving results without gpu monitoring
2024-09-19 10:56:25,882 Latency for request 3d5ceafb with model gpt2-124m: 15.9530 seconds
2024-09-19 10:56:25,882 Saving results without gpu monitoring
2024-09-19 10:56:25,883 Latency for request 0ee72f8b with model gpt2-124m: 15.8540 seconds
2024-09-19 10:56:25,883 Saving results without gpu monitoring
2024-09-19 10:56:25,883 Latency for request 5f92a3aa with model gpt2-124m: 15.6760 seconds
2024-09-19 10:56:25,883 Saving results without gpu monitoring
2024-09-19 10:56:25,883 127.0.0.1 - - [19/Sep/2024 10:56:25] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:25,883 Next: call load_model for distilgpt2-124m
2024-09-19 10:56:25,893 Unloaded previous model
2024-09-19 10:56:25,940 Loaded model distilgpt2-124m
2024-09-19 10:56:25,941 Batch processing started for model distilgpt2-124m
2024-09-19 10:56:26,687 Waiting for running processes to finish
2024-09-19 10:56:27,693 Waiting for running processes to finish
2024-09-19 10:56:28,453 Processed batch: ['052dfc59', '2414a5a5', 'df68b33d', 'd0b7ae4a', 'a90933ab', '0045875d', 'b2ac3137', 'cf17cf79', '253cc614', '4d7e4362', '5ed42bf0', '18311996', '6ff34f7b', '85bfc1e9', '682128dc', 'a3e08df6'] with model distilgpt2-124m in 2.5121 seconds
2024-09-19 10:56:28,453 Latency for request 052dfc59 with model distilgpt2-124m: 20.2030 seconds
2024-09-19 10:56:28,453 Saving results without gpu monitoring
2024-09-19 10:56:28,453 Latency for request 2414a5a5 with model distilgpt2-124m: 19.0160 seconds
2024-09-19 10:56:28,453 Saving results without gpu monitoring
2024-09-19 10:56:28,454 Latency for request df68b33d with model distilgpt2-124m: 18.4040 seconds
2024-09-19 10:56:28,454 Saving results without gpu monitoring
2024-09-19 10:56:28,454 Latency for request d0b7ae4a with model distilgpt2-124m: 18.3030 seconds
2024-09-19 10:56:28,454 Saving results without gpu monitoring
2024-09-19 10:56:28,454 Latency for request a90933ab with model distilgpt2-124m: 18.0950 seconds
2024-09-19 10:56:28,454 Saving results without gpu monitoring
2024-09-19 10:56:28,454 Latency for request 0045875d with model distilgpt2-124m: 17.8300 seconds
2024-09-19 10:56:28,455 Saving results without gpu monitoring
2024-09-19 10:56:28,455 Latency for request b2ac3137 with model distilgpt2-124m: 17.6550 seconds
2024-09-19 10:56:28,455 Saving results without gpu monitoring
2024-09-19 10:56:28,455 Latency for request cf17cf79 with model distilgpt2-124m: 17.5990 seconds
2024-09-19 10:56:28,455 Saving results without gpu monitoring
2024-09-19 10:56:28,455 Latency for request 253cc614 with model distilgpt2-124m: 17.3670 seconds
2024-09-19 10:56:28,455 Saving results without gpu monitoring
2024-09-19 10:56:28,455 Latency for request 4d7e4362 with model distilgpt2-124m: 17.2410 seconds
2024-09-19 10:56:28,455 Saving results without gpu monitoring
2024-09-19 10:56:28,456 Latency for request 5ed42bf0 with model distilgpt2-124m: 17.2160 seconds
2024-09-19 10:56:28,456 Saving results without gpu monitoring
2024-09-19 10:56:28,456 Latency for request 18311996 with model distilgpt2-124m: 16.8900 seconds
2024-09-19 10:56:28,456 Saving results without gpu monitoring
2024-09-19 10:56:28,456 Latency for request 6ff34f7b with model distilgpt2-124m: 16.7270 seconds
2024-09-19 10:56:28,456 Saving results without gpu monitoring
2024-09-19 10:56:28,456 Latency for request 85bfc1e9 with model distilgpt2-124m: 16.6960 seconds
2024-09-19 10:56:28,456 Saving results without gpu monitoring
2024-09-19 10:56:28,457 Latency for request 682128dc with model distilgpt2-124m: 16.3670 seconds
2024-09-19 10:56:28,457 Saving results without gpu monitoring
2024-09-19 10:56:28,457 Latency for request a3e08df6 with model distilgpt2-124m: 15.6240 seconds
2024-09-19 10:56:28,457 Saving results without gpu monitoring
2024-09-19 10:56:28,457 127.0.0.1 - - [19/Sep/2024 10:56:28] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:28,457 Next: call load_model for gpt2-124m
2024-09-19 10:56:28,463 Unloaded previous model
2024-09-19 10:56:28,525 Loaded model gpt2-124m
2024-09-19 10:56:28,525 Batch processing started for model gpt2-124m
2024-09-19 10:56:28,695 Waiting for running processes to finish
2024-09-19 10:56:29,697 Waiting for running processes to finish
2024-09-19 10:56:30,702 Waiting for running processes to finish
2024-09-19 10:56:31,543 Processed batch: ['b1c1045c', '5c8190e5', 'e38c88d0', '29eabcf6', 'eb7848d2', '375d7248', 'a38df03c', 'b2cf48cf', 'db9ed443', '86d3fc8d', '3ba9cbcb', '775cc397', 'e72066a2'] with model gpt2-124m in 3.0185 seconds
2024-09-19 10:56:31,544 Latency for request b1c1045c with model gpt2-124m: 17.3610 seconds
2024-09-19 10:56:31,544 Saving results without gpu monitoring
2024-09-19 10:56:31,545 Latency for request 5c8190e5 with model gpt2-124m: 17.2360 seconds
2024-09-19 10:56:31,545 Saving results without gpu monitoring
2024-09-19 10:56:31,545 Latency for request e38c88d0 with model gpt2-124m: 16.9500 seconds
2024-09-19 10:56:31,545 Saving results without gpu monitoring
2024-09-19 10:56:31,546 Latency for request 29eabcf6 with model gpt2-124m: 16.8940 seconds
2024-09-19 10:56:31,546 Saving results without gpu monitoring
2024-09-19 10:56:31,546 Latency for request eb7848d2 with model gpt2-124m: 16.7690 seconds
2024-09-19 10:56:31,546 Saving results without gpu monitoring
2024-09-19 10:56:31,546 Latency for request 375d7248 with model gpt2-124m: 16.4840 seconds
2024-09-19 10:56:31,546 Saving results without gpu monitoring
2024-09-19 10:56:31,546 Latency for request a38df03c with model gpt2-124m: 16.1130 seconds
2024-09-19 10:56:31,546 Saving results without gpu monitoring
2024-09-19 10:56:31,547 Latency for request b2cf48cf with model gpt2-124m: 15.3710 seconds
2024-09-19 10:56:31,547 Saving results without gpu monitoring
2024-09-19 10:56:31,547 Latency for request db9ed443 with model gpt2-124m: 15.3240 seconds
2024-09-19 10:56:31,547 Saving results without gpu monitoring
2024-09-19 10:56:31,547 Latency for request 86d3fc8d with model gpt2-124m: 14.4930 seconds
2024-09-19 10:56:31,547 Saving results without gpu monitoring
2024-09-19 10:56:31,547 Latency for request 3ba9cbcb with model gpt2-124m: 14.4550 seconds
2024-09-19 10:56:31,547 Saving results without gpu monitoring
2024-09-19 10:56:31,548 Latency for request 775cc397 with model gpt2-124m: 14.2220 seconds
2024-09-19 10:56:31,548 Saving results without gpu monitoring
2024-09-19 10:56:31,548 Latency for request e72066a2 with model gpt2-124m: 14.1300 seconds
2024-09-19 10:56:31,548 Saving results without gpu monitoring
2024-09-19 10:56:31,548 127.0.0.1 - - [19/Sep/2024 10:56:31] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:31,548 Next: call load_model for gpt2medium-355m
2024-09-19 10:56:31,558 Unloaded previous model
2024-09-19 10:56:31,653 Loaded model gpt2medium-355m
2024-09-19 10:56:31,653 Batch processing started for model gpt2medium-355m
2024-09-19 10:56:31,703 Waiting for running processes to finish
2024-09-19 10:56:33,712 Total time: 23.9439 seconds
2024-09-19 10:56:33,713 Total inference time: 13.1780 seconds
2024-09-19 10:56:33,713 Inference time as percentage of total time: 55.04%
2024-09-19 10:56:33,713 END
2024-09-19 10:56:33,713 127.0.0.1 - - [19/Sep/2024 10:56:33] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:49,571 Processed batch: ['62e7db5f', '30d0eb28', '3f94cc7b', 'd7e71973', '14c41925', '7abb716a', '814f938e', '8ee56042', '7f2d1e04', '340cdf32', 'c3c89476', '04acea72', '08fd33a9', 'f1a2d5a7', 'bf7562a2', 'cb9732b9'] with model gpt2medium-355m in 17.9178 seconds
2024-09-19 10:56:49,572 Latency for request 62e7db5f with model gpt2medium-355m: 41.8050 seconds
2024-09-19 10:56:49,572 Saving results without gpu monitoring
2024-09-19 10:56:49,573 Latency for request 30d0eb28 with model gpt2medium-355m: 41.3540 seconds
2024-09-19 10:56:49,573 Saving results without gpu monitoring
2024-09-19 10:56:49,573 Latency for request 3f94cc7b with model gpt2medium-355m: 40.7960 seconds
2024-09-19 10:56:49,573 Saving results without gpu monitoring
2024-09-19 10:56:49,574 Latency for request d7e71973 with model gpt2medium-355m: 40.0470 seconds
2024-09-19 10:56:49,574 Saving results without gpu monitoring
2024-09-19 10:56:49,574 Latency for request 14c41925 with model gpt2medium-355m: 39.3960 seconds
2024-09-19 10:56:49,574 Saving results without gpu monitoring
2024-09-19 10:56:49,574 Latency for request 7abb716a with model gpt2medium-355m: 39.2010 seconds
2024-09-19 10:56:49,574 Saving results without gpu monitoring
2024-09-19 10:56:49,574 Latency for request 814f938e with model gpt2medium-355m: 38.9100 seconds
2024-09-19 10:56:49,574 Saving results without gpu monitoring
2024-09-19 10:56:49,575 Latency for request 8ee56042 with model gpt2medium-355m: 38.1950 seconds
2024-09-19 10:56:49,575 Saving results without gpu monitoring
2024-09-19 10:56:49,575 Latency for request 7f2d1e04 with model gpt2medium-355m: 38.0330 seconds
2024-09-19 10:56:49,575 Saving results without gpu monitoring
2024-09-19 10:56:49,575 Latency for request 340cdf32 with model gpt2medium-355m: 37.8270 seconds
2024-09-19 10:56:49,575 Saving results without gpu monitoring
2024-09-19 10:56:49,575 Latency for request c3c89476 with model gpt2medium-355m: 37.4180 seconds
2024-09-19 10:56:49,575 Saving results without gpu monitoring
2024-09-19 10:56:49,576 Latency for request 04acea72 with model gpt2medium-355m: 37.0760 seconds
2024-09-19 10:56:49,576 Saving results without gpu monitoring
2024-09-19 10:56:49,576 Latency for request 08fd33a9 with model gpt2medium-355m: 35.8100 seconds
2024-09-19 10:56:49,576 Saving results without gpu monitoring
2024-09-19 10:56:49,576 Latency for request f1a2d5a7 with model gpt2medium-355m: 35.3590 seconds
2024-09-19 10:56:49,576 Saving results without gpu monitoring
2024-09-19 10:56:49,576 Latency for request bf7562a2 with model gpt2medium-355m: 35.1380 seconds
2024-09-19 10:56:49,576 Saving results without gpu monitoring
2024-09-19 10:56:49,577 Latency for request cb9732b9 with model gpt2medium-355m: 35.0830 seconds
2024-09-19 10:56:49,577 Saving results without gpu monitoring
2024-09-19 10:56:49,577 127.0.0.1 - - [19/Sep/2024 10:56:49] "POST /inference HTTP/1.1" 200 -
2024-09-19 10:56:49,577 No batch to process for model gpt2-124m
